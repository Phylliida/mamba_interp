{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1812c036-831d-4b08-b14a-156ede4cbb7c",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a248986-64cf-433e-8782-9cc0585347f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", font_size=None, show=True, color_continuous_midpoint=0.0, **kwargs):\n",
    "    import plotly.express as px\n",
    "    import transformer_lens.utils as utils\n",
    "    fig = px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=color_continuous_midpoint, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs)\n",
    "    if not font_size is None:\n",
    "        if 'x' in kwargs:\n",
    "            fig.update_layout(\n",
    "              xaxis = dict(\n",
    "                tickmode='array',\n",
    "                tickvals = kwargs['x'],\n",
    "                ticktext = kwargs['x'], \n",
    "                ),\n",
    "               font=dict(size=font_size, color=\"black\"))\n",
    "        if 'y' in kwargs:\n",
    "            fig.update_layout(\n",
    "              yaxis = dict(\n",
    "                tickmode='array',\n",
    "                tickvals = kwargs['y'],\n",
    "                ticktext = kwargs['y'], \n",
    "                ),\n",
    "               font=dict(size=font_size, color=\"black\"))\n",
    "    plot_args = {\n",
    "        'width': 800,\n",
    "        'height': 600,\n",
    "        \"autosize\": False,\n",
    "        'showlegend': True,\n",
    "        'margin': {\"l\":0,\"r\":0,\"t\":100,\"b\":0}\n",
    "    }\n",
    "    \n",
    "    fig.update_layout(**plot_args)\n",
    "    fig.update_layout(legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    ))\n",
    "    if show:\n",
    "        fig.show(renderer)\n",
    "    else:\n",
    "        return fig\n",
    "\n",
    "\n",
    "# idea:\n",
    "# do in higher precision because things are so smol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c3e22ed-b7f6-4ed2-b73a-eda2b4e6c365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f87c22baaa0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# requires\n",
    "# pip install git+https://github.com/Phylliida/MambaLens.git\n",
    "\n",
    "from mamba_lens import HookedMamba # this will take a little while to import\n",
    "import torch\n",
    "model_path = \"state-spaces/mamba-370m\"\n",
    "model = HookedMamba.from_pretrained(model_path, device='cuda')\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1baabd77-6d6d-416c-95a7-8b3d5b314ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using patching format\n",
      "ABC AB C\n",
      "ABC AC B\n",
      "using templates\n",
      "Then, [NAME], [NAME] and [NAME] went to the [PLACE]. [NAME] and [NAME] gave a [OBJECT] to\n",
      "Afterwards [NAME], [NAME] and [NAME] went to the [PLACE]. [NAME] and [NAME] gave a [OBJECT] to\n",
      "Friends [NAME], [NAME] and [NAME] went to the [PLACE]. [NAME] and [NAME] gave a [OBJECT] to\n",
      "with name positions (2, 4, 6, 12, 14)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from acdc.data.ioi import ioi_data_generator, ABC_TEMPLATES, get_all_single_name_abc_patching_formats\n",
    "from acdc.data.utils import generate_dataset\n",
    "\n",
    "num_patching_pairs = 200\n",
    "seed = 27\n",
    "valid_seed = 28\n",
    "constrain_to_answers = True\n",
    "# this makes our data size 800, first 400 is each (a,b) pair, and then second 400 is each pair swapped to be (b,a)\n",
    "has_symmetric_patching = True\n",
    "\n",
    "from acdc.data.ioi import BABA_TEMPLATES\n",
    "templates = ABC_TEMPLATES\n",
    "patching_formats = list(get_all_single_name_abc_patching_formats())\n",
    "PATCHING_FORMAT_I = 3\n",
    "patching_formats = [patching_formats[PATCHING_FORMAT_I]]\n",
    "print(\"using patching format\")\n",
    "print(patching_formats[0])\n",
    "\n",
    "data = generate_dataset(model=model,\n",
    "                  data_generator=ioi_data_generator,\n",
    "                  num_patching_pairs=num_patching_pairs,\n",
    "                  seed=seed,\n",
    "                  valid_seed=valid_seed,\n",
    "                  constrain_to_answers=constrain_to_answers,\n",
    "                  has_symmetric_patching=has_symmetric_patching, \n",
    "                  varying_data_lengths=True,\n",
    "                  templates=templates,\n",
    "                  patching_formats=patching_formats)\n",
    "\n",
    "\n",
    "import acdc.data.ioi\n",
    "from collections import defaultdict\n",
    "name_positions_map = defaultdict(lambda: [])\n",
    "for template in templates:\n",
    "    name = acdc.data.ioi.good_names[0]\n",
    "    template_filled_in = template.replace(\"[NAME]\", name)\n",
    "    template_filled_in = template_filled_in.replace(\"[PLACE]\", acdc.data.ioi.good_nouns['[PLACE]'][0])\n",
    "    template_filled_in = template_filled_in.replace(\"[OBJECT]\", acdc.data.ioi.good_nouns['[OBJECT]'][0])\n",
    "    # get the token positions of the [NAME] in the prompt\n",
    "    name_positions = tuple([(i) for (i,s) in enumerate(model.to_str_tokens(torch.tensor(model.tokenizer.encode(template_filled_in)))) if s == f' {name}'])\n",
    "    name_positions_map[name_positions].append(template)\n",
    "sorted_by_frequency = sorted(list(name_positions_map.items()), key=lambda x: -len(x[1]))\n",
    "most_frequent_name_positions, templates = sorted_by_frequency[0]\n",
    "print(\"using templates\")\n",
    "for template in templates:\n",
    "    print(template)\n",
    "print(f\"with name positions {most_frequent_name_positions}\")\n",
    "\n",
    "data = generate_dataset(model=model,\n",
    "              data_generator=ioi_data_generator,\n",
    "              num_patching_pairs=num_patching_pairs,\n",
    "              seed=seed,\n",
    "              valid_seed=valid_seed,\n",
    "              constrain_to_answers=constrain_to_answers,\n",
    "              has_symmetric_patching=has_symmetric_patching, \n",
    "              varying_data_lengths=True,\n",
    "              templates=templates,\n",
    "              patching_formats=patching_formats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12f2588-c57e-4a9c-a41c-a26de661a26e",
   "metadata": {},
   "source": [
    "# Run Edge Attribution Patching with Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14fc6b9-7646-449e-ad90-9ddb600e9f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10\n",
      "alpha 0.0 metric 0.9999998211860657\n",
      "alpha 0.25 metric 0.2283603996038437\n",
      "alpha 0.5 metric 0.03497320041060448\n",
      "alpha 0.75 metric 0.002300843596458435\n",
      "alpha 1.0 metric 1.5766111971515784e-07\n",
      "10 20\n",
      "alpha 0.0 metric 0.9999997019767761\n",
      "alpha 0.25 metric 0.23043695092201233\n",
      "alpha 0.5 metric 0.03488878533244133\n",
      "alpha 0.75 metric 0.0023202230222523212\n",
      "alpha 1.0 metric 1.9314957455662807e-07\n",
      "20 30\n",
      "alpha 0.0 metric 0.999999463558197\n",
      "alpha 0.25 metric 0.24007251858711243\n",
      "alpha 0.5 metric 0.03611547872424126\n",
      "alpha 0.75 metric 0.0024828233290463686\n",
      "alpha 1.0 metric 1.8625785003223427e-07\n",
      "30 40\n",
      "alpha 0.0 metric 1.0\n",
      "alpha 0.25 metric 0.2276528924703598\n",
      "alpha 0.5 metric 0.03500666096806526\n",
      "alpha 0.75 metric 0.0024260811042040586\n",
      "alpha 1.0 metric 1.2417352479587862e-07\n",
      "40 50\n",
      "alpha 0.0 metric 0.9999995231628418\n",
      "alpha 0.25 metric 0.23611842095851898\n",
      "alpha 0.5 metric 0.03439648821949959\n",
      "alpha 0.75 metric 0.002206161618232727\n",
      "alpha 1.0 metric -2.999861976604734e-07\n",
      "50 60\n",
      "alpha 0.0 metric 1.0\n",
      "alpha 0.25 metric 0.2351134866476059\n",
      "alpha 0.5 metric 0.03535215184092522\n",
      "alpha 0.75 metric 0.002289998112246394\n",
      "alpha 1.0 metric 1.1041790770605076e-07\n",
      "60 70\n",
      "alpha 0.0 metric 1.0000003576278687\n",
      "alpha 0.25 metric 0.22115640342235565\n",
      "alpha 0.5 metric 0.03385640308260918\n",
      "alpha 0.75 metric 0.0021610085386782885\n",
      "alpha 1.0 metric 2.59213351228027e-07\n",
      "70 80\n",
      "alpha 0.0 metric 1.0\n",
      "alpha 0.25 metric 0.23621439933776855\n",
      "alpha 0.5 metric 0.03641856461763382\n",
      "alpha 0.75 metric 0.0024283563252538443\n",
      "alpha 1.0 metric -2.7594618146054017e-08\n",
      "80 90\n",
      "alpha 0.0 metric 0.9999993443489075\n",
      "alpha 0.25 metric 0.23232774436473846\n",
      "alpha 0.5 metric 0.03498367592692375\n",
      "alpha 0.75 metric 0.002329037291929126\n",
      "alpha 1.0 metric -3.0216901336643787e-07\n",
      "90 100\n",
      "alpha 0.0 metric 0.9999997019767761\n",
      "alpha 0.25 metric 0.22692056000232697\n",
      "alpha 0.5 metric 0.034787844866514206\n",
      "alpha 0.75 metric 0.002355177653953433\n",
      "alpha 1.0 metric 1.1453721526777372e-07\n",
      "100 110\n",
      "alpha 0.0 metric 1.0000004768371582\n",
      "alpha 0.25 metric 0.24167834222316742\n",
      "alpha 0.5 metric 0.03581797331571579\n",
      "alpha 0.75 metric 0.002356935292482376\n",
      "alpha 1.0 metric 1.3986552005462727e-07\n",
      "110 120\n",
      "alpha 0.0 metric 1.0\n",
      "alpha 0.25 metric 0.22428308427333832\n",
      "alpha 0.5 metric 0.0346512496471405\n",
      "alpha 0.75 metric 0.0022606279235333204\n",
      "alpha 1.0 metric -1.447806425858289e-07\n",
      "120 130\n",
      "alpha 0.0 metric 0.9999996423721313\n",
      "alpha 0.25 metric 0.24265427887439728\n",
      "alpha 0.5 metric 0.0371055006980896\n",
      "alpha 0.75 metric 0.0025240033864974976\n",
      "alpha 1.0 metric -1.3048054370301543e-07\n",
      "130 140\n",
      "alpha 0.0 metric 1.000000238418579\n",
      "alpha 0.25 metric 0.22973036766052246\n",
      "alpha 0.5 metric 0.035160813480615616\n",
      "alpha 0.75 metric 0.0024395708460360765\n",
      "alpha 1.0 metric -1.3783960639557336e-09\n",
      "140 150\n"
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "from functools import partial\n",
    "from jaxtyping import Float\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from mamba_lens.input_dependent_hooks import clean_hooks\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "model_kwargs = {\n",
    "    'fast_ssm': False,\n",
    "    'fast_conv': False\n",
    "}\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# removes all hooks including \"leftover\" ones that might stick around due to interrupting the model at certain times\n",
    "clean_hooks(model)\n",
    "\n",
    "def normalized_logit_diff_metric(patched_logits, unpatched_logits, corrupted_logits, patched_correct, corrupted_correct, also_return_acc=False):\n",
    "    B,V = patched_logits.size()\n",
    "\n",
    "    #print(data.unpatched.logits.size(), data.patched.logits.size(), data.corrupted.logits.size())\n",
    "    A_logits_unpatched = unpatched_logits[torch.arange(B), patched_correct]\n",
    "    A_logits_patched = patched_logits[torch.arange(B), patched_correct]\n",
    "    A_logits_corrupted = corrupted_logits[torch.arange(B), patched_correct]\n",
    "\n",
    "    B_logits_unpatched = unpatched_logits[torch.arange(B), corrupted_correct]\n",
    "    B_logits_patched = patched_logits[torch.arange(B), corrupted_correct]\n",
    "    B_logits_corrupted = corrupted_logits[torch.arange(B), corrupted_correct]\n",
    "\n",
    "    # A and B are two potential outputs\n",
    "    # if A patched > B patched, we are correct\n",
    "    # else we are incorrect\n",
    "\n",
    "    # thus we could just return A_logits_patched - B_logits_patched\n",
    "\n",
    "    # however it is useful to \"normalize\" these values\n",
    "\n",
    "    # in the worst case, our patching causes us to act like corrupted, and our diff will be\n",
    "    # A_logits_corrupted - B_logits_corrupted\n",
    "    # this will result in a small, negative value\n",
    "    \n",
    "    # in the best case, our patching will do nothing (cause us to act like unpatched), and our diff will be\n",
    "    # A_logits_unpatched - B_logits_unpatched\n",
    "    # this will result in a large, positive value\n",
    "\n",
    "    # thus we can treat those as the \"min\" and \"max\" and normalize accordingly\n",
    "    \n",
    "    min_diff = A_logits_corrupted - B_logits_corrupted\n",
    "    max_diff = A_logits_unpatched - B_logits_unpatched\n",
    "\n",
    "    # the abs ensures that if it's wrong we don't try and make it more wrong\n",
    "    possible_range = torch.abs(max_diff-min_diff)\n",
    "    possible_range[possible_range == 0] = 1.0 # prevent divide by zero\n",
    "    \n",
    "    diff = A_logits_patched - B_logits_patched\n",
    "    \n",
    "    normalized_diff = (diff-min_diff)/possible_range\n",
    "\n",
    "    # as described, 1.0 corresponds to acting like unpatched,\n",
    "    # and 0.0 corresponds to acting like corrupted\n",
    "    res = torch.mean(normalized_diff)\n",
    "    \n",
    "    if also_return_acc:\n",
    "        num_correct = A_logits_patched > B_logits_patched\n",
    "        acc = torch.sum(num_correct)/B\n",
    "        return res, acc\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "\n",
    "# there's a subtle bug if you aren't careful:\n",
    "# consider what happens when we do edge attribution patching and patch every edge\n",
    "# what we want to happen is that it's identical to corrupted\n",
    "# however this is not what happens!\n",
    "# Start with layer 0:\n",
    "# layer 0 will be patched\n",
    "#    we subtract uncorrupted embed and add corrupted embed\n",
    "#    in other words, the embed input to layer 0 will be from the corrupted run\n",
    "# this results in layer 0 having an output of corrupted, as desired\n",
    "# now consider layer 1\n",
    "#    subtract uncorrupted embed and add corrupted embed\n",
    "#      this is fine and results in embed input to layer 1 of corrupted\n",
    "#    subtract uncorrupted layer 0 and add corrupted layer 0\n",
    "#      layer 0 is already corrupted, so this has the effect of the output of layer 0 being\n",
    "#          2*corrupted layer 0 - uncorrupted layer 0\n",
    "#          this is not the same as corrupted!\n",
    "\n",
    "# two ways to fix this:\n",
    "# 1. fetch stored layer_input from uncorrupted run, and use that instead of the layer_input given in fwd_diff\n",
    "#   this works, but then the gradients won't propogate properly (maybe? todo: test)\n",
    "# 2. mark which edges are patched and don't \"double patch\" them\n",
    "#   if we are patching all edges, this means that we simply apply only the embed diff to all layers,\n",
    "#   as that'll result in all layers being patched\n",
    "# 3. just subtract the outputs in the same forward pass, instead of a cached \"unpatched\" run\n",
    "#  we do 3\n",
    "\n",
    "global alpha\n",
    "alpha = 1\n",
    "\n",
    "def cache_output_hook( # hook_layer_output\n",
    "    layer_output : Float[torch.Tensor, \"B L D\"],\n",
    "    hook : HookPoint,\n",
    "    layer : int,\n",
    "    cached_outputs : Float[torch.Tensor, \"NLayers+1 B L D\"]):\n",
    "    global alpha\n",
    "    cached_outputs[layer+1] = layer_output\n",
    "    return layer_output\n",
    "\n",
    "def fwd_diff_hook( # hook_layer_input\n",
    "    layer_input : Float[torch.Tensor, \"B L D\"],\n",
    "    hook : HookPoint,\n",
    "    layer : int,\n",
    "    cached_outputs : Float[torch.Tensor, \"NLayers+1 B L D\"],\n",
    "    corrupted_outputs : Float[torch.Tensor, \"NLayers+1 B L D\"]):\n",
    "    global alpha\n",
    "    return layer_input + (-cached_outputs[:layer+1]+corrupted_outputs[:layer+1]).sum(dim=0)*alpha\n",
    "\n",
    "def bwd_diff_hook( # hook_layer_input\n",
    "    grad : Float[torch.Tensor, \"B L D\"],\n",
    "    hook: HookPoint,\n",
    "    layer : int,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    cached_outputs : Float[torch.Tensor, \"NLayers+1 B L D\"],\n",
    "    corrupted_outputs : Float[torch.Tensor, \"NLayers+1 B L D\"]):\n",
    "    #print(f\"running bwd with alpha {alpha} and layer {layer}\") \n",
    "    \n",
    "    # [N_upstream, B, L, D]\n",
    "    upstream_diffs = (-cached_outputs[:layer+1]+corrupted_outputs[:layer+1])\n",
    "    # grad is [B,L,D]\n",
    "\n",
    "    # to do a taylor approximation of metric with respect to diff_0, we\n",
    "    # multiply diffs and grad, then\n",
    "    # sum over the L and D dimensions (this is doing a dot product of vectors of size L*D)\n",
    "    # now we have attr of size [N_upstream, B, L]\n",
    "    attr = (grad*upstream_diffs).sum(dim=-1)\n",
    "    # [B, N_upstream, L]           [N_upstream, B, L]\n",
    "    attr         =   torch.transpose(attr, 0, 1)\n",
    "    # [B, N_upstream, L]\n",
    "    attr = attr.clone().detach()\n",
    "    if POSITIONS:\n",
    "        attributions[batch_start:batch_end,:layer+1,layer+1] += attr\n",
    "    else:\n",
    "        #[B, N_upstream]\n",
    "        attr = attr.sum(dim=-1)\n",
    "        attributions[batch_start:batch_end,:layer+1,layer+1] += attr\n",
    "\n",
    "\n",
    "\n",
    "def cache_conv_hook(\n",
    "    conv_input: Float[torch.Tensor, \"B L E\"],\n",
    "    hook: HookPoint,\n",
    "    layer: int,\n",
    "    cached_conv_inputs : Float[torch.Tensor, \"NLayers B L E\"]):\n",
    "    cached_conv_inputs.append(conv_input)\n",
    "    return conv_input\n",
    "\n",
    "# for each conv slice,\n",
    "# we have \n",
    "global filter_terms\n",
    "def conv_patching_hook(\n",
    "    conv_output: Float[torch.Tensor, \"B L E\"],\n",
    "    hook: HookPoint,\n",
    "    layer: int,\n",
    "    cached_conv_inputs : Float[torch.Tensor, \"NLayers B L E\"],\n",
    "    corrupted_conv_inputs : Float[torch.Tensor, \"NLayers B L E\"],\n",
    "    slice_terms : [],\n",
    ") -> Float[torch.Tensor, \"B L E\"]:\n",
    "    ### This is identical to what the conv is doing\n",
    "    # but we break it apart so we can patch on individual slices\n",
    "\n",
    "    # we have two input hooks, the second one is the one we want\n",
    "    D_CONV = model.cfg.d_conv\n",
    "\n",
    "    # [E,1,D_CONV]\n",
    "    conv_weight = model.blocks[layer].conv1d.weight\n",
    "    # [E]\n",
    "    conv_bias = model.blocks[layer].conv1d.bias\n",
    "    uncorrupted_input = rearrange(cached_conv_inputs[layer], 'B L E -> B E L')\n",
    "    corrupted_input = rearrange(corrupted_conv_inputs[layer], 'B L E -> B E L')\n",
    "    B,E,L = uncorrupted_input.size()\n",
    "    uncorrupted_input = torch.nn.functional.pad(uncorrupted_input, (D_CONV-1,0), mode='constant', value=0)\n",
    "    corrupted_input = torch.nn.functional.pad(corrupted_input, (D_CONV-1,0), mode='constant', value=0)\n",
    "    #print(uncorrupted_input.size(), corrupted_input.size())\n",
    "    # we want some \"patch hook\" thing that uses autodiff or somethin\n",
    "    output = torch.zeros([B,E,L], device=model.cfg.device)\n",
    "    global alpha\n",
    "    layer_slices = []\n",
    "    for i in range(D_CONV):\n",
    "        # [B,E,L]                      [E,1]                      [B,E,L]\n",
    "        uncorrupted_contribution = conv_weight[:,0,i].view(E,1)*uncorrupted_input[:,:,i:i+L]\n",
    "        corrupted_contribution = conv_weight[:,0,i].view(E,1)*corrupted_input[:,:,i:i+L]\n",
    "        slice_term = uncorrupted_contribution*(1-alpha)+corrupted_contribution*(alpha)\n",
    "        diff = -uncorrupted_contribution+corrupted_contribution\n",
    "        slice_term.retain_grad()\n",
    "        #slice_term.requires_grad = True\n",
    "        layer_slices.append((diff, slice_term))\n",
    "        output += slice_term\n",
    "    output = rearrange(output, 'B E L -> B L E')\n",
    "    output += conv_bias\n",
    "    slice_terms.append(layer_slices)\n",
    "    return output\n",
    "\n",
    "def bwd_conv_hook(\n",
    "    grad : Float[torch.Tensor, \"B L E\"], # we don't use this, hook is just so we are present in backward pass\n",
    "    hook: HookPoint,\n",
    "    layer : int,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    cached_conv_inputs : Float[torch.Tensor, \"NLayers B L E\"],\n",
    "    corrupted_conv_inputs : Float[torch.Tensor, \"NLayers B L E\"],\n",
    "    slice_terms: list # one list for each layer, each list contains one tensor for each conv slice\n",
    "):\n",
    "    # [B, L, E]\n",
    "    #diffs = (-cached_conv_inputs[layer]+corrupted_conv_inputs[layer])\n",
    "    layer_slice_terms = slice_terms[layer]\n",
    "    for slice_i, (diff, slice_term) in enumerate(layer_slice_terms):\n",
    "        # [B,E,L]\n",
    "        slice_grad = slice_term.grad\n",
    "        # dot product over the E dimension\n",
    "        # [B,L]         [B,E,L]   [B,E,L]\n",
    "        slice_attr = (slice_grad * diff).sum(dim=-2) # sum over E dimension\n",
    "        if POSITIONS:\n",
    "            # [B,L]\n",
    "            conv_attributions[batch_start:batch_end,layer,slice_i] += slice_attr \n",
    "        else:\n",
    "            #[B]\n",
    "            slice_attr = slice_attr.sum(dim=-1)\n",
    "            conv_attributions[batch_start:batch_end,layer,slice_i] += slice_attr\n",
    "\n",
    "B,L = data.data.size()\n",
    "# our data is pairs of unpatched, corrupted\n",
    "n_patching_pairs = B//2\n",
    "D_CONV = model.cfg.d_conv\n",
    "\n",
    "# attributions[b,i+1,j+1] is the (i->j) edge attribution for patching pair b\n",
    "# attributions[b,0,j] is the (embed->j) edge attribution for patching pair b\n",
    "# attributions[b,i,-1] is the (i->output) edge attribution for patching pair b\n",
    "POSITIONS = True\n",
    "if POSITIONS:\n",
    "    attributions = torch.zeros([n_patching_pairs, model.cfg.n_layers+2, model.cfg.n_layers+2, L], device=model.cfg.device)\n",
    "    conv_attributions = torch.zeros([n_patching_pairs, model.cfg.n_layers, D_CONV, L], device=model.cfg.device)\n",
    "else:\n",
    "    attributions = torch.zeros([n_patching_pairs, model.cfg.n_layers+2, model.cfg.n_layers+2], device=model.cfg.device)\n",
    "    conv_attributions = torch.zeros([n_patching_pairs, model.cfg.n_layers, D_CONV], device=model.cfg.device)\n",
    "\n",
    "\n",
    "\n",
    "input_names = [f'blocks.{i}.hook_layer_input' for i in range(model.cfg.n_layers)]\n",
    "output_names = [f'blocks.{i}.hook_out_proj' for i in range(model.cfg.n_layers)]\n",
    "conv_input_names = [f'blocks.{i}.hook_in_proj' for i in range(model.cfg.n_layers)]\n",
    "conv_names = [f'blocks.{i}.hook_conv' for i in range(model.cfg.n_layers)]\n",
    "\n",
    "for batch_start in range(0, n_patching_pairs, BATCH_SIZE):\n",
    "    batch_end = min(batch_start + BATCH_SIZE, n_patching_pairs)\n",
    "    print(batch_start, batch_end)\n",
    "    # we don't need grad for these forward passes\n",
    "    torch.set_grad_enabled(False)\n",
    "    embed_name = 'hook_embed'\n",
    "\n",
    "    clean_hooks(model)\n",
    "    # forward passes to get unpatched and corrupted\n",
    "    unpatched_logits, unpatched_layer_outputs = model.run_with_cache(data.data[::2][batch_start:batch_end], names_filter=[embed_name] + output_names + conv_input_names, **model_kwargs)\n",
    "    corrupted_logits, corrupted_layer_outputs = model.run_with_cache(data.data[1::2][batch_start:batch_end], names_filter=[embed_name] + output_names + conv_input_names, **model_kwargs)\n",
    "    \n",
    "    batch_size,L,D = unpatched_layer_outputs[output_names[0]].size()\n",
    "    _,_,E = unpatched_layer_outputs[conv_input_names[0]].size()\n",
    "    \n",
    "    # get only the last token position (logit for next predicted token)\n",
    "    # this is needed to support data of varying lengths\n",
    "    unpatched_logits = unpatched_logits[torch.arange(batch_size), data.last_token_position[::2][batch_start:batch_end]]\n",
    "    corrupted_logits = corrupted_logits[torch.arange(batch_size), data.last_token_position[1::2][batch_start:batch_end]]\n",
    "    \n",
    "    clean_hooks(model)\n",
    "\n",
    "    # backward pass to compute grad of diff\n",
    "    torch.set_grad_enabled(True)\n",
    "        \n",
    "    corrupted_outputs = torch.zeros([model.cfg.n_layers+1,batch_size,L,D], device=model.cfg.device)\n",
    "    corrupted_conv_inputs = torch.zeros([model.cfg.n_layers,batch_size,L,E], device=model.cfg.device)\n",
    "    corrupted_outputs.requires_grad = False\n",
    "    # first one is for embed\n",
    "    corrupted_outputs[0] = corrupted_layer_outputs[embed_name]\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        output_name = output_names[layer]\n",
    "        corrupted_outputs[layer+1] = corrupted_layer_outputs[output_name]\n",
    "        corrupted_conv_inputs[layer] = corrupted_layer_outputs[conv_input_names[layer]]    \n",
    "    # cleanup\n",
    "    del corrupted_layer_outputs\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "        param.grad = None # reset grads\n",
    "    \n",
    "    last_layer = model.cfg.n_layers-1\n",
    "    # forward pass to do partial patches\n",
    "    cached_outputs = torch.zeros([model.cfg.n_layers+1,batch_size,L,D], device=model.cfg.device)\n",
    "    cached_outputs.requires_grad = False\n",
    "    \n",
    "    # EAP layer hooks\n",
    "    cache_output_hooks = [(embed_name,\n",
    "                           partial(cache_output_hook,\n",
    "                                   layer=-1,\n",
    "                                   cached_outputs=cached_outputs))]\n",
    "    \n",
    "    cache_output_hooks += [(output_names[layer],\n",
    "                            partial(cache_output_hook,\n",
    "                                    layer=layer,\n",
    "                                    cached_outputs=cached_outputs)) for layer in range(model.cfg.n_layers)]\n",
    "    \n",
    "    fwd_hooks = cache_output_hooks\n",
    "    fwd_hooks += [(input_names[layer],\n",
    "                  partial(\n",
    "                      fwd_diff_hook,\n",
    "                      layer=layer,\n",
    "                      cached_outputs=cached_outputs,\n",
    "                      corrupted_outputs=corrupted_outputs,\n",
    "                  )) for layer in range(model.cfg.n_layers)]\n",
    "    bwd_hooks = [(input_names[layer],\n",
    "                  partial(bwd_diff_hook,\n",
    "                          layer=layer,\n",
    "                          cached_outputs=cached_outputs,\n",
    "                          corrupted_outputs=corrupted_outputs,\n",
    "                          batch_start=batch_start,\n",
    "                          batch_end=batch_end)) for layer in range(model.cfg.n_layers)]\n",
    "    # extra hook for the very last layer\n",
    "    fwd_hooks.append((f'blocks.{last_layer}.hook_resid_post',\n",
    "                      partial(fwd_diff_hook,\n",
    "                              layer=last_layer+1,\n",
    "                              cached_outputs=cached_outputs,\n",
    "                              corrupted_outputs=corrupted_outputs,\n",
    "                             )))\n",
    "    bwd_hooks.append((f'blocks.{last_layer}.hook_resid_post',\n",
    "                      partial(bwd_diff_hook,\n",
    "                              layer=last_layer+1,\n",
    "                              cached_outputs=cached_outputs,\n",
    "                              corrupted_outputs=corrupted_outputs,\n",
    "                              batch_start=batch_start,\n",
    "                              batch_end=batch_end,\n",
    "                             )))\n",
    "\n",
    "    # Conv hooks\n",
    "    cached_conv_inputs = []\n",
    "    slice_terms = []\n",
    "\n",
    "    fwd_hooks += [(conv_input_names[layer],\n",
    "                  partial(\n",
    "                      cache_conv_hook,\n",
    "                      layer=layer,\n",
    "                      cached_conv_inputs=cached_conv_inputs,\n",
    "                  )) for layer in range(model.cfg.n_layers)]\n",
    "    \n",
    "    \n",
    "    fwd_hooks += [(conv_names[layer],\n",
    "                  partial(\n",
    "                      conv_patching_hook,\n",
    "                      layer=layer,\n",
    "                      cached_conv_inputs=cached_conv_inputs,\n",
    "                      corrupted_conv_inputs=corrupted_conv_inputs,\n",
    "                      slice_terms=slice_terms,\n",
    "                  )) for layer in range(model.cfg.n_layers)]\n",
    "    bwd_hooks += [(input_names[layer], # anywhere before the slice terms works, so we'll just pick start of layer since that backward is called\n",
    "                  partial(\n",
    "                      bwd_conv_hook,\n",
    "                      layer=layer,\n",
    "                      cached_conv_inputs=cached_conv_inputs,\n",
    "                      corrupted_conv_inputs=corrupted_conv_inputs,\n",
    "                      slice_terms=slice_terms,\n",
    "                      batch_start=batch_start,\n",
    "                      batch_end=batch_end,\n",
    "                  )) for layer in range(model.cfg.n_layers)]\n",
    "    \n",
    "    for fwd in fwd_hooks:\n",
    "        model.add_hook(*fwd, \"fwd\")\n",
    "\n",
    "    for bwd in bwd_hooks:\n",
    "        model.add_hook(*bwd, \"bwd\")\n",
    "    \n",
    "    # with integrated gradients\n",
    "    # simply sums over doing \"partial patches\" like 0.2 patch and 0.8 unpatched \n",
    "    # ITERS = 1 is just edge attribution patching (without integraded gradients)\n",
    "    ITERS = 5\n",
    "    for i in range(ITERS+1):\n",
    "        global alpha\n",
    "        # alpha ranges from 0 to 1\n",
    "        if ITERS > 1:\n",
    "            alpha = i/float(ITERS-1)\n",
    "        elif ITERS == 1: # no integrated gradients, set alpha to 1\n",
    "            alpha = 1.0\n",
    "\n",
    "        # it tries to propogate gradients to these, detach them\n",
    "        slice_terms.clear()\n",
    "        cached_conv_inputs.clear()\n",
    "        torch.cuda.empty_cache()\n",
    "        cached_outputs[:] = 0\n",
    "        cached_outputs.grad = None\n",
    "        cached_outputs.detach_()\n",
    "        corrupted_outputs.grad = None\n",
    "        corrupted_outputs.detach_()\n",
    "        #cached_conv_inputs.grad = None\n",
    "        #cached_conv_inputs.detach_()\n",
    "        corrupted_conv_inputs.grad = None\n",
    "        corrupted_conv_inputs.detach_()\n",
    "        model.zero_grad()\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        if i == ITERS:\n",
    "            torch.cuda.empty_cache()\n",
    "            break # we just use this for cleaning up\n",
    "        logits = model(data.data[::2][batch_start:batch_end], **model_kwargs)\n",
    "        logits = logits[torch.arange(batch_size), data.last_token_position[::2][batch_start:batch_end]]\n",
    "        metric = normalized_logit_diff_metric(\n",
    "            patched_logits=logits,\n",
    "            unpatched_logits=unpatched_logits,\n",
    "            corrupted_logits=corrupted_logits,\n",
    "            patched_correct=data.correct[::2][batch_start:batch_end][:,0],\n",
    "            corrupted_correct=data.correct[1::2][batch_start:batch_end][:,0]\n",
    "        )\n",
    "        print(f\"alpha {alpha} metric {metric}\")\n",
    "        # run backward pass, which adds to attributions\n",
    "        metric.backward()\n",
    "        #conv_attrs = conv_attributions.mean(dim=0)\n",
    "        #print(conv_attrs)\n",
    "        #attrs = attributions.mean(dim=0)\n",
    "        #print(attrs)\n",
    "\n",
    "# todo: maybe the diffs should have alpha in the backward pass? No, that would mean alpha of 0 gives all zero attrs\n",
    "\n",
    "# average over all the samples\n",
    "attributions[:] = attributions[:]/ITERS\n",
    "conv_attributions[:] = conv_attributions[:]/ITERS\n",
    "\n",
    "\n",
    "# don't need grad for rest of this\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "clean_hooks(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab16482e-a3a9-4985-99ad-e7fff25d0b3a",
   "metadata": {},
   "source": [
    "# Binary search to find circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de617a-4a2f-4154-88bc-fef42263d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save([conv_attributions, attributions], \"patching3attrs.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d041cde-732a-4989-9156-f3f7a7fda77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_attributions, attributions = torch.load(\"patching3attrs.pkl\")\n",
    "model_kwargs = {\n",
    "    'fast_ssm': False,\n",
    "    'fast_conv': False\n",
    "}\n",
    "import sys\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import wandb\n",
    "import os\n",
    "import signal\n",
    "import acdc\n",
    "from tqdm import tqdm\n",
    "from typing import Any  \n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from acdc import (\n",
    "    Edge,\n",
    "    ACDCConfig,\n",
    "    LOG_LEVEL_INFO,\n",
    "    LOG_LEVEL_DEBUG,\n",
    "    run_acdc,\n",
    "    ACDCEvalData,\n",
    "    load_checkpoint,\n",
    "    get_most_recent_checkpoint\n",
    ")\n",
    "\n",
    "\n",
    "from acdc import get_pad_token\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "global storage\n",
    "def storage_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    **kwargs,\n",
    "):\n",
    "    global storage\n",
    "    #if hook.name == 'hook_embed':\n",
    "    #    for k in list(storage.keys()):\n",
    "    #        del storage[k]\n",
    "    storage[hook.name] = x\n",
    "    return x\n",
    "\n",
    "def resid_patching_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    input_hook_name: str,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    position: int = None,\n",
    "):\n",
    "    global storage\n",
    "    x_uncorrupted = storage[input_hook_name][batch_start:batch_end:2]\n",
    "    x_corrupted = storage[input_hook_name][batch_start+1:batch_end:2]\n",
    "    if position is None: # if position not specified, apply to all positions\n",
    "        x[batch_start:batch_end:2] = x[batch_start:batch_end:2] - x_uncorrupted + x_corrupted\n",
    "    else:\n",
    "        x[batch_start:batch_end:2,position] = x[batch_start:batch_end:2,position] - x_uncorrupted[:,position] + x_corrupted[:,position]\n",
    "    return x\n",
    "\n",
    "def overwrite_patching_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    input_hook_name: str,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    position: int = None,\n",
    "):\n",
    "    x_corrupted = x[batch_start+1:batch_end:2]\n",
    "    if position is None: # if position not specified, apply to all positions\n",
    "        x[batch_start:batch_end:2] = x_corrupted\n",
    "    else:\n",
    "        if x_corrupted.size()[1] != L: raise ValueError(f'warning: in hook {hook.name} with input_hook_name {input_hook_name} you are patching on position in the second index but size is {x_corrupted.size()}')\n",
    "        x[batch_start:batch_end:2,position] = x_corrupted[:,position]\n",
    "    return x\n",
    "\n",
    "\n",
    "def overwrite_h_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    input_hook_name: str,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    position: int = None,\n",
    "):\n",
    "    x[batch_start:batch_end:2] = x[batch_start+1:batch_end:2]\n",
    "    return x\n",
    "\n",
    "# we do a hacky thing where this first hook clears the global storage\n",
    "# second hook stores all the hooks\n",
    "# then third hook computes the output (over all the hooks)\n",
    "# this avoids recomputing and so is much faster\n",
    "CONV_HOOKS = \"conv hooks\"\n",
    "CONV_BATCHES = \"conv batches\"\n",
    "def conv_patching_init_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    **kwargs\n",
    "):\n",
    "    # we need to clear this here\n",
    "    # i tried having a \"current layer\" variable in the conv_storage that only clears when it doesn't match\n",
    "    # but that doesn't work if you only patch the same layer over and over,\n",
    "    # as stuff gets carried over\n",
    "    # this way of doing things is much safer and lets us assume it'll be empty\n",
    "    # well not quite, note that conv_patching_hook will be called with different batch_start and batch_end inputs during one forward pass\n",
    "    # so we need to account for that in the keys we use\n",
    "    global conv_storage\n",
    "    conv_storage = {CONV_BATCHES: set()}\n",
    "    return x\n",
    "\n",
    "# hook h has a weird index!!!!!\n",
    "\n",
    "def conv_patching_storage_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    conv_filter_i: int,\n",
    "    position: int,\n",
    "    layer: int,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    **kwargs,\n",
    "):\n",
    "    global storage\n",
    "    storage[hook.name] = x\n",
    "    global conv_storage\n",
    "    hooks_key = (CONV_HOOKS, batch_start, batch_end)\n",
    "    if not hooks_key in conv_storage:\n",
    "        conv_storage[hooks_key] = [] # we can't do this above because it'll be emptied again on the next batch before this is called\n",
    "    conv_storage[hooks_key].append({\"position\": position, \"conv_filter_i\": conv_filter_i})\n",
    "    conv_storage[CONV_BATCHES].add((batch_start, batch_end))\n",
    "    return x\n",
    "\n",
    "from jaxtyping import Float\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "global storage_for_grad\n",
    "\n",
    "global conv_storage\n",
    "def conv_patching_hook(\n",
    "    conv_output: Float[torch.Tensor, \"B L E\"],\n",
    "    hook: HookPoint,\n",
    "    input_hook_name: str,\n",
    "    layer: int,\n",
    "    **kwargs,\n",
    ") -> Float[torch.Tensor, \"B L E\"]:\n",
    "    global conv_storage\n",
    "    global storage\n",
    "    ### This is identical to what the conv is doing\n",
    "    # but we break it apart so we can patch on individual filters\n",
    "\n",
    "    # we have two input hooks, the second one is the one we want\n",
    "    input_hook_name = input_hook_name[1]\n",
    "    \n",
    "    D_CONV = model.cfg.d_conv\n",
    "\n",
    "    # given batches like [(2,4), (5,6)] and total size 7 this returns (0,2), (4,5), (6,7) \n",
    "    def get_missing_batches(batches, B):\n",
    "        covered_i = torch.zeros([B])\n",
    "        for batch_start, batch_end in batches:\n",
    "            covered_i[batch_start:batch_end] = 1\n",
    "    \n",
    "        missing_batches = []\n",
    "        missing_start = 0\n",
    "        for i in range(B):\n",
    "            if covered_i[i] == 1:\n",
    "                if i != missing_start:\n",
    "                    missing_batches.append((missing_start, i))\n",
    "                missing_start = i+1\n",
    "        if covered_i[B-1] == 0:\n",
    "            missing_batches.append((missing_start, B))\n",
    "        return missing_batches\n",
    "    \n",
    "\n",
    "    \n",
    "    # [E,1,D_CONV]\n",
    "    conv_weight = model.blocks[layer].conv1d.weight\n",
    "    # [E]\n",
    "    conv_bias = model.blocks[layer].conv1d.bias\n",
    "    \n",
    "    # don't recompute these if we don't need to\n",
    "    # because we stored all the hooks and batches in conv_storage, we can just do them all at once\n",
    "    output_key = f'output' # they need to share an output because they write to the same output tensor\n",
    "    if not output_key in conv_storage:\n",
    "        #print(\"layer\", layer, \"keys\", conv_storage)\n",
    "        apply_to_all_hooks = [] # this is important because otherwise the [0:None] would overwrite the previous results (or vice versa)\n",
    "        apply_to_all_key = (CONV_HOOKS, 0, None)\n",
    "        if apply_to_all_key in conv_storage:\n",
    "            apply_to_all_hooks = conv_storage[apply_to_all_key]\n",
    "            # we need to do this so it applies to the other batches that we aren't otherwise patching\n",
    "            for batch_start, batch_end in get_missing_batches(conv_storage[CONV_BATCHES], conv_output.size()[0]):\n",
    "                conv_storage[CONV_BATCHES].add(batch_start, batch_end)\n",
    "                conv_storage[(CONV_HOOKS, batch_start, batch_end)] = []\n",
    "        for batch_start, batch_end in conv_storage[CONV_BATCHES]:\n",
    "            if batch_start == 0 and batch_end == None: continue # we cover this in the apply to all hooks above\n",
    "            def get_filter_key(i):\n",
    "                return f'filter_{i}'\n",
    "            conv_input_uncorrupted = storage[input_hook_name][batch_start:batch_end:2]\n",
    "            conv_input_corrupted = storage[input_hook_name][batch_start+1:batch_end:2]\n",
    "            B, L, E = conv_input_uncorrupted.size()\n",
    "            \n",
    "            conv_input_uncorrupted = rearrange(conv_input_uncorrupted, 'B L E -> B E L')\n",
    "            conv_input_corrupted = rearrange(conv_input_corrupted, 'B L E -> B E L')\n",
    "            \n",
    "            # pad zeros in front\n",
    "            # [B,E,D_CONV-1+L]\n",
    "            padded_input_uncorrupted = torch.nn.functional.pad(conv_input_uncorrupted, (D_CONV-1,0), mode='constant', value=0)\n",
    "            padded_input_corrupted = torch.nn.functional.pad(conv_input_corrupted, (D_CONV-1,0), mode='constant', value=0)\n",
    "    \n",
    "            # compute the initial filter values\n",
    "            for i in range(D_CONV):\n",
    "                filter_key = get_filter_key(i)\n",
    "                # [B,E,L]                      [E,1]                      [B,E,L]\n",
    "                filter_contribution = conv_weight[:,0,i].view(E,1)*padded_input_uncorrupted[:,:,i:i+L]\n",
    "                conv_storage[filter_key] = filter_contribution\n",
    "            \n",
    "            # apply all the hooks\n",
    "            for hook in conv_storage[(CONV_HOOKS, batch_start, batch_end)] + apply_to_all_hooks:\n",
    "                position = hook['position']\n",
    "                conv_filter_i = hook['conv_filter_i']\n",
    "                #print(f\"position {position} conv_filter_i {conv_filter_i} batch_start {batch_start} batch_end {batch_end}\")\n",
    "                filter_key = get_filter_key(conv_filter_i)\n",
    "                # [1,E,L]                                   [E,1]                          # [B,E,L]\n",
    "                corrupted_filter_contribution = conv_weight[:,0,conv_filter_i].view(E,1)*padded_input_corrupted[:,:,conv_filter_i:conv_filter_i+L]\n",
    "                filter_contribution = conv_storage[filter_key]\n",
    "                if position is None:\n",
    "                    # [B,E,L]                    [B,E,L]\n",
    "                    filter_contribution = corrupted_filter_contribution\n",
    "                else:\n",
    "                    # [B,E]                                                  [B,E]\n",
    "                    filter_contribution[:,:,position] = corrupted_filter_contribution[:,:,position]\n",
    "                conv_storage[filter_key] = filter_contribution\n",
    "            \n",
    "            # compute the output\n",
    "            output = sum([conv_storage[get_filter_key(i)] for i in range(D_CONV)])\n",
    "            #output = torch.zeros([B,E,L], device=model.cfg.device)\n",
    "            #print(f'B {B} B2 {B2} E {E} L {L} conv_storage keys {conv_storage.keys()} filter sizes {[(k,v.size()) for (k,v) in conv_storage.items() if not type(v) is int]}')\n",
    "            for i in range(D_CONV):\n",
    "                filter_key = get_filter_key(i)\n",
    "                #output += conv_storage[filter_key]\n",
    "                del conv_storage[filter_key] # clean up now we are done with it, just to be safe\n",
    "                \n",
    "            # bias is not dependent on input so no reason to patch on it, just apply it as normal\n",
    "            output += conv_bias.view(E, 1)\n",
    "            output = rearrange(output, 'B E L -> B L E')\n",
    "            # interleave it back with the corrupted as every other\n",
    "            conv_output[batch_start:batch_end:2] = output\n",
    "        conv_storage[output_key] = conv_output\n",
    "    return conv_storage[output_key]\n",
    "\n",
    "\n",
    "limited_layers = list(range(model.cfg.n_layers))\n",
    "\n",
    "\n",
    "from acdc.data.ioi import ioi_data_generator, ABC_TEMPLATES, get_all_single_name_abc_patching_formats\n",
    "from acdc.data.utils import generate_dataset\n",
    "\n",
    "num_patching_pairs = 200\n",
    "seed = 27\n",
    "valid_seed = 28\n",
    "constrain_to_answers = True\n",
    "# this makes our data size 800, first 400 is each (a,b) pair, and then second 400 is each pair swapped to be (b,a)\n",
    "has_symmetric_patching = True\n",
    "\n",
    "from acdc.data.ioi import BABA_TEMPLATES\n",
    "templates = ABC_TEMPLATES\n",
    "'''\n",
    "patching_formats = list(get_all_single_name_abc_patching_formats())\n",
    "patching\n",
    "# generate data once to populate good_names and good_nouns (holds single-token choices)\n",
    "data = generate_dataset(model=model,\n",
    "                  data_generator=ioi_data_generator,\n",
    "                  num_patching_pairs=num_patching_pairs,\n",
    "                  seed=seed,\n",
    "                  valid_seed=valid_seed,\n",
    "                  constrain_to_answers=constrain_to_answers,\n",
    "                  has_symmetric_patching=has_symmetric_patching, \n",
    "                  varying_data_lengths=True,\n",
    "                  templates=templates,\n",
    "                  patching_formats=patching_formats)\n",
    "'''\n",
    "import acdc.data.ioi\n",
    "from collections import defaultdict\n",
    "name_positions_map = defaultdict(lambda: [])\n",
    "for template in templates:\n",
    "    name = acdc.data.ioi.good_names[0]\n",
    "    template_filled_in = template.replace(\"[NAME]\", name)\n",
    "    template_filled_in = template_filled_in.replace(\"[PLACE]\", acdc.data.ioi.good_nouns['[PLACE]'][0])\n",
    "    template_filled_in = template_filled_in.replace(\"[OBJECT]\", acdc.data.ioi.good_nouns['[OBJECT]'][0])\n",
    "    # get the token positions of the [NAME] in the prompt\n",
    "    name_positions = tuple([(i) for (i,s) in enumerate(model.to_str_tokens(torch.tensor(model.tokenizer.encode(template_filled_in)))) if s == f' {name}'])\n",
    "    name_positions_map[name_positions].append(template)\n",
    "sorted_by_frequency = sorted(list(name_positions_map.items()), key=lambda x: -len(x[1]))\n",
    "most_frequent_name_positions, templates = sorted_by_frequency[0]\n",
    "print(\"using templates\")\n",
    "for template in templates:\n",
    "    print(template)\n",
    "print(f\"with name positions {most_frequent_name_positions}\")\n",
    "\n",
    "data = generate_dataset(model=model,\n",
    "              data_generator=ioi_data_generator,\n",
    "              num_patching_pairs=num_patching_pairs,\n",
    "              seed=seed,\n",
    "              valid_seed=valid_seed,\n",
    "              constrain_to_answers=constrain_to_answers,\n",
    "              has_symmetric_patching=has_symmetric_patching, \n",
    "              varying_data_lengths=True,\n",
    "              templates=templates,\n",
    "              patching_formats=patching_formats)\n",
    "\n",
    "\n",
    "## Setup edges for ACDC\n",
    "edges = []\n",
    "B,L = data.data.size()\n",
    "#positions = list(range(L)) # \n",
    "\n",
    "if POSITIONS:\n",
    "    positions = list(range(L))\n",
    "else:\n",
    "    positions = [None]\n",
    "\n",
    "INPUT_HOOK = f'hook_embed'\n",
    "INPUT_NODE = 'embed'\n",
    "\n",
    "last_layer = model.cfg.n_layers-1\n",
    "OUTPUT_HOOK = f'blocks.{last_layer}.hook_resid_post'\n",
    "OUTPUT_NODE = 'output'\n",
    "\n",
    "def input(layer):\n",
    "    return f'{layer}.i'\n",
    "\n",
    "def output(layer):\n",
    "    return f'{layer}.o'\n",
    "\n",
    "def conv(layer):\n",
    "    return f'{layer}.conv'\n",
    "\n",
    "def skip(layer):\n",
    "    return f'{layer}.skip'\n",
    "\n",
    "def ssm(layer):\n",
    "    return f'{layer}.ssm'\n",
    "\n",
    "# important to have storage be global and not passed into the hooks! Otherwise it gets very slow (tbh, i don't know why)\n",
    "global storage\n",
    "storage = {}\n",
    "\n",
    "\n",
    "'''\n",
    "    # embed -> i\n",
    "    for i in range(model.cfg.n_layers):\n",
    "        edges.append((attrs[0,i+1,pos].flatten()[0], 'embed', input(i), pos))\n",
    "    # i -> j\n",
    "    for i in range(model.cfg.n_layers):\n",
    "        for j in range(i+1, model.cfg.n_layers):\n",
    "            edges.append((attrs[i+1,j+1,pos].flatten()[0], output(i), input(j), pos))\n",
    "    # j -> output\n",
    "    for j in range(model.cfg.n_layers):\n",
    "        edges.append((attrs[j+1,model.cfg.n_layers+1,pos].flatten()[0], output(j), 'output', pos))\n",
    "    # embed -> output\n",
    "    edges.append((attrs[0,model.cfg.n_layers+1,pos].flatten()[0], 'embed', 'output', pos))\n",
    "'''\n",
    "\n",
    "attrs = attributions.mean(dim=0)\n",
    "conv_attrs = conv_attributions.mean(dim=0)\n",
    "ALWAYS_KEEP_WEIGHT = -10\n",
    "for pos in positions:\n",
    "    # direct connections from embed to output\n",
    "    edges.append(Edge(\n",
    "            label=str(pos),\n",
    "            input_node=INPUT_NODE,\n",
    "            input_hook=(INPUT_HOOK, storage_hook),\n",
    "            output_node=OUTPUT_NODE,\n",
    "            output_hook=(OUTPUT_HOOK, partial(resid_patching_hook, position=pos)),\n",
    "            score_diff_when_patched=attrs[0,model.cfg.n_layers+1,pos].flatten()[0],\n",
    "    ))\n",
    "\n",
    "for layer in limited_layers:\n",
    "    for pos_i, pos in enumerate(positions):\n",
    "        # edge from embed to layer input\n",
    "        edges.append(Edge(\n",
    "                label=str(pos),\n",
    "                input_node=INPUT_NODE,\n",
    "                input_hook=(INPUT_HOOK, partial(storage_hook)),\n",
    "                output_node=input(layer),\n",
    "                output_hook=(f'blocks.{layer}.hook_layer_input', partial(resid_patching_hook, position=pos)),\n",
    "                score_diff_when_patched=attrs[0,layer+1,pos].flatten()[0],\n",
    "        ))\n",
    "\n",
    "        # input to conv\n",
    "        for conv_i in range(model.cfg.d_conv):\n",
    "            edges.append(Edge(\n",
    "                    label=(f'[{pos}:{conv_i-model.cfg.d_conv+1}]'.replace(\"None:\", \"\")), # [-D_CONV+1, -D_CONV+2, ..., -2, -1, 0]\n",
    "                    input_node=input(layer),\n",
    "                    input_hook=[\n",
    "                        (f'blocks.{layer}.hook_layer_input', conv_patching_init_hook),\n",
    "                        (f'blocks.{layer}.hook_in_proj', partial(conv_patching_storage_hook, position=pos, layer=layer, conv_filter_i=conv_i))\n",
    "                    ],\n",
    "                    output_node=conv(layer),\n",
    "                    output_hook=(f'blocks.{layer}.hook_conv', partial(conv_patching_hook, position=pos, layer=layer, conv_filter_i=conv_i)),\n",
    "                    score_diff_when_patched=conv_attrs[layer,conv_i,pos].flatten()[0],\n",
    "            ))\n",
    "        \n",
    "        # conv to ssm\n",
    "        if pos is None:\n",
    "            # we need a seperate hook for each pos, but put them all into one edge\n",
    "            hooks = []\n",
    "            for other_pos in range(L):\n",
    "                hooks.append((f'blocks.{layer}.hook_h.{other_pos}', overwrite_h_hook))\n",
    "            edges.append(Edge(\n",
    "                    input_node=conv(layer),\n",
    "                    output_node=ssm(layer),\n",
    "                    output_hook=hooks,\n",
    "                    score_diff_when_patched=ALWAYS_KEEP_WEIGHT,\n",
    "            ))\n",
    "        else:\n",
    "            edges.append(Edge(\n",
    "                    label=f'{pos}',\n",
    "                    input_node=conv(layer),\n",
    "                    output_node=ssm(layer),\n",
    "                    output_hook=(f'blocks.{layer}.hook_h.{pos}', overwrite_h_hook),\n",
    "                    score_diff_when_patched=ALWAYS_KEEP_WEIGHT\n",
    "            ))\n",
    "\n",
    "        if pos_i == 0: # we only need one of these\n",
    "            # ssm to output\n",
    "            edges.append(Edge(\n",
    "                    input_node=ssm(layer),\n",
    "                    output_node=output(layer),\n",
    "                    score_diff_when_patched=ALWAYS_KEEP_WEIGHT\n",
    "            ))\n",
    "        \n",
    "        # input to skip\n",
    "        edges.append(Edge(\n",
    "                label=f'{pos}',\n",
    "                input_node=input(layer),\n",
    "                output_node=skip(layer),\n",
    "                output_hook=(f'blocks.{layer}.hook_skip', partial(overwrite_patching_hook, position=pos)),\n",
    "                score_diff_when_patched=ALWAYS_KEEP_WEIGHT\n",
    "        ))\n",
    "\n",
    "        if pos_i == 0: # we only need one of these\n",
    "            # skip to output\n",
    "            edges.append(Edge(\n",
    "                    input_node=skip(layer),\n",
    "                    output_node=output(layer),\n",
    "                    score_diff_when_patched=ALWAYS_KEEP_WEIGHT\n",
    "            ))\n",
    "        \n",
    "        for later_layer in limited_layers:\n",
    "                if layer < later_layer:\n",
    "                    # edge from layer output to other layer input\n",
    "                    edges.append(Edge(\n",
    "                            label=str(pos),\n",
    "                            input_node=output(layer),\n",
    "                            input_hook=(f'blocks.{layer}.hook_out_proj', storage_hook),\n",
    "                            output_node=input(later_layer),\n",
    "                            output_hook=(f'blocks.{later_layer}.hook_layer_input', partial(resid_patching_hook, position=pos)),\n",
    "                            score_diff_when_patched=attrs[layer+1,later_layer+1,pos].flatten()[0],\n",
    "                    ))\n",
    "        \n",
    "        # edge from layer output to final layer output\n",
    "        edges.append(Edge(\n",
    "                label=str(pos),\n",
    "                input_node=output(layer),\n",
    "                input_hook=(f'blocks.{layer}.hook_out_proj', storage_hook),\n",
    "                output_node=OUTPUT_NODE,\n",
    "                output_hook=(OUTPUT_HOOK, partial(resid_patching_hook, position=pos)),\n",
    "                score_diff_when_patched=attrs[layer+1,model.cfg.n_layers+1,pos].flatten()[0],\n",
    "        ))\n",
    "\n",
    "\n",
    "\n",
    "def normalized_logit_diff_acdc_metric(data: ACDCEvalData, printing=False):\n",
    "    B,V = data.patched.logits.size()\n",
    "\n",
    "    # [batch_size]\n",
    "    patched_correct = data.patched.correct[:,0]\n",
    "    #print(data.unpatched.logits.size(), data.patched.logits.size(), data.corrupted.logits.size())\n",
    "    A_logits_unpatched = data.unpatched.logits[torch.arange(B), patched_correct]\n",
    "    A_logits_patched = data.patched.logits[torch.arange(B), patched_correct]\n",
    "    A_logits_corrupted = data.corrupted.logits[torch.arange(B), patched_correct]\n",
    "\n",
    "    corrupted_correct = data.corrupted.correct[:,0]\n",
    "    B_logits_unpatched = data.unpatched.logits[torch.arange(B), corrupted_correct]\n",
    "    B_logits_patched = data.patched.logits[torch.arange(B), corrupted_correct]\n",
    "    B_logits_corrupted = data.corrupted.logits[torch.arange(B), corrupted_correct]\n",
    "\n",
    "    # A and B are two potential outputs\n",
    "    # if A patched > B patched, we are correct\n",
    "    # else we are incorrect\n",
    "\n",
    "    # thus we could just return A_logits_patched - B_logits_patched\n",
    "\n",
    "    # however it is useful to \"normalize\" these values\n",
    "\n",
    "    # in the worst case, our patching causes us to act like corrupted, and our diff will be\n",
    "    # A_logits_corrupted - B_logits_corrupted\n",
    "    # this will result in a small, negative value\n",
    "    \n",
    "    # in the best case, our patching will do nothing (cause us to act like unpatched), and our diff will be\n",
    "    # A_logits_unpatched - B_logits_unpatched\n",
    "    # this will result in a large, positive value\n",
    "\n",
    "    # thus we can treat those as the \"min\" and \"max\" and normalize accordingly\n",
    "    \n",
    "    min_diff = A_logits_corrupted - B_logits_corrupted\n",
    "    max_diff = A_logits_unpatched - B_logits_unpatched\n",
    "\n",
    "    possible_range = (max_diff-min_diff)\n",
    "    possible_range[possible_range == 0] = 1.0 # prevent divide by zero\n",
    "    \n",
    "    diff = A_logits_patched - B_logits_patched\n",
    "    normalized_diff = (diff-min_diff)/torch.abs(possible_range) # abs prevents incorrect data from wanting to be more incorrect\n",
    "\n",
    "    if printing:\n",
    "        print(f\"A corrupted {A_logits_corrupted}\")\n",
    "        print(f\"B corrupted {B_logits_corrupted}\")\n",
    "        print(f\"A unpatched {A_logits_unpatched}\")\n",
    "        print(f\"B unpatched {B_logits_unpatched}\")\n",
    "        print(f\"A patched {A_logits_patched}\")\n",
    "        print(f\"B patched {B_logits_patched}\")\n",
    "        print(f\"min diff {min_diff}\")\n",
    "        print(f\"max diff {max_diff}\")\n",
    "        print(f\"possible range {possible_range}\")\n",
    "        print(f\"diff {diff}\")\n",
    "        print(f\"normalized diff {normalized_diff}\")\n",
    "    # as described, 1.0 corresponds to acting like unpatched,\n",
    "    # and 0.0 corresponds to acting like corrupted\n",
    "\n",
    "    return torch.mean(normalized_diff)\n",
    "import acdc\n",
    "cfg = ACDCConfig(\n",
    "    ckpt_directory = \"blah\",\n",
    "    thresh = 0.0001,\n",
    "    rollback_thresh = 0.0001,\n",
    "    metric=acdc.accuracy_metric,\n",
    "    # extra inference args\n",
    "    model_kwargs=model_kwargs,\n",
    "    # these are needed for doing graph pruning\n",
    "    input_node=INPUT_NODE,\n",
    "    output_node=OUTPUT_NODE,\n",
    "    # batch size for evaluating data points\n",
    "    batch_size=1,\n",
    "    log_level=LOG_LEVEL_INFO,\n",
    "    # if False, will be equivalent to batch_size=1\n",
    "    batched = True,\n",
    "    # set these two to false to use traditional ACDC\n",
    "    # recursive will try patching multiple at a time (this is faster sometimes)\n",
    "    recursive = True,\n",
    "    # try_patching_multiple_at_same_time will evaluate many different patchings before commiting to any\n",
    "    # and includes a rollback scheme if after patching one, the others get worse\n",
    "    try_patching_multiple_at_same_time = True,\n",
    "    ## if true, you metric will also have the logits from a run with no patching available\n",
    "    # (useful for normalized logit diff)\n",
    "    store_unpatched_logits = True,\n",
    ")\n",
    "from acdc import get_currently_patched_edge_hooks, eval_acdc, wrap_run_with_hooks, get_logits_of_predicted_next_token\n",
    "unpatched_logits = get_logits_of_predicted_next_token(\n",
    "        model=model,\n",
    "        data=data.valid_data,\n",
    "        last_token_position=data.valid_last_token_position,\n",
    "        **model_kwargs\n",
    ")\n",
    "\n",
    "valid_unpatched_logits = get_logits_of_predicted_next_token(\n",
    "        model=model,\n",
    "        data=data.valid_data,\n",
    "        last_token_position=data.valid_last_token_position,\n",
    "        **model_kwargs\n",
    ")\n",
    "\n",
    "def eval_edges(edges_keeping, all_edges, valid=False):\n",
    "    for edge in all_edges:\n",
    "        edge.patching = True\n",
    "        edge.checked = True\n",
    "    for edge in edges_keeping:\n",
    "        edge.patching = False\n",
    "        edge.checked = True\n",
    "    currently_patched_edge_hooks = get_currently_patched_edge_hooks(cfg=cfg, edges=edges)\n",
    "    if valid:\n",
    "        return eval_acdc(\n",
    "                model=wrap_run_with_hooks(model=model, fwd_hooks=currently_patched_edge_hooks, **cfg.model_kwargs),\n",
    "                data=data.valid_data,\n",
    "                last_token_position=data.valid_last_token_position,\n",
    "                correct=data.valid_correct,\n",
    "                incorrect=data.valid_incorrect,\n",
    "                metric=cfg.metric,\n",
    "                num_edges=1,\n",
    "                constrain_to_answers=data.constrain_to_answers,\n",
    "                unpatched_logits=valid_unpatched_logits)[0].item()\n",
    "    else:\n",
    "        return eval_acdc(\n",
    "            model=wrap_run_with_hooks(model=model, fwd_hooks=currently_patched_edge_hooks, **cfg.model_kwargs),\n",
    "            data=data.data,\n",
    "            last_token_position=data.last_token_position,\n",
    "            correct=data.correct,\n",
    "            incorrect=data.incorrect,\n",
    "            metric=cfg.metric,\n",
    "            num_edges=1,\n",
    "            constrain_to_answers=data.constrain_to_answers,\n",
    "            unpatched_logits=unpatched_logits)[0].item()\n",
    "\n",
    "# can also do this instead to include negative contributions but I find the graph is larger, ymmv\n",
    "#edges.sort(key=lambda x: -abs(x[0]))\n",
    "edges.sort(key=lambda edge: edge.score_diff_when_patched)\n",
    "\n",
    "import math\n",
    "def test_pos(pos):\n",
    "    print(f\"testing pos {pos}\")\n",
    "    edges_to_keep = edges[:pos]\n",
    "    metric = eval_edges(edges_to_keep, edges)\n",
    "    print(f\"testing pos {pos} got metric {metric}\")\n",
    "    return metric\n",
    "\n",
    "# from https://en.wikipedia.org/wiki/Binary_search_algorithm\n",
    "def binary_search(n, T):\n",
    "    L = 0\n",
    "    R = n - 1\n",
    "    while L != R:\n",
    "        m = math.ceil((L + R) / 2)\n",
    "        if test_pos(m) > T:\n",
    "            R = m - 1\n",
    "        else:\n",
    "            L = m\n",
    "    # go one further because this gives us below thresh\n",
    "    return min(n-1, L+1)\n",
    "\n",
    "ACC_THRESH = 0.85\n",
    "torch.set_grad_enabled(False)\n",
    "cutoff = binary_search(len(edges), T=ACC_THRESH)\n",
    "\n",
    "edges_to_keep = edges[:cutoff]\n",
    "scores = [edge.score_diff_when_patched for edge in edges[:cutoff]]\n",
    "print(f\"keeping top {cutoff} edges {edges_to_keep}\")\n",
    "metric = eval_edges(edges_to_keep, edges)\n",
    "print(f\"got metric {metric}\")    \n",
    "\n",
    "metric = eval_edges(edges_to_keep, edges, valid=True)\n",
    "print(f\"got valid metric {metric}\")    \n",
    "# .85 has 3061 edges for 5 ITERS\n",
    "# .85 has 2876 edges for 30 ITERS\n",
    "num_always_keep = 0\n",
    "for edge in edges_to_keep:\n",
    "    if edge.score_diff_when_patched == ALWAYS_KEEP_WEIGHT:\n",
    "        num_always_keep += 1\n",
    "print(f\"always keep {num_always_keep}\")\n",
    "\n",
    "\n",
    "# patching format 0:\n",
    "# 2162 edges (2016 always keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d716bc32-b8d0-4320-8057-373638c8499b",
   "metadata": {},
   "source": [
    "# Display ACDC Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a72d39-8320-4c57-ba05-b3ebb97ac465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from acdc import ACDCEvalData\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "import torch\n",
    "from IPython.display import display, FileLink, Image\n",
    "import acdc\n",
    "from importlib import reload\n",
    "edges = edges_to_keep\n",
    "L = data.data.size()[1]\n",
    "print(f\"got L of {L}\")\n",
    "if POSITIONS:\n",
    "    by_filters = torch.zeros([model.cfg.d_conv*L, model.cfg.n_layers])\n",
    "else:\n",
    "    by_filters = torch.zeros([model.cfg.d_conv, model.cfg.n_layers])\n",
    "\n",
    "# change this to get the different plots\n",
    "CAP = 0.0\n",
    "# 1.0 is boolean (present or not)\n",
    "# 0.0 is just give score\n",
    "\n",
    "def filter_score(score):\n",
    "    if CAP == 0.0: return score\n",
    "    if CAP == 1.0: return 1.0\n",
    "    return max(score, -CAP)\n",
    "for edge in edges:\n",
    "    if not edge.patching and edge.checked:\n",
    "        if '.conv' in edge.output_node:\n",
    "            if \":\" in edge.label:\n",
    "                pos = int(edge.label.split(\":\")[0][1:])\n",
    "                rest = edge.label.split(\":\")[1]\n",
    "            else:\n",
    "                pos = None\n",
    "                rest = edge.label[1:]\n",
    "            filter = int(rest.split(\"]\")[0])        \n",
    "            layer = edge.output_node.split(\".\")[0]\n",
    "            if POSITIONS:\n",
    "                try:\n",
    "                    by_filters[model.cfg.d_conv*pos + abs(int(filter)),int(layer)] = filter_score(edge.score_diff_when_patched)\n",
    "                except:\n",
    "                    print(L, pos, abs(int(filter)))\n",
    "                    raise\n",
    "            else:\n",
    "                by_filters[abs(int(filter)),int(layer)] = filter_score(edge.score_diff_when_patched)\n",
    "        if '.skip' in edge.output_node and False:        \n",
    "            layer = edge.output_node.split(\".\")[0]\n",
    "            by_filters[model.cfg.d_conv,int(layer)] = filter_score(edge.score_diff_when_patched)\n",
    "        if '.ssm' in edge.output_node and False:\n",
    "            layer = edge.output_node.split(\".\")[0]\n",
    "            by_filters[model.cfg.d_conv+1,int(layer)] = filter_score(edge.score_diff_when_patched)\n",
    "            \n",
    "\n",
    "def layer_to_i(node):\n",
    "    if node == INPUT_NODE:\n",
    "        return 0\n",
    "    elif node == OUTPUT_NODE:\n",
    "        return model.cfg.n_layers+1 # because embed is 0\n",
    "    else:\n",
    "        return int(node)+1 # because embed is 0\n",
    "\n",
    "\n",
    "def between_layers_info(edge):\n",
    "    if edge.patching: return False, None, None\n",
    "    is_between_layers = False\n",
    "    layer_input = None\n",
    "    layer_output = None\n",
    "    if edge.input_node == INPUT_NODE:\n",
    "        is_between_layers = True\n",
    "        layer_input = INPUT_NODE\n",
    "    if edge.output_node == OUTPUT_NODE:\n",
    "        is_between_layers = True\n",
    "        layer_output = OUTPUT_NODE\n",
    "\n",
    "    if '.' in edge.input_node:\n",
    "        input_layer, input_type = edge.input_node.split(\".\")\n",
    "        if edge.input_node == output(input_layer):\n",
    "            is_between_layers = True\n",
    "            layer_input = str(input_layer)\n",
    "    if '.' in edge.output_node:\n",
    "        output_layer, output_type = edge.output_node.split(\".\")\n",
    "        if edge.output_node == input(output_layer):\n",
    "            is_between_layers = True\n",
    "            layer_output = str(output_layer)\n",
    "    return is_between_layers, layer_input, layer_output\n",
    "\n",
    "def compute_adj_mat(edges):\n",
    "    if POSITIONS:\n",
    "        adj_mat = torch.zeros([L, model.cfg.n_layers+2, model.cfg.n_layers+2])\n",
    "    else:\n",
    "        adj_mat = torch.zeros([model.cfg.n_layers+2, model.cfg.n_layers+2])\n",
    "    for edge in edges:\n",
    "        is_between_layers, layer_input, layer_output = between_layers_info(edge)\n",
    "        if is_between_layers:\n",
    "            if POSITIONS:\n",
    "                pos = int(edge.label)\n",
    "                adj_mat[pos, layer_to_i(layer_input), layer_to_i(layer_output)] = edge.score_diff_when_patched\n",
    "            else:\n",
    "                adj_mat[layer_to_i(layer_input), layer_to_i(layer_output)] = edge.score_diff_when_patched\n",
    "    return adj_mat\n",
    "\n",
    "\n",
    "L = data.data.size()[1]\n",
    "import networkx as nx\n",
    "def better_get_nx_graph(adj_mat) -> nx.DiGraph:\n",
    "    '''\n",
    "    Converts the edges into a networkx graph\n",
    "    only edges that have checked == True and patching == False are included\n",
    "    if include_unchecked=True, any edge that has checked == False is also included\n",
    "    '''\n",
    "    num_nodes = adj_mat.size()[-1]\n",
    "    G = nx.DiGraph()\n",
    "    edges = []\n",
    "    if POSITIONS:\n",
    "        for pos in range(L):\n",
    "            for i in range(num_nodes):\n",
    "                for j in range(num_nodes):\n",
    "                    if adj_mat[pos,i,j] != 0:\n",
    "                        G.add_edge(str(i), str(j))\n",
    "                        edges.append((i,j,pos,adj_mat[pos,i,j]))\n",
    "    else:\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(num_nodes):\n",
    "                if adj_mat[i,j] != 0:\n",
    "                    G.add_edge(str(i), str(j))\n",
    "                    edges.append((i,j,None,adj_mat[i,j]))\n",
    "    return G, edges\n",
    "\n",
    "def better_prune_edges(adj_mat):\n",
    "    num_nodes = adj_mat.size()[-1]\n",
    "    input_node = 0\n",
    "    output_node = num_nodes-1\n",
    "    import networkx as nx\n",
    "    G, edges = better_get_nx_graph(adj_mat=adj_mat)\n",
    "    pruned_adj_mat = torch.zeros(adj_mat.size())\n",
    "    pruned_edges = []\n",
    "    for i,j,pos,attr in edges:\n",
    "        connected_to_input = False\n",
    "        try:\n",
    "            to_input = nx.shortest_path(G, source=str(input_node), target=str(i))\n",
    "            connected_to_input = True\n",
    "        except nx.NetworkXNoPath:\n",
    "            pass\n",
    "        except nx.NodeNotFound:\n",
    "            raise ValueError(f\"Graph does not have input node {input_node}\")\n",
    "        \n",
    "        connected_to_output = False\n",
    "        try:\n",
    "            to_output = nx.shortest_path(G, source=str(j), target=str(output_node))\n",
    "            connected_to_output = True\n",
    "        except nx.NetworkXNoPath:\n",
    "            pass\n",
    "        except nx.NodeNotFound:\n",
    "            raise ValueError(f\"Graph does not have output node {output_node}\")\n",
    "        print(f\"testing {i}->{j} {connected_to_input} {connected_to_output}\")\n",
    "        if connected_to_input and connected_to_output:\n",
    "            pruned_adj_mat[pos,i,j] = attr\n",
    "        else:\n",
    "            print(f\"pruning {pos} {i}->{j} {connected_to_input} {connected_to_output}\")\n",
    "    return pruned_adj_mat\n",
    "\n",
    "adj_mat = compute_adj_mat(edges_to_keep)\n",
    "pruned_adj_mat = better_prune_edges(adj_mat)\n",
    "\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", font_size=None, show=True, color_continuous_midpoint=0.0, **kwargs):\n",
    "    import plotly.express as px\n",
    "    import transformer_lens.utils as utils\n",
    "    fig = px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=color_continuous_midpoint, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs)\n",
    "    if not font_size is None:\n",
    "        if 'x' in kwargs and False:\n",
    "            fig.update_layout(\n",
    "              xaxis = dict(\n",
    "                tickmode='array',\n",
    "                tickvals = kwargs['x'],\n",
    "                ticktext = kwargs['x'], \n",
    "                ),\n",
    "               font=dict(size=font_size, color=\"black\"))\n",
    "        if 'y' in kwargs:\n",
    "            fig.update_layout(\n",
    "              yaxis = dict(\n",
    "                tickmode='array',\n",
    "                tickvals = kwargs['y'],\n",
    "                ticktext = kwargs['y'], \n",
    "                ),\n",
    "               font=dict(size=font_size, color=\"black\"))\n",
    "    plot_args = {\n",
    "        'width': 1200,\n",
    "        'height': 900,\n",
    "        \"autosize\": False,\n",
    "        'showlegend': True,\n",
    "        'margin': {\"l\":0,\"r\":0,\"t\":100,\"b\":0}\n",
    "    }\n",
    "    \n",
    "    fig.update_layout(**plot_args)\n",
    "    fig.update_layout(legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    ))\n",
    "    if show:\n",
    "        fig.show(renderer)\n",
    "    else:\n",
    "        return fig\n",
    "\n",
    "def to_str(lis):\n",
    "    return [str(x) for x in lis]\n",
    "x_labels = to_str([-x for x in range(model.cfg.d_conv)])\n",
    "toks = model.to_str_tokens(data.data[0])\n",
    "x_labels_all_pos = []\n",
    "for l in range(L):\n",
    "    x_labels_all_pos += [label + toks[l+int(label)] + \"_\" + str(l) for label in x_labels]\n",
    "print(x_labels, by_filters.size())\n",
    "print(len(x_labels_all_pos))\n",
    "title = f'which parts of layer using (conv filters, skip, and/or conv), capped to {-CAP}'\n",
    "if CAP == 1.0:\n",
    "    title = f'which parts of layer using (conv filters, skip, and/or conv)'\n",
    "print(x_labels_all_pos)\n",
    "imshow(by_filters.T, y=to_str(range(model.cfg.n_layers)), x=x_labels_all_pos, title=title, font_size=9)\n",
    "#for layer in range(model.cfg.n_layers):\n",
    "#    imshow(by_filters[:,layer:layer+1].T, y=to_str([layer]), x=x_labels_all_pos, title=str(layer), font_size=9)\n",
    "\n",
    "labels = ['embed'] + [str(x) for x in range(model.cfg.n_layers)] + ['output']\n",
    "if POSITIONS:\n",
    "    for position in range(L):\n",
    "        if torch.any(pruned_adj_mat[position] != 0):\n",
    "            imshow(pruned_adj_mat[position], y=labels, x=labels, title=f'pos {position} tok {toks[position]} adjacency matrix clamped to {CAP}', font_size=8)\n",
    "else:\n",
    "    imshow(pruned_adj_mat, y=labels, x=labels, title=f'adjacency matrix clamped to {CAP}', font_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6557bd1-5b65-47c2-92d9-b2c28467694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_positions = [3,5,7,13,15]\n",
    "position_map = {}\n",
    "for l in range(L):\n",
    "    position_map[l] = f'pos{l}{toks[l]}'\n",
    "position_map[3] = 'n1'\n",
    "position_map[5] = 'n2'\n",
    "position_map[7] = 'n3'\n",
    "position_map[13] = 'n4'\n",
    "position_map[15] = 'n5'\n",
    "position_map[19] = 'out'\n",
    "\n",
    "\n",
    "\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    layer_attrs = []\n",
    "    for l in range(L):\n",
    "        for filter_i in range(model.cfg.d_conv):\n",
    "            filter_offset = -filter_i\n",
    "            index = l*model.cfg.d_conv + filter_i\n",
    "            attr = by_filters[index, layer]\n",
    "            if attr < 0:\n",
    "                layer_attrs.append((position_map[l+filter_offset], attr, filter_offset))\n",
    "    print(layer)\n",
    "    for tok, attr, filter_offset in layer_attrs:\n",
    "        attr = -attr*1000\n",
    "        print(f\"  {tok}[{filter_offset}] {attr:.5f}\")\n",
    "\n",
    "def map_layer(layer):\n",
    "    if layer == 0: return 'embed'\n",
    "    if layer == model.cfg.n_layers+1: return 'output'\n",
    "    else: return str(layer-1)\n",
    "\n",
    "for layer in range(model.cfg.n_layers+2):\n",
    "    layer_attrs = []\n",
    "    for other_layer in range(model.cfg.n_layers+2):\n",
    "        for pos in range(L):\n",
    "            if pruned_adj_mat[pos, layer, other_layer] != 0:\n",
    "                layer_attrs.append((position_map[pos], f\"->{map_layer(other_layer)}\", pruned_adj_mat[pos, layer, other_layer]))\n",
    "            if pruned_adj_mat[pos, other_layer, layer] != 0:\n",
    "                layer_attrs.append((position_map[pos], f\"{map_layer(other_layer)}->\", pruned_adj_mat[pos, other_layer, layer]))\n",
    "    print(map_layer(layer))\n",
    "    for tok, label, attr in layer_attrs:\n",
    "        attr = -attr*1000\n",
    "        print(f\"  {tok} {label} {attr:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92335cb9-9698-4f90-8121-7aa53c8af690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def layer_to_i(layer):\n",
    "    if layer == 'embed': return 0\n",
    "    elif layer == 'output': return model.cfg.n_layers+1\n",
    "    else: return int(layer)+1\n",
    "\n",
    "forbidden = [0,1,40]\n",
    "pruned_adj_mat = better_prune_edges(adj_mat)\n",
    "for i in forbidden:\n",
    "    pruned_adj_mat[:,i,:] = 0\n",
    "    pruned_adj_mat[:,:,i] = 0\n",
    "present_layers = set()\n",
    "for i in range(model.cfg.n_layers):\n",
    "    for j in range(model.cfg.n_layers):\n",
    "        if not i+1 in forbidden and not j+1 in forbidden:\n",
    "            if torch.any(pruned_adj_mat[:,i+1,j+1] != 0):\n",
    "                present_layers.add(i)\n",
    "                present_layers.add(j)\n",
    "                #print(f\"present edge {i}->{j}\")\n",
    "print(present_layers)\n",
    "pruned_dot = graphviz.Digraph('graph')\n",
    "for edge in edges_to_keep:\n",
    "    is_between_layers, layer_input, layer_output = between_layers_info(edge)\n",
    "    if is_between_layers:\n",
    "        ini, outi = layer_to_i(layer_input), layer_to_i(layer_output)\n",
    "        pos = int(edge.label)\n",
    "        if pruned_adj_mat[pos, ini, outi] != 0:\n",
    "            if not ini in forbidden and not outi in forbidden:\n",
    "                pruned_dot.edge(edge.input_node, edge.output_node, label=position_map[int(edge.label)])\n",
    "    else:\n",
    "        layer = int(edge.input_node.split(\".\")[0])\n",
    "        if layer in present_layers: # don't display stuff that are disconnected\n",
    "            if '.ssm' in edge.output_node or '.skip' in edge.output_node or '.skip' in edge.input_node: # don't need all these since they are all forced on\n",
    "                if edge.label == 0 or edge.label == \"\" or edge.label is None or edge.label == \"0\":\n",
    "                    pruned_dot.edge(edge.input_node, edge.output_node)\n",
    "            else:\n",
    "                pruned_dot.edge(edge.input_node, edge.output_node, label=edge.label)\n",
    "output_name = f'pruned dot {ACC_THRESH}'\n",
    "pruned_dot.render(output_name, format=\"png\") # it automatically appends png\n",
    "display(Image(filename=output_name + \".png\"))\n",
    "display(FileLink(output_name + \".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593bfde8-9bd8-4f43-8c08-cb8b3e0eda8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c843768-7705-4c53-8907-6f901d21ab61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
