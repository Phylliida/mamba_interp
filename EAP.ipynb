{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1812c036-831d-4b08-b14a-156ede4cbb7c",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a248986-64cf-433e-8782-9cc0585347f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", font_size=None, show=True, color_continuous_midpoint=0.0, **kwargs):\n",
    "    import plotly.express as px\n",
    "    import transformer_lens.utils as utils\n",
    "    fig = px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=color_continuous_midpoint, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs)\n",
    "    if not font_size is None:\n",
    "        if 'x' in kwargs:\n",
    "            fig.update_layout(\n",
    "              xaxis = dict(\n",
    "                tickmode='array',\n",
    "                tickvals = kwargs['x'],\n",
    "                ticktext = kwargs['x'], \n",
    "                ),\n",
    "               font=dict(size=font_size, color=\"black\"))\n",
    "        if 'y' in kwargs:\n",
    "            fig.update_layout(\n",
    "              yaxis = dict(\n",
    "                tickmode='array',\n",
    "                tickvals = kwargs['y'],\n",
    "                ticktext = kwargs['y'], \n",
    "                ),\n",
    "               font=dict(size=font_size, color=\"black\"))\n",
    "    plot_args = {\n",
    "        'width': 800,\n",
    "        'height': 600,\n",
    "        \"autosize\": False,\n",
    "        'showlegend': True,\n",
    "        'margin': {\"l\":0,\"r\":0,\"t\":100,\"b\":0}\n",
    "    }\n",
    "    \n",
    "    fig.update_layout(**plot_args)\n",
    "    fig.update_layout(legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    ))\n",
    "    if show:\n",
    "        fig.show(renderer)\n",
    "    else:\n",
    "        return fig\n",
    "\n",
    "\n",
    "# idea:\n",
    "# do in higher precision because things are so smol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c3e22ed-b7f6-4ed2-b73a-eda2b4e6c365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning:\n",
      "\n",
      "TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fba654e5e40>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# requires\n",
    "# pip install git+https://github.com/Phylliida/MambaLens.git\n",
    "\n",
    "from mamba_lens import HookedMamba # this will take a little while to import\n",
    "import torch\n",
    "model_path = \"state-spaces/mamba-370m\"\n",
    "model = HookedMamba.from_pretrained(model_path, device='cuda')\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aec4aa-5af8-49be-b773-dc4400ad5bfb",
   "metadata": {},
   "source": [
    "## Load SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f0f8f40-e5c1-42ed-b149-b644e64070a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from safetensors.torch import load_file\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import sys\n",
    "if not \"/home/dev/sae-k-sparse-mamba/sae\" in sys.path:\n",
    "    sys.path.append(\"/home/dev/sae-k-sparse-mamba\")\n",
    "import os\n",
    "os.chdir('/home/dev/sae-k-sparse-mamba')\n",
    "saes = []\n",
    "from importlib import reload\n",
    "import sae\n",
    "reload(sae.sae)\n",
    "from sae.sae import Sae\n",
    "\n",
    "for i in range(40):\n",
    "    print(i)\n",
    "    path = f'/home/dev/sae-k-sparse-mamba/blocks.{i}.hook_resid_pre/hook_blocks.{i}.hook_resid_pre.pt'\n",
    "    saes.append(Sae.load_from_disk(path, hook=f'blocks.{i}.hook_resid_pre', device=model.cfg.device))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1baabd77-6d6d-416c-95a7-8b3d5b314ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using patching format\n",
      "ABC AB C\n",
      "ABD AB D\n",
      "\n",
      "ABC BA C\n",
      "ABD BA D\n",
      "\n",
      "using templates\n",
      "Then, [NAME], [NAME] and [NAME] went to the [PLACE]. [NAME] and [NAME] gave a [OBJECT] to\n",
      "Afterwards [NAME], [NAME] and [NAME] went to the [PLACE]. [NAME] and [NAME] gave a [OBJECT] to\n",
      "Friends [NAME], [NAME] and [NAME] went to the [PLACE]. [NAME] and [NAME] gave a [OBJECT] to\n",
      "with name positions (2, 4, 6, 12, 14)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from acdc.data.ioi import ioi_data_generator, ABC_TEMPLATES, get_all_single_name_abc_patching_formats\n",
    "from acdc.data.utils import generate_dataset\n",
    "\n",
    "num_patching_pairs = 200\n",
    "seed = 27\n",
    "valid_seed = 28\n",
    "constrain_to_answers = True\n",
    "# this makes our data size 800, first 400 is each (a,b) pair, and then second 400 is each pair swapped to be (b,a)\n",
    "has_symmetric_patching = True\n",
    "\n",
    "n1_patchings = [\"\"\"\n",
    "ABC BC A\n",
    "DBC BC D\"\"\",\n",
    "    \"\"\"\n",
    "ABC CB A\n",
    "DBC CB D\"\"\"]\n",
    "\n",
    "n2_patchings = [\"\"\"\n",
    "ABC AC B\n",
    "ADC AC D\"\"\",\n",
    "    \"\"\"\n",
    "ABC CA B\n",
    "ADC CA D\"\"\"]\n",
    "\n",
    "n3_patchings = [\"\"\"\n",
    "ABC AB C\n",
    "ABD AB D\"\"\",\n",
    "    \"\"\"\n",
    "ABC BA C\n",
    "ABD BA D\"\"\"]\n",
    "\n",
    "n4_patchings = [\"\"\"\n",
    "ABC AC B\n",
    "ABC BC A\"\"\",\n",
    "    \"\"\"\n",
    "ABC AB C\n",
    "ABC CB A\"\"\",\n",
    "    \"\"\"\n",
    "ABC BA C\n",
    "ABC CA B\"\"\"]\n",
    "\n",
    "n5_patchings = [\"\"\"\n",
    "ABC CA B\n",
    "ABC CB A\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "ABC BA C\n",
    "ABC BC A\"\"\",\n",
    "    \"\"\"\n",
    "ABC AB C\n",
    "ABC AC B\"\"\"]\n",
    "\n",
    "patchings = {\n",
    "    'n1': n1_patchings,\n",
    "    'n2': n2_patchings,\n",
    "    'n3': n3_patchings,\n",
    "    'n4': n4_patchings,\n",
    "    'n5': n5_patchings\n",
    "}\n",
    "\n",
    "all_patchings = []\n",
    "for patching in patchings.values():\n",
    "    all_patchings += patching\n",
    "all_patchings = sorted(all_patchings) # make deterministic \n",
    "\n",
    "patch_all_names = [\"\"\"\n",
    "ABC AB C\n",
    "DEF DE F\"\"\",\n",
    "    \"\"\"\n",
    "ABC AC B\n",
    "DEF DF E\"\"\",     \n",
    "    \"\"\"\n",
    "ABC BA C\n",
    "DEF ED F\"\"\",\n",
    "    \"\"\"\n",
    "ABC BC A\n",
    "DEF EF D\"\"\",\n",
    "    \"\"\"\n",
    "ABC CA B\n",
    "DEF FD E\"\"\",\n",
    "    \"\"\"\n",
    "ABC CB A\n",
    "DEF FE D\"\"\"]\n",
    "\n",
    "\n",
    "patchings['all'] = all_patchings\n",
    "patchings['allatonce'] = patch_all_names\n",
    "from acdc.data.ioi import BABA_TEMPLATES\n",
    "templates = ABC_TEMPLATES\n",
    "#patching_formats = list(get_all_single_name_abc_patching_formats())\n",
    "PATCHING_FORMAT_I = 'n3'\n",
    "patching_formats = [x.strip() for x in patchings[PATCHING_FORMAT_I]]\n",
    "\n",
    "print(\"using patching format\")\n",
    "for patch in patching_formats:\n",
    "    print(patch)\n",
    "    print(\"\")\n",
    "#print(patching_formats)\n",
    "\n",
    "data = generate_dataset(model=model,\n",
    "                  data_generator=ioi_data_generator,\n",
    "                  num_patching_pairs=num_patching_pairs,\n",
    "                  seed=seed,\n",
    "                  valid_seed=valid_seed,\n",
    "                  constrain_to_answers=constrain_to_answers,\n",
    "                  has_symmetric_patching=has_symmetric_patching, \n",
    "                  varying_data_lengths=True,\n",
    "                  templates=templates,\n",
    "                  patching_formats=patching_formats)\n",
    "\n",
    "\n",
    "import acdc.data.ioi\n",
    "from collections import defaultdict\n",
    "name_positions_map = defaultdict(lambda: [])\n",
    "for template in templates:\n",
    "    name = acdc.data.ioi.good_names[0]\n",
    "    template_filled_in = template.replace(\"[NAME]\", name)\n",
    "    template_filled_in = template_filled_in.replace(\"[PLACE]\", acdc.data.ioi.good_nouns['[PLACE]'][0])\n",
    "    template_filled_in = template_filled_in.replace(\"[OBJECT]\", acdc.data.ioi.good_nouns['[OBJECT]'][0])\n",
    "    # get the token positions of the [NAME] in the prompt\n",
    "    name_positions = tuple([(i) for (i,s) in enumerate(model.to_str_tokens(torch.tensor(model.tokenizer.encode(template_filled_in)))) if s == f' {name}'])\n",
    "    name_positions_map[name_positions].append(template)\n",
    "sorted_by_frequency = sorted(list(name_positions_map.items()), key=lambda x: -len(x[1]))\n",
    "most_frequent_name_positions, templates = sorted_by_frequency[0]\n",
    "print(\"using templates\")\n",
    "for template in templates:\n",
    "    print(template)\n",
    "print(f\"with name positions {most_frequent_name_positions}\")\n",
    "\n",
    "data = generate_dataset(model=model,\n",
    "              data_generator=ioi_data_generator,\n",
    "              num_patching_pairs=num_patching_pairs,\n",
    "              seed=seed,\n",
    "              valid_seed=valid_seed,\n",
    "              constrain_to_answers=constrain_to_answers,\n",
    "              has_symmetric_patching=has_symmetric_patching, \n",
    "              varying_data_lengths=True,\n",
    "              templates=templates,\n",
    "              patching_formats=patching_formats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb43551-a414-45e2-a7ea-71f55cdbccf9",
   "metadata": {},
   "source": [
    "# Eval with SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9e656558-d3ab-4b08-aa75-804b0ed2e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "#K = saes[0].W_dec.size()[0]\n",
    "\n",
    "common_acts = defaultdict(lambda: [])\n",
    "\n",
    "cached_features = []\n",
    "def apply_sae(x, layer):\n",
    "    # X is [B,L,E]\n",
    "    # encoder.weight is [NFeatures,E]\n",
    "    # encoder.bias is [NFeatures]\n",
    "    # W_dec is [NFeatures,E]\n",
    "    # b_dec is [E]\n",
    "    #encoder_weight = sae_params['encoder.weight'][layer]\n",
    "    #encoder_bias = sae_params['encoder.bias'][layer]\n",
    "    #decoder_weight = sae_params['W_dec'][layer]\n",
    "    #decoder_bias = sae_params['b_dec'][layer]\n",
    "    # [B,L,NFeatures]  [B,L,E]    [E,NFeatures]       [NFeatures]\n",
    "    #features          = torch.nn.functional.relu(( (x - decoder_bias)   @  encoder_weight.T) + encoder_bias)\n",
    "    #cached_features.append(features)\n",
    "    # [B,L,E]  [B,L,NFeatures]  [NFeatures,E]      [E]\n",
    "    #decoded =    (features @ decoder_weight) + decoder_bias\n",
    "    K = saes[layer].cfg.k\n",
    "    #K = saes[0].W_dec.size()[0]\n",
    "    sae = saes[layer]\n",
    "    latent_acts = sae.encode(x)\n",
    "    token_outs = []\n",
    "    for l in range(latent_acts.size()[1]):\n",
    "        token_acts = latent_acts[:,l]\n",
    "        top_acts, top_indices = latent_acts.topk(K, sorted=False)\n",
    "        # Decode and compute residual\n",
    "        sae_out = sae.decode(top_acts, top_indices)\n",
    "        token_outs.append(sae_out)\n",
    "    token_outs = torch.stack(token_outs)\n",
    "    token_outs = rearrange(token_outs, 'L B E -> B L E')\n",
    "    # [B,L,E]\n",
    "    return token_outs\n",
    "'''\n",
    "def sae_hook(\n",
    "    x,\n",
    "    hook,\n",
    "    layer\n",
    "):\n",
    "    return apply_sae(x, layer)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7892bc46-1202-4af1-a70e-5343e579b4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed on this data point:\n",
      "<|endoftext|>Afterwards Anna, Miranda and Kyle went to the house. Anna and Kyle gave a snack to\n",
      "correct prs:\n",
      "0.4678356349468231  Miranda\n",
      "incorrect prs:\n",
      "0.515097439289093  Anna\n",
      "0.01706693507730961  Kyle\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards Lauren, Parker and Leslie went to the school. Leslie and Lauren gave a snack to\n",
      "correct prs:\n",
      "0.24796685576438904  Parker\n",
      "incorrect prs:\n",
      "0.0009757627849467099  Jorge\n",
      "0.5476138591766357  Lauren\n",
      "0.20344354212284088  Leslie\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards Lauren, Jorge and Leslie went to the school. Leslie and Lauren gave a snack to\n",
      "correct prs:\n",
      "0.3560652434825897  Jorge\n",
      "incorrect prs:\n",
      "0.5156988501548767  Lauren\n",
      "0.12706568837165833  Leslie\n",
      "0.0011702885385602713  Parker\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Katie, Caroline and Trevor went to the station. Caroline and Katie gave a necklace to\n",
      "correct prs:\n",
      "0.13432921469211578  Trevor\n",
      "incorrect prs:\n",
      "0.29621371626853943  Caroline\n",
      "0.5689470767974854  Katie\n",
      "0.0005100477719679475  Raymond\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Katie, Caroline and Raymond went to the station. Caroline and Katie gave a necklace to\n",
      "correct prs:\n",
      "0.23683786392211914  Raymond\n",
      "incorrect prs:\n",
      "0.26190298795700073  Caroline\n",
      "0.4958469569683075  Katie\n",
      "0.005412210710346699  Trevor\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Peter, Richard and Timothy went to the house. Peter and Richard gave a apple to\n",
      "correct prs:\n",
      "0.1838369071483612  Timothy\n",
      "incorrect prs:\n",
      "0.7555700540542603  Peter\n",
      "0.05766558274626732  Richard\n",
      "0.0029274190310388803  Sebastian\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Tyler, Melissa and Amanda went to the restaurant. Melissa and Tyler gave a drink to\n",
      "correct prs:\n",
      "0.2751378118991852  Amanda\n",
      "incorrect prs:\n",
      "0.11453130841255188  Melissa\n",
      "0.6103308200836182  Tyler\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards Megan, Lucas and Jared went to the garden. Megan and Lucas gave a snack to\n",
      "correct prs:\n",
      "0.24228855967521667  Jared\n",
      "incorrect prs:\n",
      "0.4557517170906067  Lucas\n",
      "0.301959753036499  Megan\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Kennedy, Fernando and Andrea went to the school. Fernando and Kennedy gave a bone to\n",
      "correct prs:\n",
      "0.2132282555103302  Andrea\n",
      "incorrect prs:\n",
      "0.7687004208564758  Fernando\n",
      "0.018071327358484268  Kennedy\n",
      "failed on this data point:\n",
      "<|endoftext|>Then, Henry, Carson and Isaac went to the garden. Isaac and Henry gave a kiss to\n",
      "correct prs:\n",
      "0.3001061975955963  Carson\n",
      "incorrect prs:\n",
      "0.0031214586924761534  Cole\n",
      "0.4592883586883545  Henry\n",
      "0.2374839335680008  Isaac\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards Isabel, Steven and Lindsey went to the school. Steven and Isabel gave a kiss to\n",
      "correct prs:\n",
      "0.368106871843338  Lindsey\n",
      "incorrect prs:\n",
      "0.18715868890285492  Isabel\n",
      "0.44473445415496826  Steven\n",
      "failed on this data point:\n",
      "<|endoftext|>Then, Sophie, Jonathan and Caroline went to the station. Sophie and Caroline gave a apple to\n",
      "correct prs:\n",
      "0.453285276889801  Jonathan\n",
      "incorrect prs:\n",
      "0.037167664617300034  Caroline\n",
      "0.5095470547676086  Sophie\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Parker, Evan and Allison went to the house. Parker and Allison gave a apple to\n",
      "correct prs:\n",
      "0.4090804159641266  Evan\n",
      "incorrect prs:\n",
      "0.010590064339339733  Allison\n",
      "0.0002117567346431315  Kenneth\n",
      "0.5801178216934204  Parker\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards Luis, Allison and Hannah went to the house. Hannah and Luis gave a bone to\n",
      "correct prs:\n",
      "0.4254439175128937  Allison\n",
      "incorrect prs:\n",
      "0.060995377600193024  Hannah\n",
      "0.001233173650689423  Katherine\n",
      "0.5123276114463806  Luis\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards Luis, Katherine and Hannah went to the house. Hannah and Luis gave a bone to\n",
      "correct prs:\n",
      "0.40658360719680786  Katherine\n",
      "incorrect prs:\n",
      "0.0010809185914695263  Allison\n",
      "0.07512538880109787  Hannah\n",
      "0.5172101259231567  Luis\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Nolan, Blake and Gregory went to the school. Blake and Nolan gave a necklace to\n",
      "correct prs:\n",
      "0.250016450881958  Gregory\n",
      "incorrect prs:\n",
      "0.2610858380794525  Blake\n",
      "0.48889774084091187  Nolan\n",
      "failed on this data point:\n",
      "<|endoftext|>Then, Ivan, Carter and David went to the house. Ivan and David gave a snack to\n",
      "correct prs:\n",
      "0.3859652578830719  Carter\n",
      "incorrect prs:\n",
      "0.08020540326833725  David\n",
      "0.5266004204750061  Ivan\n",
      "0.00722892303019762  Justin\n",
      "failed on this data point:\n",
      "<|endoftext|>Then, Brady, Alan and Jeffrey went to the hospital. Alan and Brady gave a basketball to\n",
      "correct prs:\n",
      "0.3776753842830658  Jeffrey\n",
      "incorrect prs:\n",
      "0.5813324451446533  Alan\n",
      "0.04099215194582939  Brady\n",
      "failed on this data point:\n",
      "<|endoftext|>Then, Jorge, Jake and Cameron went to the garden. Jake and Jorge gave a snack to\n",
      "correct prs:\n",
      "0.16350339353084564  Cameron\n",
      "incorrect prs:\n",
      "0.5828945636749268  Jake\n",
      "0.2536020278930664  Jorge\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Lucy, Grant and Victoria went to the office. Lucy and Grant gave a apple to\n",
      "correct prs:\n",
      "0.4390901029109955  Victoria\n",
      "incorrect prs:\n",
      "0.028157765045762062  Grant\n",
      "0.5327521562576294  Lucy\n",
      "failed on this data point:\n",
      "<|endoftext|>Then, Miguel, Sierra and Jared went to the house. Sierra and Miguel gave a kiss to\n",
      "correct prs:\n",
      "0.3988560736179352  Jared\n",
      "incorrect prs:\n",
      "0.001279194955714047  Leslie\n",
      "0.12797527015209198  Miguel\n",
      "0.4718894958496094  Sierra\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Noah, Sebastian and Brandon went to the hospital. Noah and Sebastian gave a snack to\n",
      "correct prs:\n",
      "0.37587597966194153  Brandon\n",
      "incorrect prs:\n",
      "0.5754498243331909  Noah\n",
      "0.048674192279577255  Sebastian\n",
      "failed on this data point:\n",
      "<|endoftext|>Then, Paul, Shelby and Jeffrey went to the garden. Shelby and Paul gave a basketball to\n",
      "correct prs:\n",
      "0.2531343102455139  Jeffrey\n",
      "incorrect prs:\n",
      "0.39352336525917053  Paul\n",
      "0.35334232449531555  Shelby\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards Gregory, Timothy and Matthew went to the garden. Timothy and Matthew gave a snack to\n",
      "correct prs:\n",
      "0.04486716911196709  Gregory\n",
      "incorrect prs:\n",
      "0.28587836027145386  Matthew\n",
      "0.004443123005330563  Riley\n",
      "0.6648113131523132  Timothy\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards Gregory, Genesis and Charles went to the school. Gregory and Charles gave a snack to\n",
      "correct prs:\n",
      "0.27295270562171936  Genesis\n",
      "incorrect prs:\n",
      "0.30548009276390076  Charles\n",
      "0.3980514109134674  Gregory\n",
      "0.02351580373942852  Timothy\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards William, Hannah and Charles went to the hospital. Charles and William gave a basketball to\n",
      "correct prs:\n",
      "0.36400166153907776  Hannah\n",
      "incorrect prs:\n",
      "0.1968749463558197  Charles\n",
      "0.4391234219074249  William\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Lily, Adrian and Nathan went to the hospital. Lily and Adrian gave a bone to\n",
      "correct prs:\n",
      "0.4203762114048004  Nathan\n",
      "incorrect prs:\n",
      "0.13869504630565643  Adrian\n",
      "0.44092875719070435  Lily\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Genesis, Nathan and Leslie went to the restaurant. Leslie and Nathan gave a bone to\n",
      "correct prs:\n",
      "0.3197724223136902  Genesis\n",
      "incorrect prs:\n",
      "0.011367478407919407  Angela\n",
      "0.29608485102653503  Leslie\n",
      "0.37277522683143616  Nathan\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Katie, Robert and Alexandria went to the office. Robert and Katie gave a necklace to\n",
      "correct prs:\n",
      "0.1105879694223404  Alexandria\n",
      "incorrect prs:\n",
      "0.669276773929596  Katie\n",
      "0.20043693482875824  Robert\n",
      "0.01969824731349945  Thomas\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards Andrew, Amy and Evan went to the office. Andrew and Amy gave a snack to\n",
      "correct prs:\n",
      "0.25399309396743774  Evan\n",
      "incorrect prs:\n",
      "0.28270819783210754  Amy\n",
      "0.4521056115627289  Andrew\n",
      "0.011193125508725643  Lily\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards Eric, Sean and Tyler went to the house. Eric and Sean gave a basketball to\n",
      "correct prs:\n",
      "0.4099655747413635  Tyler\n",
      "incorrect prs:\n",
      "0.5436820387840271  Eric\n",
      "0.04635244235396385  Sean\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards Anna, Miranda and Kyle went to the house. Anna and Kyle gave a snack to\n",
      "correct prs:\n",
      "0.46466514468193054  Miranda\n",
      "incorrect prs:\n",
      "0.518332839012146  Anna\n",
      "0.017002010717988014  Kyle\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards Lauren, Jorge and Leslie went to the school. Leslie and Lauren gave a snack to\n",
      "correct prs:\n",
      "0.3542657196521759  Jorge\n",
      "incorrect prs:\n",
      "0.5156980156898499  Lauren\n",
      "0.12887544929981232  Leslie\n",
      "0.001160869374871254  Parker\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards Lauren, Parker and Leslie went to the school. Leslie and Lauren gave a snack to\n",
      "correct prs:\n",
      "0.2528664767742157  Parker\n",
      "incorrect prs:\n",
      "0.0009681918891146779  Jorge\n",
      "0.5458744168281555  Lauren\n",
      "0.2002909630537033  Leslie\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Katie, Caroline and Raymond went to the station. Caroline and Katie gave a necklace to\n",
      "correct prs:\n",
      "0.2325407862663269  Raymond\n",
      "incorrect prs:\n",
      "0.2655881643295288  Caroline\n",
      "0.4964310824871063  Katie\n",
      "0.005439955275505781  Trevor\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Katie, Caroline and Trevor went to the station. Caroline and Katie gave a necklace to\n",
      "correct prs:\n",
      "0.13594283163547516  Trevor\n",
      "incorrect prs:\n",
      "0.29289674758911133  Caroline\n",
      "0.5706468224525452  Katie\n",
      "0.0005135659594088793  Raymond\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Peter, Richard and Timothy went to the house. Peter and Richard gave a apple to\n",
      "correct prs:\n",
      "0.18742936849594116  Timothy\n",
      "incorrect prs:\n",
      "0.7538506984710693  Peter\n",
      "0.05584276467561722  Richard\n",
      "0.00287712924182415  Sebastian\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Tyler, Melissa and Amanda went to the restaurant. Melissa and Tyler gave a drink to\n",
      "correct prs:\n",
      "0.2712344825267792  Amanda\n",
      "incorrect prs:\n",
      "0.11501757055521011  Melissa\n",
      "0.6137480139732361  Tyler\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards Megan, Lucas and Jared went to the garden. Megan and Lucas gave a snack to\n",
      "correct prs:\n",
      "0.24525494873523712  Jared\n",
      "incorrect prs:\n",
      "0.4551640450954437  Lucas\n",
      "0.29958105087280273  Megan\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Kennedy, Fernando and Andrea went to the school. Fernando and Kennedy gave a bone to\n",
      "correct prs:\n",
      "0.2107558399438858  Andrea\n",
      "incorrect prs:\n",
      "0.7713459730148315  Fernando\n",
      "0.017898159101605415  Kennedy\n",
      "failed on this data point:\n",
      "<|endoftext|>Then, Henry, Carson and Isaac went to the garden. Isaac and Henry gave a kiss to\n",
      "correct prs:\n",
      "0.293459415435791  Carson\n",
      "incorrect prs:\n",
      "0.0029634181410074234  Cole\n",
      "0.461674302816391  Henry\n",
      "0.2419029027223587  Isaac\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards Isabel, Steven and Lindsey went to the school. Steven and Isabel gave a kiss to\n",
      "correct prs:\n",
      "0.3581514358520508  Lindsey\n",
      "incorrect prs:\n",
      "0.19044004380702972  Isabel\n",
      "0.4514084458351135  Steven\n",
      "failed on this data point:\n",
      "<|endoftext|>Then, Sophie, Jonathan and Caroline went to the station. Sophie and Caroline gave a apple to\n",
      "correct prs:\n",
      "0.44705891609191895  Jonathan\n",
      "incorrect prs:\n",
      "0.0375027097761631  Caroline\n",
      "0.5154383182525635  Sophie\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Parker, Evan and Allison went to the house. Parker and Allison gave a apple to\n",
      "correct prs:\n",
      "0.40961721539497375  Evan\n",
      "incorrect prs:\n",
      "0.010785802267491817  Allison\n",
      "0.00021166610531508923  Kenneth\n",
      "0.5793852806091309  Parker\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards Luis, Katherine and Hannah went to the house. Hannah and Luis gave a bone to\n",
      "correct prs:\n",
      "0.40242868661880493  Katherine\n",
      "incorrect prs:\n",
      "0.0011176554253324866  Allison\n",
      "0.07765605300664902  Hannah\n",
      "0.5187975764274597  Luis\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards Luis, Allison and Hannah went to the house. Hannah and Luis gave a bone to\n",
      "correct prs:\n",
      "0.42246001958847046  Allison\n",
      "incorrect prs:\n",
      "0.05894865468144417  Hannah\n",
      "0.0012282192474231124  Katherine\n",
      "0.5173630714416504  Luis\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Nolan, Blake and Gregory went to the school. Blake and Nolan gave a necklace to\n",
      "correct prs:\n",
      "0.2443380206823349  Gregory\n",
      "incorrect prs:\n",
      "0.25916990637779236  Blake\n",
      "0.49649202823638916  Nolan\n",
      "failed on this data point:\n",
      "<|endoftext|>Then, Ivan, Carter and David went to the house. Ivan and David gave a snack to\n",
      "correct prs:\n",
      "0.388973206281662  Carter\n",
      "incorrect prs:\n",
      "0.08162081241607666  David\n",
      "0.5219576358795166  Ivan\n",
      "0.007448315620422363  Justin\n",
      "failed on this data point:\n",
      "<|endoftext|>Then, Brady, Alan and Jeffrey went to the hospital. Alan and Brady gave a basketball to\n",
      "correct prs:\n",
      "0.3810572922229767  Jeffrey\n",
      "incorrect prs:\n",
      "0.57691890001297  Alan\n",
      "0.04202378913760185  Brady\n",
      "failed on this data point:\n",
      "<|endoftext|>Then, Jorge, Jake and Cameron went to the garden. Jake and Jorge gave a snack to\n",
      "correct prs:\n",
      "0.15673604607582092  Cameron\n",
      "incorrect prs:\n",
      "0.589480459690094  Jake\n",
      "0.2537834942340851  Jorge\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Lucy, Grant and Victoria went to the office. Lucy and Grant gave a apple to\n",
      "correct prs:\n",
      "0.44615477323532104  Victoria\n",
      "incorrect prs:\n",
      "0.027628304436802864  Grant\n",
      "0.5262169241905212  Lucy\n",
      "failed on this data point:\n",
      "<|endoftext|>Then, Miguel, Sierra and Jared went to the house. Sierra and Miguel gave a kiss to\n",
      "correct prs:\n",
      "0.38681483268737793  Jared\n",
      "incorrect prs:\n",
      "0.0012323049595579505  Leslie\n",
      "0.12666916847229004  Miguel\n",
      "0.48528367280960083  Sierra\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Noah, Sebastian and Brandon went to the hospital. Noah and Sebastian gave a snack to\n",
      "correct prs:\n",
      "0.37327656149864197  Brandon\n",
      "incorrect prs:\n",
      "0.578618049621582  Noah\n",
      "0.04810534045100212  Sebastian\n",
      "failed on this data point:\n",
      "<|endoftext|>Then, Paul, Shelby and Jeffrey went to the garden. Shelby and Paul gave a basketball to\n",
      "correct prs:\n",
      "0.25151294469833374  Jeffrey\n",
      "incorrect prs:\n",
      "0.4030799865722656  Paul\n",
      "0.345407098531723  Shelby\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards Gregory, Timothy and Matthew went to the garden. Timothy and Matthew gave a snack to\n",
      "correct prs:\n",
      "0.05120861530303955  Gregory\n",
      "incorrect prs:\n",
      "0.27687883377075195  Matthew\n",
      "0.004456991329789162  Riley\n",
      "0.6674555540084839  Timothy\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards Gregory, Genesis and Charles went to the school. Gregory and Charles gave a snack to\n",
      "correct prs:\n",
      "0.2678346335887909  Genesis\n",
      "incorrect prs:\n",
      "0.31384122371673584  Charles\n",
      "0.39450380206108093  Gregory\n",
      "0.023820320144295692  Timothy\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards William, Hannah and Charles went to the hospital. Charles and William gave a basketball to\n",
      "correct prs:\n",
      "0.3684155344963074  Hannah\n",
      "incorrect prs:\n",
      "0.19645042717456818  Charles\n",
      "0.4351341426372528  William\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Lily, Adrian and Nathan went to the hospital. Lily and Adrian gave a bone to\n",
      "correct prs:\n",
      "0.41318345069885254  Nathan\n",
      "incorrect prs:\n",
      "0.13932479918003082  Adrian\n",
      "0.44749170541763306  Lily\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Genesis, Nathan and Leslie went to the restaurant. Leslie and Nathan gave a bone to\n",
      "correct prs:\n",
      "0.3128919303417206  Genesis\n",
      "incorrect prs:\n",
      "0.01132222730666399  Angela\n",
      "0.3059902787208557  Leslie\n",
      "0.3697955012321472  Nathan\n",
      "failed on this data point:\n",
      "<|endoftext|>Friends Katie, Robert and Alexandria went to the office. Robert and Katie gave a necklace to\n",
      "correct prs:\n",
      "0.11277931928634644  Alexandria\n",
      "incorrect prs:\n",
      "0.6683989763259888  Katie\n",
      "0.19902962446212769  Robert\n",
      "0.01979214884340763  Thomas\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards Andrew, Amy and Evan went to the office. Andrew and Amy gave a snack to\n",
      "correct prs:\n",
      "0.25357308983802795  Evan\n",
      "incorrect prs:\n",
      "0.28767403960227966  Amy\n",
      "0.4478805363178253  Andrew\n",
      "0.010872350074350834  Lily\n",
      "failed on this data point:\n",
      "<|endoftext|>Afterwards Eric, Sean and Tyler went to the house. Eric and Sean gave a basketball to\n",
      "correct prs:\n",
      "0.4093315303325653  Tyler\n",
      "incorrect prs:\n",
      "0.5447481274604797  Eric\n",
      "0.04592038691043854  Sean\n",
      "patching with SAEs, got accuracy 0.7583358883857727\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "sae_hooks = [(f'blocks.{layer}.hook_layer_input', partial(sae_hook, layer=layer)) for layer in range(15,16)]\n",
    "model_kwargs = {\n",
    "    \"fast_ssm\": True,\n",
    "    \"fast_conv\": True\n",
    "}\n",
    "from acdc import ACDCEvalData, get_pad_token\n",
    "def logging_incorrect_metric(data: ACDCEvalData):\n",
    "    pad_token = get_pad_token(model.tokenizer)\n",
    "    for data_subset in [data.patched, data.corrupted]:\n",
    "        batch, _ = data_subset.data.size()\n",
    "        for b in range(batch):\n",
    "            if not data_subset.top_is_correct[b].item():\n",
    "                toks = data_subset.data[b][:data_subset.last_token_position[b]+1]\n",
    "                print(\"failed on this data point:\")\n",
    "                print(model.tokenizer.decode(toks))\n",
    "                print(\"correct prs:\")\n",
    "                for i, tok in enumerate(data_subset.correct[b]):\n",
    "                    if tok.item() != pad_token:\n",
    "                        print(data_subset.correct_prs[b,i].item(), model.tokenizer.decode([tok.item()]))\n",
    "                print(\"incorrect prs:\")\n",
    "                for i, tok in enumerate(data_subset.incorrect[b]):\n",
    "                    if tok.item() != pad_token:\n",
    "                        print(data_subset.incorrect_prs[b,i].item(), model.tokenizer.decode([tok.item()]))\n",
    "    return data.patched.correct_prs[:,0]\n",
    "\n",
    "from acdc import accuracy_metric, wrap_run_with_hooks\n",
    "acc = data.eval(model=wrap_run_with_hooks(model=model, fwd_hooks=sae_hooks, **model_kwargs),\n",
    "                batch_size=10,\n",
    "                metric=logging_incorrect_metric).mean()\n",
    "#acc_no_sae = data.eval(model=wrap_run_with_hooks(model=model, fwd_hooks=[], **model_kwargs),\n",
    "#                batch_size=10,\n",
    "#                metric=accuracy_metric).mean()\n",
    "print(f\"patching with SAEs, got accuracy {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12f2588-c57e-4a9c-a41c-a26de661a26e",
   "metadata": {},
   "source": [
    "# Run Edge Attribution Patching with Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14fc6b9-7646-449e-ad90-9ddb600e9f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1344: UserWarning:\n",
      "\n",
      "Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha 0.0 metric 1.0\n",
      "alpha 0.25 metric 0.2686183750629425\n",
      "alpha 0.5 metric 0.040551550686359406\n",
      "alpha 0.75 metric 0.002687363652512431\n",
      "alpha 1.0 metric 9.968756842226867e-08\n",
      "20 40\n",
      "alpha 0.0 metric 1.000000238418579\n",
      "alpha 0.25 metric 0.24507883191108704\n",
      "alpha 0.5 metric 0.036623287945985794\n",
      "alpha 0.75 metric 0.002468405058607459\n",
      "alpha 1.0 metric 8.330686540602983e-08\n",
      "40 60\n",
      "alpha 0.0 metric 1.0000001192092896\n",
      "alpha 0.25 metric 0.2676418423652649\n",
      "alpha 0.5 metric 0.040014296770095825\n",
      "alpha 0.75 metric 0.002649752190336585\n",
      "alpha 1.0 metric 7.570692517333555e-09\n",
      "60 80\n",
      "alpha 0.0 metric 0.9999998211860657\n",
      "alpha 0.25 metric 0.2508822977542877\n",
      "alpha 0.5 metric 0.03671620413661003\n",
      "alpha 0.75 metric 0.002453301101922989\n",
      "alpha 1.0 metric 8.819274910365493e-08\n",
      "80 100\n",
      "alpha 0.0 metric 0.9999999403953552\n",
      "alpha 0.25 metric 0.2485286295413971\n",
      "alpha 0.5 metric 0.03709960728883743\n",
      "alpha 0.75 metric 0.002484970958903432\n",
      "alpha 1.0 metric 2.2463675364292612e-08\n",
      "100 120\n",
      "alpha 0.0 metric 0.9999998211860657\n",
      "alpha 0.25 metric 0.26231157779693604\n",
      "alpha 0.5 metric 0.03913697227835655\n",
      "alpha 0.75 metric 0.002637533936649561\n",
      "alpha 1.0 metric 9.492233488117563e-08\n",
      "120 140\n",
      "alpha 0.0 metric 1.000000238418579\n",
      "alpha 0.25 metric 0.24860747158527374\n",
      "alpha 0.5 metric 0.037431132048368454\n"
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "from functools import partial\n",
    "from jaxtyping import Float\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from mamba_lens.input_dependent_hooks import clean_hooks\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "model_kwargs = {\n",
    "    'fast_ssm': False,\n",
    "    'fast_conv': False\n",
    "}\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "# removes all hooks including \"leftover\" ones that might stick around due to interrupting the model at certain times\n",
    "clean_hooks(model)\n",
    "\n",
    "def normalized_logit_diff_metric(patched_logits, unpatched_logits, corrupted_logits, patched_correct, corrupted_correct, also_return_acc=False):\n",
    "    B,V = patched_logits.size()\n",
    "\n",
    "    #print(data.unpatched.logits.size(), data.patched.logits.size(), data.corrupted.logits.size())\n",
    "    A_logits_unpatched = unpatched_logits[torch.arange(B), patched_correct]\n",
    "    A_logits_patched = patched_logits[torch.arange(B), patched_correct]\n",
    "    A_logits_corrupted = corrupted_logits[torch.arange(B), patched_correct]\n",
    "\n",
    "    B_logits_unpatched = unpatched_logits[torch.arange(B), corrupted_correct]\n",
    "    B_logits_patched = patched_logits[torch.arange(B), corrupted_correct]\n",
    "    B_logits_corrupted = corrupted_logits[torch.arange(B), corrupted_correct]\n",
    "\n",
    "    # A and B are two potential outputs\n",
    "    # if A patched > B patched, we are correct\n",
    "    # else we are incorrect\n",
    "\n",
    "    # thus we could just return A_logits_patched - B_logits_patched\n",
    "\n",
    "    # however it is useful to \"normalize\" these values\n",
    "\n",
    "    # in the worst case, our patching causes us to act like corrupted, and our diff will be\n",
    "    # A_logits_corrupted - B_logits_corrupted\n",
    "    # this will result in a small, negative value\n",
    "    \n",
    "    # in the best case, our patching will do nothing (cause us to act like unpatched), and our diff will be\n",
    "    # A_logits_unpatched - B_logits_unpatched\n",
    "    # this will result in a large, positive value\n",
    "\n",
    "    # thus we can treat those as the \"min\" and \"max\" and normalize accordingly\n",
    "    \n",
    "    min_diff = A_logits_corrupted - B_logits_corrupted\n",
    "    max_diff = A_logits_unpatched - B_logits_unpatched\n",
    "\n",
    "    # the abs ensures that if it's wrong we don't try and make it more wrong\n",
    "    possible_range = torch.abs(max_diff-min_diff)\n",
    "    possible_range[possible_range == 0] = 1.0 # prevent divide by zero\n",
    "    \n",
    "    diff = A_logits_patched - B_logits_patched\n",
    "    \n",
    "    normalized_diff = (diff-min_diff)/possible_range\n",
    "\n",
    "    # as described, 1.0 corresponds to acting like unpatched,\n",
    "    # and 0.0 corresponds to acting like corrupted\n",
    "    res = torch.mean(normalized_diff)\n",
    "    \n",
    "    if also_return_acc:\n",
    "        num_correct = A_logits_patched > B_logits_patched\n",
    "        acc = torch.sum(num_correct)/B\n",
    "        return res, acc\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "\n",
    "# there's a subtle bug if you aren't careful:\n",
    "# consider what happens when we do edge attribution patching and patch every edge\n",
    "# what we want to happen is that it's identical to corrupted\n",
    "# however this is not what happens!\n",
    "# Start with layer 0:\n",
    "# layer 0 will be patched\n",
    "#    we subtract uncorrupted embed and add corrupted embed\n",
    "#    in other words, the embed input to layer 0 will be from the corrupted run\n",
    "# this results in layer 0 having an output of corrupted, as desired\n",
    "# now consider layer 1\n",
    "#    subtract uncorrupted embed and add corrupted embed\n",
    "#      this is fine and results in embed input to layer 1 of corrupted\n",
    "#    subtract uncorrupted layer 0 and add corrupted layer 0\n",
    "#      layer 0 is already corrupted, so this has the effect of the output of layer 0 being\n",
    "#          2*corrupted layer 0 - uncorrupted layer 0\n",
    "#          this is not the same as corrupted!\n",
    "\n",
    "# two ways to fix this:\n",
    "# 1. fetch stored layer_input from uncorrupted run, and use that instead of the layer_input given in fwd_diff\n",
    "#   this works, but then the gradients won't propogate properly (maybe? todo: test)\n",
    "# 2. mark which edges are patched and don't \"double patch\" them\n",
    "#   if we are patching all edges, this means that we simply apply only the embed diff to all layers,\n",
    "#   as that'll result in all layers being patched\n",
    "# 3. just subtract the outputs in the same forward pass, instead of a cached \"unpatched\" run\n",
    "#  we do 3\n",
    "\n",
    "global alpha\n",
    "alpha = 1\n",
    "\n",
    "global cached_outputs\n",
    "global corrupted_outputs\n",
    "\n",
    "def cache_output_hook( # hook_layer_output\n",
    "    layer_output : Float[torch.Tensor, \"B L D\"],\n",
    "    hook : HookPoint,\n",
    "    layer : int):\n",
    "    global cached_outputs\n",
    "    cached_outputs[layer+1] = layer_output\n",
    "    return layer_output\n",
    "\n",
    "def fwd_diff_hook( # hook_layer_input\n",
    "    layer_input : Float[torch.Tensor, \"B L D\"],\n",
    "    hook : HookPoint,\n",
    "    layer : int):\n",
    "    global alpha\n",
    "    global cached_outputs\n",
    "    global corrupted_outputs\n",
    "    return layer_input + (-cached_outputs[:layer+1]+corrupted_outputs[:layer+1]).sum(dim=0)*alpha\n",
    "\n",
    "global attributions\n",
    "def bwd_diff_hook( # hook_layer_input\n",
    "    grad : Float[torch.Tensor, \"B L D\"],\n",
    "    hook: HookPoint,\n",
    "    layer : int,\n",
    "    batch_start: int,\n",
    "    batch_end: int):\n",
    "    #print(f\"running bwd with alpha {alpha} and layer {layer}\") \n",
    "    global cached_outputs\n",
    "    global corrupted_outputs\n",
    "    global attributions\n",
    "    # [N_upstream, B, L, D]\n",
    "    upstream_diffs = (-cached_outputs[:layer+1]+corrupted_outputs[:layer+1])\n",
    "    # grad is [B,L,D]\n",
    "\n",
    "    # to do a taylor approximation of metric with respect to diff_0, we\n",
    "    # multiply diffs and grad, then\n",
    "    # sum over the L and D dimensions (this is doing a dot product of vectors of size L*D)\n",
    "    # now we have attr of size [N_upstream, B, L]\n",
    "    attr = (grad*upstream_diffs).sum(dim=-1)\n",
    "    # [B, N_upstream, L]           [N_upstream, B, L]\n",
    "    attr         =   torch.transpose(attr, 0, 1)\n",
    "    # [B, N_upstream, L]\n",
    "    attr = attr.clone().detach()\n",
    "    if POSITIONS:\n",
    "        attributions[batch_start:batch_end,:layer+1,layer+1] += attr\n",
    "    else:\n",
    "        #[B, N_upstream]\n",
    "        attr = attr.sum(dim=-1)\n",
    "        attributions[batch_start:batch_end,:layer+1,layer+1] += attr\n",
    "\n",
    "\n",
    "\n",
    "def cache_conv_hook(\n",
    "    conv_input: Float[torch.Tensor, \"B L E\"],\n",
    "    hook: HookPoint,\n",
    "    layer: int):\n",
    "    global cached_conv_inputs\n",
    "    cached_conv_inputs.append(conv_input)\n",
    "    return conv_input\n",
    "\n",
    "# for each conv slice,\n",
    "# we have \n",
    "global slice_terms\n",
    "global cached_conv_inputs\n",
    "global corrupted_conv_inputs\n",
    "def conv_patching_hook(\n",
    "    conv_output: Float[torch.Tensor, \"B L E\"],\n",
    "    hook: HookPoint,\n",
    "    layer: int,\n",
    ") -> Float[torch.Tensor, \"B L E\"]:\n",
    "    ### This is identical to what the conv is doing\n",
    "    # but we break it apart so we can patch on individual slices\n",
    "    global alpha\n",
    "    global slice_terms\n",
    "    global cached_conv_inputs\n",
    "    global corrupted_conv_inputs\n",
    "\n",
    "    # we have two input hooks, the second one is the one we want\n",
    "    D_CONV = model.cfg.d_conv\n",
    "\n",
    "    # [E,1,D_CONV]\n",
    "    conv_weight = model.blocks[layer].conv1d.weight\n",
    "    # [E]\n",
    "    conv_bias = model.blocks[layer].conv1d.bias\n",
    "    uncorrupted_input = rearrange(cached_conv_inputs[layer], 'B L E -> B E L')\n",
    "    corrupted_input = rearrange(corrupted_conv_inputs[layer], 'B L E -> B E L')\n",
    "    B,E,L = uncorrupted_input.size()\n",
    "    uncorrupted_input = torch.nn.functional.pad(uncorrupted_input, (D_CONV-1,0), mode='constant', value=0)\n",
    "    corrupted_input = torch.nn.functional.pad(corrupted_input, (D_CONV-1,0), mode='constant', value=0)\n",
    "    #print(uncorrupted_input.size(), corrupted_input.size())\n",
    "    # we want some \"patch hook\" thing that uses autodiff or somethin\n",
    "    output = torch.zeros([B,E,L], device=model.cfg.device)\n",
    "    layer_slices = []\n",
    "    for i in range(D_CONV):\n",
    "        # [B,E,L]                      [E,1]                      [B,E,L]\n",
    "        uncorrupted_contribution = conv_weight[:,0,i].view(E,1)*uncorrupted_input[:,:,i:i+L]\n",
    "        corrupted_contribution = conv_weight[:,0,i].view(E,1)*corrupted_input[:,:,i:i+L]\n",
    "        slice_term = uncorrupted_contribution*(1-alpha)+corrupted_contribution*(alpha)\n",
    "        diff = -uncorrupted_contribution+corrupted_contribution\n",
    "        slice_term.retain_grad()\n",
    "        #slice_term.requires_grad = True\n",
    "        layer_slices.append((diff, slice_term))\n",
    "        output += slice_term\n",
    "    output = rearrange(output, 'B E L -> B L E')\n",
    "    output += conv_bias\n",
    "    slice_terms.append(layer_slices)\n",
    "    return output\n",
    "\n",
    "def bwd_conv_hook(\n",
    "    grad : Float[torch.Tensor, \"B L E\"], # we don't use this, hook is just so we are present in backward pass\n",
    "    hook: HookPoint,\n",
    "    layer : int,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "):\n",
    "    global slice_terms\n",
    "    global cached_conv_inputs\n",
    "    global corrupted_conv_inputs\n",
    "    # [B, L, E]\n",
    "    #diffs = (-cached_conv_inputs[layer]+corrupted_conv_inputs[layer])\n",
    "    layer_slice_terms = slice_terms[layer]\n",
    "    for slice_i, (diff, slice_term) in enumerate(layer_slice_terms):\n",
    "        # [B,E,L]\n",
    "        slice_grad = slice_term.grad\n",
    "        # dot product over the E dimension\n",
    "        # [B,L]         [B,E,L]   [B,E,L]\n",
    "        slice_attr = (slice_grad * diff).sum(dim=-2) # sum over E dimension\n",
    "        if POSITIONS:\n",
    "            # [B,L]\n",
    "            conv_attributions[batch_start:batch_end,layer,slice_i] += slice_attr \n",
    "        else:\n",
    "            #[B]\n",
    "            slice_attr = slice_attr.sum(dim=-1)\n",
    "            conv_attributions[batch_start:batch_end,layer,slice_i] += slice_attr\n",
    "\n",
    "B,L = data.data.size()\n",
    "# our data is pairs of unpatched, corrupted\n",
    "n_patching_pairs = B//2\n",
    "D_CONV = model.cfg.d_conv\n",
    "\n",
    "# attributions[b,i+1,j+1] is the (i->j) edge attribution for patching pair b\n",
    "# attributions[b,0,j] is the (embed->j) edge attribution for patching pair b\n",
    "# attributions[b,i,-1] is the (i->output) edge attribution for patching pair b\n",
    "POSITIONS = True\n",
    "if POSITIONS:\n",
    "    attributions = torch.zeros([n_patching_pairs, model.cfg.n_layers+2, model.cfg.n_layers+2, L], device=model.cfg.device)\n",
    "    conv_attributions = torch.zeros([n_patching_pairs, model.cfg.n_layers, D_CONV, L], device=model.cfg.device)\n",
    "else:\n",
    "    attributions = torch.zeros([n_patching_pairs, model.cfg.n_layers+2, model.cfg.n_layers+2], device=model.cfg.device)\n",
    "    conv_attributions = torch.zeros([n_patching_pairs, model.cfg.n_layers, D_CONV], device=model.cfg.device)\n",
    "\n",
    "\n",
    "\n",
    "input_names = [f'blocks.{i}.hook_layer_input' for i in range(model.cfg.n_layers)]\n",
    "output_names = [f'blocks.{i}.hook_out_proj' for i in range(model.cfg.n_layers)]\n",
    "conv_input_names = [f'blocks.{i}.hook_in_proj' for i in range(model.cfg.n_layers)]\n",
    "conv_names = [f'blocks.{i}.hook_conv' for i in range(model.cfg.n_layers)]\n",
    "\n",
    "for batch_start in range(0, n_patching_pairs, BATCH_SIZE):\n",
    "    batch_end = min(batch_start + BATCH_SIZE, n_patching_pairs)\n",
    "    print(batch_start, batch_end)\n",
    "    # we don't need grad for these forward passes\n",
    "    torch.set_grad_enabled(False)\n",
    "    embed_name = 'hook_embed'\n",
    "\n",
    "    clean_hooks(model)\n",
    "    # forward passes to get unpatched and corrupted\n",
    "    unpatched_logits = model(data.data[::2][batch_start:batch_end], **model_kwargs)\n",
    "    corrupted_logits, corrupted_layer_outputs = model.run_with_cache(data.data[1::2][batch_start:batch_end], names_filter=[embed_name] + output_names + conv_input_names, **model_kwargs)\n",
    "    \n",
    "    batch_size,L,D = corrupted_layer_outputs[output_names[0]].size()\n",
    "    _,_,E = corrupted_layer_outputs[conv_input_names[0]].size()\n",
    "    \n",
    "    # get only the last token position (logit for next predicted token)\n",
    "    # this is needed to support data of varying lengths\n",
    "    unpatched_logits = unpatched_logits[torch.arange(batch_size), data.last_token_position[::2][batch_start:batch_end]]\n",
    "    corrupted_logits = corrupted_logits[torch.arange(batch_size), data.last_token_position[1::2][batch_start:batch_end]]\n",
    "    \n",
    "    clean_hooks(model)\n",
    "\n",
    "    # backward pass to compute grad of diff\n",
    "    torch.set_grad_enabled(True)\n",
    "        \n",
    "    corrupted_outputs = torch.zeros([model.cfg.n_layers+1,batch_size,L,D], device=model.cfg.device)\n",
    "    corrupted_conv_inputs = torch.zeros([model.cfg.n_layers,batch_size,L,E], device=model.cfg.device)\n",
    "    corrupted_outputs.requires_grad = False\n",
    "    # first one is for embed\n",
    "    corrupted_outputs[0] = corrupted_layer_outputs[embed_name]\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        output_name = output_names[layer]\n",
    "        corrupted_outputs[layer+1] = corrupted_layer_outputs[output_name]\n",
    "        corrupted_conv_inputs[layer] = corrupted_layer_outputs[conv_input_names[layer]]    \n",
    "    # cleanup\n",
    "    del corrupted_layer_outputs\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "        param.grad = None # reset grads\n",
    "    \n",
    "    last_layer = model.cfg.n_layers-1\n",
    "    # forward pass to do partial patches\n",
    "    cached_outputs = torch.zeros([model.cfg.n_layers+1,batch_size,L,D], device=model.cfg.device)\n",
    "    cached_outputs.requires_grad = False\n",
    "    \n",
    "    # EAP layer hooks\n",
    "    cache_output_hooks = [(embed_name,\n",
    "                           partial(cache_output_hook,\n",
    "                                   layer=-1))]\n",
    "    \n",
    "    cache_output_hooks += [(output_names[layer],\n",
    "                            partial(cache_output_hook,\n",
    "                                    layer=layer)) for layer in range(model.cfg.n_layers)]\n",
    "    \n",
    "    fwd_hooks = cache_output_hooks\n",
    "    fwd_hooks += [(input_names[layer],\n",
    "                  partial(\n",
    "                      fwd_diff_hook,\n",
    "                      layer=layer,\n",
    "                  )) for layer in range(model.cfg.n_layers)]\n",
    "    bwd_hooks = [(input_names[layer],\n",
    "                  partial(bwd_diff_hook,\n",
    "                          layer=layer,\n",
    "                          batch_start=batch_start,\n",
    "                          batch_end=batch_end)) for layer in range(model.cfg.n_layers)]\n",
    "    # extra hook for the very last layer\n",
    "    fwd_hooks.append((f'blocks.{last_layer}.hook_resid_post',\n",
    "                      partial(fwd_diff_hook,\n",
    "                              layer=last_layer+1,\n",
    "                             )))\n",
    "    bwd_hooks.append((f'blocks.{last_layer}.hook_resid_post',\n",
    "                      partial(bwd_diff_hook,\n",
    "                              layer=last_layer+1,\n",
    "                              batch_start=batch_start,\n",
    "                              batch_end=batch_end,\n",
    "                             )))\n",
    "\n",
    "    # Conv hooks\n",
    "    cached_conv_inputs = []\n",
    "    slice_terms = []\n",
    "\n",
    "    fwd_hooks += [(conv_input_names[layer],\n",
    "                  partial(\n",
    "                      cache_conv_hook,\n",
    "                      layer=layer,\n",
    "                  )) for layer in range(model.cfg.n_layers)]\n",
    "    \n",
    "    \n",
    "    fwd_hooks += [(conv_names[layer],\n",
    "                  partial(\n",
    "                      conv_patching_hook,\n",
    "                      layer=layer,\n",
    "                  )) for layer in range(model.cfg.n_layers)]\n",
    "    bwd_hooks += [(input_names[layer], # anywhere before the slice terms works, so we'll just pick start of layer since that backward is called\n",
    "                  partial(\n",
    "                      bwd_conv_hook,\n",
    "                      layer=layer,\n",
    "                      batch_start=batch_start,\n",
    "                      batch_end=batch_end,\n",
    "                  )) for layer in range(model.cfg.n_layers)]\n",
    "    \n",
    "    for fwd in fwd_hooks:\n",
    "        model.add_hook(*fwd, \"fwd\")\n",
    "\n",
    "    for bwd in bwd_hooks:\n",
    "        model.add_hook(*bwd, \"bwd\")\n",
    "    \n",
    "    # with integrated gradients\n",
    "    # simply sums over doing \"partial patches\" like 0.2 patch and 0.8 unpatched \n",
    "    # ITERS = 1 is just edge attribution patching (without integraded gradients)\n",
    "    ITERS = 5\n",
    "    for i in range(ITERS+1):\n",
    "        global alpha\n",
    "        # alpha ranges from 0 to 1\n",
    "        if ITERS > 1:\n",
    "            alpha = i/float(ITERS-1)\n",
    "        elif ITERS == 1: # no integrated gradients, set alpha to 1\n",
    "            alpha = 1.0\n",
    "\n",
    "        # it tries to propogate gradients to these, detach them\n",
    "        slice_terms.clear()\n",
    "        cached_conv_inputs.clear()\n",
    "        torch.cuda.empty_cache()\n",
    "        cached_outputs[:] = 0\n",
    "        cached_outputs.grad = None\n",
    "        cached_outputs.detach_()\n",
    "        corrupted_outputs.grad = None\n",
    "        corrupted_outputs.detach_()\n",
    "        #cached_conv_inputs.grad = None\n",
    "        #cached_conv_inputs.detach_()\n",
    "        corrupted_conv_inputs.grad = None\n",
    "        corrupted_conv_inputs.detach_()\n",
    "        model.zero_grad()\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        if i == ITERS:\n",
    "            torch.cuda.empty_cache()\n",
    "            break # we just use this for cleaning up\n",
    "        logits = model(data.data[::2][batch_start:batch_end], **model_kwargs)\n",
    "        logits = logits[torch.arange(batch_size), data.last_token_position[::2][batch_start:batch_end]]\n",
    "        metric = normalized_logit_diff_metric(\n",
    "            patched_logits=logits,\n",
    "            unpatched_logits=unpatched_logits,\n",
    "            corrupted_logits=corrupted_logits,\n",
    "            patched_correct=data.correct[::2][batch_start:batch_end][:,0],\n",
    "            corrupted_correct=data.correct[1::2][batch_start:batch_end][:,0]\n",
    "        )\n",
    "        print(f\"alpha {alpha} metric {metric}\")\n",
    "        # run backward pass, which adds to attributions\n",
    "        metric.backward()\n",
    "        #conv_attrs = conv_attributions.mean(dim=0)\n",
    "        #print(conv_attrs)\n",
    "        #attrs = attributions.mean(dim=0)\n",
    "        #print(attrs)\n",
    "\n",
    "# todo: maybe the diffs should have alpha in the backward pass? No, that would mean alpha of 0 gives all zero attrs\n",
    "\n",
    "# average over all the samples\n",
    "attributions[:] = attributions[:]/ITERS\n",
    "conv_attributions[:] = conv_attributions[:]/ITERS\n",
    "\n",
    "\n",
    "# don't need grad for rest of this\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "clean_hooks(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab16482e-a3a9-4985-99ad-e7fff25d0b3a",
   "metadata": {},
   "source": [
    "# Binary search to find circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de617a-4a2f-4154-88bc-fef42263d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save([conv_attributions, attributions], f\"patching{PATCHING_FORMAT_I}attrs.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d041cde-732a-4989-9156-f3f7a7fda77f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conv_attributions, attributions = torch.load(f\"patching{PATCHING_FORMAT_I}attrs.pkl\")\n",
    "model_kwargs = {\n",
    "    'fast_ssm': False,\n",
    "    'fast_conv': False\n",
    "}\n",
    "import sys\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import wandb\n",
    "import os\n",
    "import signal\n",
    "import acdc\n",
    "from tqdm import tqdm\n",
    "from typing import Any  \n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from acdc import (\n",
    "    Edge,\n",
    "    ACDCConfig,\n",
    "    LOG_LEVEL_INFO,\n",
    "    LOG_LEVEL_DEBUG,\n",
    "    run_acdc,\n",
    "    ACDCEvalData,\n",
    "    load_checkpoint,\n",
    "    get_most_recent_checkpoint\n",
    ")\n",
    "\n",
    "\n",
    "from acdc import get_pad_token\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "global storage\n",
    "def storage_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    **kwargs,\n",
    "):\n",
    "    global storage\n",
    "    #if hook.name == 'hook_embed':\n",
    "    #    for k in list(storage.keys()):\n",
    "    #        del storage[k]\n",
    "    storage[hook.name] = x\n",
    "    return x\n",
    "\n",
    "def resid_patching_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    input_hook_name: str,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    position: int = None,\n",
    "):\n",
    "    global storage\n",
    "    x_uncorrupted = storage[input_hook_name][batch_start:batch_end:2]\n",
    "    x_corrupted = storage[input_hook_name][batch_start+1:batch_end:2]\n",
    "    if position is None: # if position not specified, apply to all positions\n",
    "        x[batch_start:batch_end:2] = x[batch_start:batch_end:2] - x_uncorrupted + x_corrupted\n",
    "    else:\n",
    "        x[batch_start:batch_end:2,position] = x[batch_start:batch_end:2,position] - x_uncorrupted[:,position] + x_corrupted[:,position]\n",
    "    return x\n",
    "\n",
    "def overwrite_patching_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    input_hook_name: str,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    position: int = None,\n",
    "):\n",
    "    x_corrupted = x[batch_start+1:batch_end:2]\n",
    "    if position is None: # if position not specified, apply to all positions\n",
    "        x[batch_start:batch_end:2] = x_corrupted\n",
    "    else:\n",
    "        if x_corrupted.size()[1] != L: raise ValueError(f'warning: in hook {hook.name} with input_hook_name {input_hook_name} you are patching on position in the second index but size is {x_corrupted.size()}')\n",
    "        x[batch_start:batch_end:2,position] = x_corrupted[:,position]\n",
    "    return x\n",
    "\n",
    "\n",
    "def overwrite_h_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    input_hook_name: str,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    position: int = None,\n",
    "):\n",
    "    x[batch_start:batch_end:2] = x[batch_start+1:batch_end:2]\n",
    "    return x\n",
    "\n",
    "# we do a hacky thing where this first hook clears the global storage\n",
    "# second hook stores all the hooks\n",
    "# then third hook computes the output (over all the hooks)\n",
    "# this avoids recomputing and so is much faster\n",
    "CONV_HOOKS = \"conv hooks\"\n",
    "CONV_BATCHES = \"conv batches\"\n",
    "def conv_patching_init_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    **kwargs\n",
    "):\n",
    "    # we need to clear this here\n",
    "    # i tried having a \"current layer\" variable in the conv_storage that only clears when it doesn't match\n",
    "    # but that doesn't work if you only patch the same layer over and over,\n",
    "    # as stuff gets carried over\n",
    "    # this way of doing things is much safer and lets us assume it'll be empty\n",
    "    # well not quite, note that conv_patching_hook will be called with different batch_start and batch_end inputs during one forward pass\n",
    "    # so we need to account for that in the keys we use\n",
    "    global conv_storage\n",
    "    conv_storage = {CONV_BATCHES: set()}\n",
    "    return x\n",
    "\n",
    "# hook h has a weird index!!!!!\n",
    "\n",
    "def conv_patching_storage_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    conv_filter_i: int,\n",
    "    position: int,\n",
    "    layer: int,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    **kwargs,\n",
    "):\n",
    "    global storage\n",
    "    storage[hook.name] = x\n",
    "    global conv_storage\n",
    "    hooks_key = (CONV_HOOKS, batch_start, batch_end)\n",
    "    if not hooks_key in conv_storage:\n",
    "        conv_storage[hooks_key] = [] # we can't do this above because it'll be emptied again on the next batch before this is called\n",
    "    conv_storage[hooks_key].append({\"position\": position, \"conv_filter_i\": conv_filter_i})\n",
    "    conv_storage[CONV_BATCHES].add((batch_start, batch_end))\n",
    "    return x\n",
    "\n",
    "from jaxtyping import Float\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "global storage_for_grad\n",
    "\n",
    "global conv_storage\n",
    "def conv_patching_hook(\n",
    "    conv_output: Float[torch.Tensor, \"B L E\"],\n",
    "    hook: HookPoint,\n",
    "    input_hook_name: str,\n",
    "    layer: int,\n",
    "    **kwargs,\n",
    ") -> Float[torch.Tensor, \"B L E\"]:\n",
    "    global conv_storage\n",
    "    global storage\n",
    "    ### This is identical to what the conv is doing\n",
    "    # but we break it apart so we can patch on individual filters\n",
    "\n",
    "    # we have two input hooks, the second one is the one we want\n",
    "    input_hook_name = input_hook_name[1]\n",
    "    \n",
    "    D_CONV = model.cfg.d_conv\n",
    "\n",
    "    # given batches like [(2,4), (5,6)] and total size 7 this returns (0,2), (4,5), (6,7) \n",
    "    def get_missing_batches(batches, B):\n",
    "        covered_i = torch.zeros([B])\n",
    "        for batch_start, batch_end in batches:\n",
    "            covered_i[batch_start:batch_end] = 1\n",
    "    \n",
    "        missing_batches = []\n",
    "        missing_start = 0\n",
    "        for i in range(B):\n",
    "            if covered_i[i] == 1:\n",
    "                if i != missing_start:\n",
    "                    missing_batches.append((missing_start, i))\n",
    "                missing_start = i+1\n",
    "        if covered_i[B-1] == 0:\n",
    "            missing_batches.append((missing_start, B))\n",
    "        return missing_batches\n",
    "    \n",
    "\n",
    "    \n",
    "    # [E,1,D_CONV]\n",
    "    conv_weight = model.blocks[layer].conv1d.weight\n",
    "    # [E]\n",
    "    conv_bias = model.blocks[layer].conv1d.bias\n",
    "    \n",
    "    # don't recompute these if we don't need to\n",
    "    # because we stored all the hooks and batches in conv_storage, we can just do them all at once\n",
    "    output_key = f'output' # they need to share an output because they write to the same output tensor\n",
    "    if not output_key in conv_storage:\n",
    "        #print(\"layer\", layer, \"keys\", conv_storage)\n",
    "        apply_to_all_hooks = [] # this is important because otherwise the [0:None] would overwrite the previous results (or vice versa)\n",
    "        apply_to_all_key = (CONV_HOOKS, 0, None)\n",
    "        if apply_to_all_key in conv_storage:\n",
    "            apply_to_all_hooks = conv_storage[apply_to_all_key]\n",
    "            # we need to do this so it applies to the other batches that we aren't otherwise patching\n",
    "            for batch_start, batch_end in get_missing_batches(conv_storage[CONV_BATCHES], conv_output.size()[0]):\n",
    "                conv_storage[CONV_BATCHES].add(batch_start, batch_end)\n",
    "                conv_storage[(CONV_HOOKS, batch_start, batch_end)] = []\n",
    "        for batch_start, batch_end in conv_storage[CONV_BATCHES]:\n",
    "            if batch_start == 0 and batch_end == None: continue # we cover this in the apply to all hooks above\n",
    "            def get_filter_key(i):\n",
    "                return f'filter_{i}'\n",
    "            conv_input_uncorrupted = storage[input_hook_name][batch_start:batch_end:2]\n",
    "            conv_input_corrupted = storage[input_hook_name][batch_start+1:batch_end:2]\n",
    "            B, L, E = conv_input_uncorrupted.size()\n",
    "            \n",
    "            conv_input_uncorrupted = rearrange(conv_input_uncorrupted, 'B L E -> B E L')\n",
    "            conv_input_corrupted = rearrange(conv_input_corrupted, 'B L E -> B E L')\n",
    "            \n",
    "            # pad zeros in front\n",
    "            # [B,E,D_CONV-1+L]\n",
    "            padded_input_uncorrupted = torch.nn.functional.pad(conv_input_uncorrupted, (D_CONV-1,0), mode='constant', value=0)\n",
    "            padded_input_corrupted = torch.nn.functional.pad(conv_input_corrupted, (D_CONV-1,0), mode='constant', value=0)\n",
    "    \n",
    "            # compute the initial filter values\n",
    "            for i in range(D_CONV):\n",
    "                filter_key = get_filter_key(i)\n",
    "                # [B,E,L]                      [E,1]                      [B,E,L]\n",
    "                filter_contribution = conv_weight[:,0,i].view(E,1)*padded_input_uncorrupted[:,:,i:i+L]\n",
    "                conv_storage[filter_key] = filter_contribution\n",
    "            \n",
    "            # apply all the hooks\n",
    "            for hook in conv_storage[(CONV_HOOKS, batch_start, batch_end)] + apply_to_all_hooks:\n",
    "                position = hook['position']\n",
    "                conv_filter_i = hook['conv_filter_i']\n",
    "                #print(f\"position {position} conv_filter_i {conv_filter_i} batch_start {batch_start} batch_end {batch_end}\")\n",
    "                filter_key = get_filter_key(conv_filter_i)\n",
    "                # [1,E,L]                                   [E,1]                          # [B,E,L]\n",
    "                corrupted_filter_contribution = conv_weight[:,0,conv_filter_i].view(E,1)*padded_input_corrupted[:,:,conv_filter_i:conv_filter_i+L]\n",
    "                filter_contribution = conv_storage[filter_key]\n",
    "                if position is None:\n",
    "                    # [B,E,L]                    [B,E,L]\n",
    "                    filter_contribution = corrupted_filter_contribution\n",
    "                else:\n",
    "                    # [B,E]                                                  [B,E]\n",
    "                    filter_contribution[:,:,position] = corrupted_filter_contribution[:,:,position]\n",
    "                conv_storage[filter_key] = filter_contribution\n",
    "            \n",
    "            # compute the output\n",
    "            output = sum([conv_storage[get_filter_key(i)] for i in range(D_CONV)])\n",
    "            #output = torch.zeros([B,E,L], device=model.cfg.device)\n",
    "            #print(f'B {B} B2 {B2} E {E} L {L} conv_storage keys {conv_storage.keys()} filter sizes {[(k,v.size()) for (k,v) in conv_storage.items() if not type(v) is int]}')\n",
    "            for i in range(D_CONV):\n",
    "                filter_key = get_filter_key(i)\n",
    "                #output += conv_storage[filter_key]\n",
    "                del conv_storage[filter_key] # clean up now we are done with it, just to be safe\n",
    "                \n",
    "            # bias is not dependent on input so no reason to patch on it, just apply it as normal\n",
    "            output += conv_bias.view(E, 1)\n",
    "            output = rearrange(output, 'B E L -> B L E')\n",
    "            # interleave it back with the corrupted as every other\n",
    "            conv_output[batch_start:batch_end:2] = output\n",
    "        conv_storage[output_key] = conv_output\n",
    "    return conv_storage[output_key]\n",
    "\n",
    "\n",
    "limited_layers = list(range(model.cfg.n_layers))\n",
    "\n",
    "\n",
    "from acdc.data.ioi import ioi_data_generator, ABC_TEMPLATES, get_all_single_name_abc_patching_formats\n",
    "from acdc.data.utils import generate_dataset\n",
    "\n",
    "num_patching_pairs = 200\n",
    "seed = 27\n",
    "valid_seed = 28\n",
    "constrain_to_answers = True\n",
    "# this makes our data size 800, first 400 is each (a,b) pair, and then second 400 is each pair swapped to be (b,a)\n",
    "has_symmetric_patching = True\n",
    "\n",
    "from acdc.data.ioi import BABA_TEMPLATES\n",
    "templates = ABC_TEMPLATES\n",
    "'''\n",
    "patching_formats = list(get_all_single_name_abc_patching_formats())\n",
    "patching\n",
    "# generate data once to populate good_names and good_nouns (holds single-token choices)\n",
    "data = generate_dataset(model=model,\n",
    "                  data_generator=ioi_data_generator,\n",
    "                  num_patching_pairs=num_patching_pairs,\n",
    "                  seed=seed,\n",
    "                  valid_seed=valid_seed,\n",
    "                  constrain_to_answers=constrain_to_answers,\n",
    "                  has_symmetric_patching=has_symmetric_patching, \n",
    "                  varying_data_lengths=True,\n",
    "                  templates=templates,\n",
    "                  patching_formats=patching_formats)\n",
    "'''\n",
    "import acdc.data.ioi\n",
    "from collections import defaultdict\n",
    "name_positions_map = defaultdict(lambda: [])\n",
    "for template in templates:\n",
    "    name = acdc.data.ioi.good_names[0]\n",
    "    template_filled_in = template.replace(\"[NAME]\", name)\n",
    "    template_filled_in = template_filled_in.replace(\"[PLACE]\", acdc.data.ioi.good_nouns['[PLACE]'][0])\n",
    "    template_filled_in = template_filled_in.replace(\"[OBJECT]\", acdc.data.ioi.good_nouns['[OBJECT]'][0])\n",
    "    # get the token positions of the [NAME] in the prompt\n",
    "    name_positions = tuple([(i) for (i,s) in enumerate(model.to_str_tokens(torch.tensor(model.tokenizer.encode(template_filled_in)))) if s == f' {name}'])\n",
    "    name_positions_map[name_positions].append(template)\n",
    "sorted_by_frequency = sorted(list(name_positions_map.items()), key=lambda x: -len(x[1]))\n",
    "most_frequent_name_positions, templates = sorted_by_frequency[0]\n",
    "print(\"using templates\")\n",
    "for template in templates:\n",
    "    print(template)\n",
    "print(f\"with name positions {most_frequent_name_positions}\")\n",
    "\n",
    "data = generate_dataset(model=model,\n",
    "              data_generator=ioi_data_generator,\n",
    "              num_patching_pairs=num_patching_pairs,\n",
    "              seed=seed,\n",
    "              valid_seed=valid_seed,\n",
    "              constrain_to_answers=constrain_to_answers,\n",
    "              has_symmetric_patching=has_symmetric_patching, \n",
    "              varying_data_lengths=True,\n",
    "              templates=templates,\n",
    "              patching_formats=patching_formats)\n",
    "\n",
    "\n",
    "## Setup edges for ACDC\n",
    "edges = []\n",
    "B,L = data.data.size()\n",
    "#positions = list(range(L)) # \n",
    "\n",
    "if POSITIONS:\n",
    "    positions = list(range(L))\n",
    "else:\n",
    "    positions = [None]\n",
    "\n",
    "INPUT_HOOK = f'hook_embed'\n",
    "INPUT_NODE = 'embed'\n",
    "\n",
    "last_layer = model.cfg.n_layers-1\n",
    "OUTPUT_HOOK = f'blocks.{last_layer}.hook_resid_post'\n",
    "OUTPUT_NODE = 'output'\n",
    "\n",
    "def input(layer):\n",
    "    return f'{layer}.i'\n",
    "\n",
    "def output(layer):\n",
    "    return f'{layer}.o'\n",
    "\n",
    "def conv(layer):\n",
    "    return f'{layer}.conv'\n",
    "\n",
    "def skip(layer):\n",
    "    return f'{layer}.skip'\n",
    "\n",
    "def ssm(layer):\n",
    "    return f'{layer}.ssm'\n",
    "\n",
    "# important to have storage be global and not passed into the hooks! Otherwise it gets very slow (tbh, i don't know why)\n",
    "global storage\n",
    "storage = {}\n",
    "\n",
    "\n",
    "'''\n",
    "    # embed -> i\n",
    "    for i in range(model.cfg.n_layers):\n",
    "        edges.append((attrs[0,i+1,pos].flatten()[0], 'embed', input(i), pos))\n",
    "    # i -> j\n",
    "    for i in range(model.cfg.n_layers):\n",
    "        for j in range(i+1, model.cfg.n_layers):\n",
    "            edges.append((attrs[i+1,j+1,pos].flatten()[0], output(i), input(j), pos))\n",
    "    # j -> output\n",
    "    for j in range(model.cfg.n_layers):\n",
    "        edges.append((attrs[j+1,model.cfg.n_layers+1,pos].flatten()[0], output(j), 'output', pos))\n",
    "    # embed -> output\n",
    "    edges.append((attrs[0,model.cfg.n_layers+1,pos].flatten()[0], 'embed', 'output', pos))\n",
    "'''\n",
    "\n",
    "attrs = attributions.mean(dim=0)\n",
    "conv_attrs = conv_attributions.mean(dim=0)\n",
    "ALWAYS_KEEP_WEIGHT = -10\n",
    "for pos in positions:\n",
    "    # direct connections from embed to output\n",
    "    edges.append(Edge(\n",
    "            label=str(pos),\n",
    "            input_node=INPUT_NODE,\n",
    "            input_hook=(INPUT_HOOK, storage_hook),\n",
    "            output_node=OUTPUT_NODE,\n",
    "            output_hook=(OUTPUT_HOOK, partial(resid_patching_hook, position=pos)),\n",
    "            score_diff_when_patched=attrs[0,model.cfg.n_layers+1,pos].flatten()[0],\n",
    "    ))\n",
    "\n",
    "for layer in limited_layers:\n",
    "    for pos_i, pos in enumerate(positions):\n",
    "        # edge from embed to layer input\n",
    "        edges.append(Edge(\n",
    "                label=str(pos),\n",
    "                input_node=INPUT_NODE,\n",
    "                input_hook=(INPUT_HOOK, partial(storage_hook)),\n",
    "                output_node=input(layer),\n",
    "                output_hook=(f'blocks.{layer}.hook_layer_input', partial(resid_patching_hook, position=pos)),\n",
    "                score_diff_when_patched=attrs[0,layer+1,pos].flatten()[0],\n",
    "        ))\n",
    "\n",
    "        # input to conv\n",
    "        for conv_i in range(model.cfg.d_conv):\n",
    "            edges.append(Edge(\n",
    "                    label=(f'[{pos}:{conv_i-model.cfg.d_conv+1}]'.replace(\"None:\", \"\")), # [-D_CONV+1, -D_CONV+2, ..., -2, -1, 0]\n",
    "                    input_node=input(layer),\n",
    "                    input_hook=[\n",
    "                        (f'blocks.{layer}.hook_layer_input', conv_patching_init_hook),\n",
    "                        (f'blocks.{layer}.hook_in_proj', partial(conv_patching_storage_hook, position=pos, layer=layer, conv_filter_i=conv_i))\n",
    "                    ],\n",
    "                    output_node=conv(layer),\n",
    "                    output_hook=(f'blocks.{layer}.hook_conv', partial(conv_patching_hook, position=pos, layer=layer, conv_filter_i=conv_i)),\n",
    "                    score_diff_when_patched=conv_attrs[layer,conv_i,pos].flatten()[0],\n",
    "            ))\n",
    "        \n",
    "        # conv to ssm\n",
    "        if pos is None:\n",
    "            # we need a seperate hook for each pos, but put them all into one edge\n",
    "            hooks = []\n",
    "            for other_pos in range(L):\n",
    "                hooks.append((f'blocks.{layer}.hook_h.{other_pos}', overwrite_h_hook))\n",
    "            edges.append(Edge(\n",
    "                    input_node=conv(layer),\n",
    "                    output_node=ssm(layer),\n",
    "                    output_hook=hooks,\n",
    "                    score_diff_when_patched=ALWAYS_KEEP_WEIGHT,\n",
    "            ))\n",
    "        else:\n",
    "            edges.append(Edge(\n",
    "                    label=f'{pos}',\n",
    "                    input_node=conv(layer),\n",
    "                    output_node=ssm(layer),\n",
    "                    output_hook=(f'blocks.{layer}.hook_h.{pos}', overwrite_h_hook),\n",
    "                    score_diff_when_patched=ALWAYS_KEEP_WEIGHT\n",
    "            ))\n",
    "\n",
    "        if pos_i == 0: # we only need one of these\n",
    "            # ssm to output\n",
    "            edges.append(Edge(\n",
    "                    input_node=ssm(layer),\n",
    "                    output_node=output(layer),\n",
    "                    score_diff_when_patched=ALWAYS_KEEP_WEIGHT\n",
    "            ))\n",
    "        \n",
    "        # input to skip\n",
    "        edges.append(Edge(\n",
    "                label=f'{pos}',\n",
    "                input_node=input(layer),\n",
    "                output_node=skip(layer),\n",
    "                output_hook=(f'blocks.{layer}.hook_skip', partial(overwrite_patching_hook, position=pos)),\n",
    "                score_diff_when_patched=ALWAYS_KEEP_WEIGHT\n",
    "        ))\n",
    "\n",
    "        if pos_i == 0: # we only need one of these\n",
    "            # skip to output\n",
    "            edges.append(Edge(\n",
    "                    input_node=skip(layer),\n",
    "                    output_node=output(layer),\n",
    "                    score_diff_when_patched=ALWAYS_KEEP_WEIGHT\n",
    "            ))\n",
    "        \n",
    "        for later_layer in limited_layers:\n",
    "                if layer < later_layer:\n",
    "                    # edge from layer output to other layer input\n",
    "                    edges.append(Edge(\n",
    "                            label=str(pos),\n",
    "                            input_node=output(layer),\n",
    "                            input_hook=(f'blocks.{layer}.hook_out_proj', storage_hook),\n",
    "                            output_node=input(later_layer),\n",
    "                            output_hook=(f'blocks.{later_layer}.hook_layer_input', partial(resid_patching_hook, position=pos)),\n",
    "                            score_diff_when_patched=attrs[layer+1,later_layer+1,pos].flatten()[0],\n",
    "                    ))\n",
    "        \n",
    "        # edge from layer output to final layer output\n",
    "        edges.append(Edge(\n",
    "                label=str(pos),\n",
    "                input_node=output(layer),\n",
    "                input_hook=(f'blocks.{layer}.hook_out_proj', storage_hook),\n",
    "                output_node=OUTPUT_NODE,\n",
    "                output_hook=(OUTPUT_HOOK, partial(resid_patching_hook, position=pos)),\n",
    "                score_diff_when_patched=attrs[layer+1,model.cfg.n_layers+1,pos].flatten()[0],\n",
    "        ))\n",
    "\n",
    "\n",
    "\n",
    "def normalized_logit_diff_acdc_metric(data: ACDCEvalData, printing=False):\n",
    "    B,V = data.patched.logits.size()\n",
    "\n",
    "    # [batch_size]\n",
    "    patched_correct = data.patched.correct[:,0]\n",
    "    #print(data.unpatched.logits.size(), data.patched.logits.size(), data.corrupted.logits.size())\n",
    "    A_logits_unpatched = data.unpatched.logits[torch.arange(B), patched_correct]\n",
    "    A_logits_patched = data.patched.logits[torch.arange(B), patched_correct]\n",
    "    A_logits_corrupted = data.corrupted.logits[torch.arange(B), patched_correct]\n",
    "\n",
    "    corrupted_correct = data.corrupted.correct[:,0]\n",
    "    B_logits_unpatched = data.unpatched.logits[torch.arange(B), corrupted_correct]\n",
    "    B_logits_patched = data.patched.logits[torch.arange(B), corrupted_correct]\n",
    "    B_logits_corrupted = data.corrupted.logits[torch.arange(B), corrupted_correct]\n",
    "\n",
    "    # A and B are two potential outputs\n",
    "    # if A patched > B patched, we are correct\n",
    "    # else we are incorrect\n",
    "\n",
    "    # thus we could just return A_logits_patched - B_logits_patched\n",
    "\n",
    "    # however it is useful to \"normalize\" these values\n",
    "\n",
    "    # in the worst case, our patching causes us to act like corrupted, and our diff will be\n",
    "    # A_logits_corrupted - B_logits_corrupted\n",
    "    # this will result in a small, negative value\n",
    "    \n",
    "    # in the best case, our patching will do nothing (cause us to act like unpatched), and our diff will be\n",
    "    # A_logits_unpatched - B_logits_unpatched\n",
    "    # this will result in a large, positive value\n",
    "\n",
    "    # thus we can treat those as the \"min\" and \"max\" and normalize accordingly\n",
    "    \n",
    "    min_diff = A_logits_corrupted - B_logits_corrupted\n",
    "    max_diff = A_logits_unpatched - B_logits_unpatched\n",
    "\n",
    "    possible_range = (max_diff-min_diff)\n",
    "    possible_range[possible_range == 0] = 1.0 # prevent divide by zero\n",
    "    \n",
    "    diff = A_logits_patched - B_logits_patched\n",
    "    normalized_diff = (diff-min_diff)/torch.abs(possible_range) # abs prevents incorrect data from wanting to be more incorrect\n",
    "\n",
    "    if printing:\n",
    "        print(f\"A corrupted {A_logits_corrupted}\")\n",
    "        print(f\"B corrupted {B_logits_corrupted}\")\n",
    "        print(f\"A unpatched {A_logits_unpatched}\")\n",
    "        print(f\"B unpatched {B_logits_unpatched}\")\n",
    "        print(f\"A patched {A_logits_patched}\")\n",
    "        print(f\"B patched {B_logits_patched}\")\n",
    "        print(f\"min diff {min_diff}\")\n",
    "        print(f\"max diff {max_diff}\")\n",
    "        print(f\"possible range {possible_range}\")\n",
    "        print(f\"diff {diff}\")\n",
    "        print(f\"normalized diff {normalized_diff}\")\n",
    "    # as described, 1.0 corresponds to acting like unpatched,\n",
    "    # and 0.0 corresponds to acting like corrupted\n",
    "\n",
    "    return torch.mean(normalized_diff)\n",
    "import acdc\n",
    "cfg = ACDCConfig(\n",
    "    ckpt_directory = \"blah\",\n",
    "    thresh = 0.0001,\n",
    "    rollback_thresh = 0.0001,\n",
    "    metric=acdc.accuracy_metric,\n",
    "    # extra inference args\n",
    "    model_kwargs=model_kwargs,\n",
    "    # these are needed for doing graph pruning\n",
    "    input_node=INPUT_NODE,\n",
    "    output_node=OUTPUT_NODE,\n",
    "    # batch size for evaluating data points\n",
    "    batch_size=1,\n",
    "    log_level=LOG_LEVEL_INFO,\n",
    "    # if False, will be equivalent to batch_size=1\n",
    "    batched = True,\n",
    "    # set these two to false to use traditional ACDC\n",
    "    # recursive will try patching multiple at a time (this is faster sometimes)\n",
    "    recursive = True,\n",
    "    # try_patching_multiple_at_same_time will evaluate many different patchings before commiting to any\n",
    "    # and includes a rollback scheme if after patching one, the others get worse\n",
    "    try_patching_multiple_at_same_time = True,\n",
    "    ## if true, you metric will also have the logits from a run with no patching available\n",
    "    # (useful for normalized logit diff)\n",
    "    store_unpatched_logits = True,\n",
    ")\n",
    "\n",
    "# accuracy assumes only one name\n",
    "\n",
    "\n",
    "from acdc import get_currently_patched_edge_hooks, eval_acdc, wrap_run_with_hooks, get_logits_of_predicted_next_token\n",
    "unpatched_logits = get_logits_of_predicted_next_token(\n",
    "        model=model,\n",
    "        data=data.data,\n",
    "        last_token_position=data.last_token_position,\n",
    "        **model_kwargs\n",
    ")\n",
    "\n",
    "valid_unpatched_logits = get_logits_of_predicted_next_token(\n",
    "        model=model,\n",
    "        data=data.valid_data,\n",
    "        last_token_position=data.valid_last_token_position,\n",
    "        **model_kwargs\n",
    ")\n",
    "\n",
    "def eval_edges(edges_keeping, all_edges, valid=False):\n",
    "    for edge in all_edges:\n",
    "        edge.patching = True\n",
    "        edge.checked = True\n",
    "    for edge in edges_keeping:\n",
    "        edge.patching = False\n",
    "        edge.checked = True\n",
    "    currently_patched_edge_hooks = get_currently_patched_edge_hooks(cfg=cfg, edges=edges)\n",
    "    if valid:\n",
    "        return eval_acdc(\n",
    "                model=wrap_run_with_hooks(model=model, fwd_hooks=currently_patched_edge_hooks, **cfg.model_kwargs),\n",
    "                data=data.valid_data,\n",
    "                last_token_position=data.valid_last_token_position,\n",
    "                correct=data.valid_correct,\n",
    "                incorrect=data.valid_incorrect,\n",
    "                metric=cfg.metric,\n",
    "                num_edges=1,\n",
    "                constrain_to_answers=data.constrain_to_answers,\n",
    "                unpatched_logits=valid_unpatched_logits)[0].item()\n",
    "    else:\n",
    "        return eval_acdc(\n",
    "            model=wrap_run_with_hooks(model=model, fwd_hooks=currently_patched_edge_hooks, **cfg.model_kwargs),\n",
    "            data=data.data,\n",
    "            last_token_position=data.last_token_position,\n",
    "            correct=data.correct,\n",
    "            incorrect=data.incorrect,\n",
    "            metric=cfg.metric,\n",
    "            num_edges=1,\n",
    "            constrain_to_answers=data.constrain_to_answers,\n",
    "            unpatched_logits=unpatched_logits)[0].item()\n",
    "\n",
    "# can also do this instead to include negative contributions but I find the graph is larger, ymmv\n",
    "#edges.sort(key=lambda x: -abs(x[0]))\n",
    "edges.sort(key=lambda edge: edge.score_diff_when_patched)\n",
    "\n",
    "import math\n",
    "def test_pos(pos):\n",
    "    print(f\"testing pos {pos}\")\n",
    "    edges_to_keep = edges[:pos]\n",
    "    metric = eval_edges(edges_to_keep, edges)\n",
    "    print(f\"testing pos {pos} got metric {metric}\")\n",
    "    return metric\n",
    "\n",
    "# from https://en.wikipedia.org/wiki/Binary_search_algorithm\n",
    "def binary_search(n, T):\n",
    "    L = 0\n",
    "    R = n - 1\n",
    "    while L != R:\n",
    "        m = math.ceil((L + R) / 2)\n",
    "        if test_pos(m) > T:\n",
    "            R = m - 1\n",
    "        else:\n",
    "            L = m\n",
    "    # go one further because this gives us below thresh\n",
    "    return min(n-1, L+1)\n",
    "\n",
    "ACC_THRESH = 0.85\n",
    "torch.set_grad_enabled(False)\n",
    "cutoff = binary_search(len(edges), T=ACC_THRESH)\n",
    "\n",
    "edges_to_keep = edges[:cutoff]\n",
    "scores = [edge.score_diff_when_patched for edge in edges[:cutoff]]\n",
    "print(f\"keeping top {cutoff} edges {edges_to_keep}\")\n",
    "metric = eval_edges(edges_to_keep, edges)\n",
    "print(f\"got metric {metric}\")    \n",
    "\n",
    "metric = eval_edges(edges_to_keep, edges, valid=True)\n",
    "print(f\"got valid metric {metric}\")    \n",
    "# .85 has 3061 edges for 5 ITERS\n",
    "# .85 has 2876 edges for 30 ITERS\n",
    "num_always_keep = 0\n",
    "for edge in edges_to_keep:\n",
    "    if edge.score_diff_when_patched == ALWAYS_KEEP_WEIGHT:\n",
    "        num_always_keep += 1\n",
    "print(f\"always keep {num_always_keep}\")\n",
    "\n",
    "\n",
    "# patching format 0:\n",
    "# 2162 edges (2016 always keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d716bc32-b8d0-4320-8057-373638c8499b",
   "metadata": {},
   "source": [
    "# Display ACDC Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a72d39-8320-4c57-ba05-b3ebb97ac465",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from acdc import ACDCEvalData\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "import torch\n",
    "from IPython.display import display, FileLink, Image\n",
    "import acdc\n",
    "from importlib import reload\n",
    "edges = edges_to_keep\n",
    "L = data.data.size()[1]\n",
    "print(f\"got L of {L}\")\n",
    "if POSITIONS:\n",
    "    by_filters = torch.zeros([model.cfg.d_conv*L, model.cfg.n_layers])\n",
    "else:\n",
    "    by_filters = torch.zeros([model.cfg.d_conv, model.cfg.n_layers])\n",
    "\n",
    "# change this to get the different plots\n",
    "CAP = 0.0\n",
    "# 1.0 is boolean (present or not)\n",
    "# 0.0 is just give score\n",
    "\n",
    "def filter_score(score):\n",
    "    if CAP == 0.0: return score\n",
    "    if CAP == 1.0: return 1.0\n",
    "    return max(score, -CAP)\n",
    "for edge in edges:\n",
    "    if not edge.patching and edge.checked:\n",
    "        if '.conv' in edge.output_node:\n",
    "            if \":\" in edge.label:\n",
    "                pos = int(edge.label.split(\":\")[0][1:])\n",
    "                rest = edge.label.split(\":\")[1]\n",
    "            else:\n",
    "                pos = None\n",
    "                rest = edge.label[1:]\n",
    "            filter = int(rest.split(\"]\")[0])        \n",
    "            layer = edge.output_node.split(\".\")[0]\n",
    "            if POSITIONS:\n",
    "                try:\n",
    "                    by_filters[model.cfg.d_conv*pos + abs(int(filter)),int(layer)] = filter_score(edge.score_diff_when_patched)\n",
    "                except:\n",
    "                    print(L, pos, abs(int(filter)))\n",
    "                    raise\n",
    "            else:\n",
    "                by_filters[abs(int(filter)),int(layer)] = filter_score(edge.score_diff_when_patched)\n",
    "        if '.skip' in edge.output_node and False:        \n",
    "            layer = edge.output_node.split(\".\")[0]\n",
    "            by_filters[model.cfg.d_conv,int(layer)] = filter_score(edge.score_diff_when_patched)\n",
    "        if '.ssm' in edge.output_node and False:\n",
    "            layer = edge.output_node.split(\".\")[0]\n",
    "            by_filters[model.cfg.d_conv+1,int(layer)] = filter_score(edge.score_diff_when_patched)\n",
    "            \n",
    "\n",
    "def layer_to_i(node):\n",
    "    if node == INPUT_NODE:\n",
    "        return 0\n",
    "    elif node == OUTPUT_NODE:\n",
    "        return model.cfg.n_layers+1 # because embed is 0\n",
    "    else:\n",
    "        return int(node)+1 # because embed is 0\n",
    "\n",
    "\n",
    "def between_layers_info(edge):\n",
    "    if edge.patching: return False, None, None\n",
    "    is_between_layers = False\n",
    "    layer_input = None\n",
    "    layer_output = None\n",
    "    if edge.input_node == INPUT_NODE:\n",
    "        is_between_layers = True\n",
    "        layer_input = INPUT_NODE\n",
    "    if edge.output_node == OUTPUT_NODE:\n",
    "        is_between_layers = True\n",
    "        layer_output = OUTPUT_NODE\n",
    "\n",
    "    if '.' in edge.input_node:\n",
    "        input_layer, input_type = edge.input_node.split(\".\")\n",
    "        if edge.input_node == output(input_layer):\n",
    "            is_between_layers = True\n",
    "            layer_input = str(input_layer)\n",
    "    if '.' in edge.output_node:\n",
    "        output_layer, output_type = edge.output_node.split(\".\")\n",
    "        if edge.output_node == input(output_layer):\n",
    "            is_between_layers = True\n",
    "            layer_output = str(output_layer)\n",
    "    return is_between_layers, layer_input, layer_output\n",
    "\n",
    "def compute_adj_mat(edges):\n",
    "    if POSITIONS:\n",
    "        adj_mat = torch.zeros([L, model.cfg.n_layers+2, model.cfg.n_layers+2])\n",
    "    else:\n",
    "        adj_mat = torch.zeros([model.cfg.n_layers+2, model.cfg.n_layers+2])\n",
    "    for edge in edges:\n",
    "        is_between_layers, layer_input, layer_output = between_layers_info(edge)\n",
    "        if is_between_layers:\n",
    "            if POSITIONS:\n",
    "                pos = int(edge.label)\n",
    "                adj_mat[pos, layer_to_i(layer_input), layer_to_i(layer_output)] = edge.score_diff_when_patched\n",
    "            else:\n",
    "                adj_mat[layer_to_i(layer_input), layer_to_i(layer_output)] = edge.score_diff_when_patched\n",
    "    return adj_mat\n",
    "\n",
    "\n",
    "L = data.data.size()[1]\n",
    "import networkx as nx\n",
    "def better_get_nx_graph(adj_mat) -> nx.DiGraph:\n",
    "    '''\n",
    "    Converts the edges into a networkx graph\n",
    "    only edges that have checked == True and patching == False are included\n",
    "    if include_unchecked=True, any edge that has checked == False is also included\n",
    "    '''\n",
    "    num_nodes = adj_mat.size()[-1]\n",
    "    G = nx.DiGraph()\n",
    "    edges = []\n",
    "    if POSITIONS:\n",
    "        for pos in range(L):\n",
    "            for i in range(num_nodes):\n",
    "                for j in range(num_nodes):\n",
    "                    if adj_mat[pos,i,j] != 0:\n",
    "                        G.add_edge(str(i), str(j))\n",
    "                        edges.append((i,j,pos,adj_mat[pos,i,j]))\n",
    "    else:\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(num_nodes):\n",
    "                if adj_mat[i,j] != 0:\n",
    "                    G.add_edge(str(i), str(j))\n",
    "                    edges.append((i,j,None,adj_mat[i,j]))\n",
    "    return G, edges\n",
    "\n",
    "def better_prune_edges(adj_mat):\n",
    "    num_nodes = adj_mat.size()[-1]\n",
    "    input_node = 0\n",
    "    output_node = num_nodes-1\n",
    "    import networkx as nx\n",
    "    G, edges = better_get_nx_graph(adj_mat=adj_mat)\n",
    "    pruned_adj_mat = torch.zeros(adj_mat.size())\n",
    "    pruned_edges = []\n",
    "    for i,j,pos,attr in edges:\n",
    "        connected_to_input = False\n",
    "        try:\n",
    "            to_input = nx.shortest_path(G, source=str(input_node), target=str(i))\n",
    "            connected_to_input = True\n",
    "        except nx.NetworkXNoPath:\n",
    "            pass\n",
    "        except nx.NodeNotFound:\n",
    "            raise ValueError(f\"Graph does not have input node {input_node}\")\n",
    "        \n",
    "        connected_to_output = False\n",
    "        try:\n",
    "            to_output = nx.shortest_path(G, source=str(j), target=str(output_node))\n",
    "            connected_to_output = True\n",
    "        except nx.NetworkXNoPath:\n",
    "            pass\n",
    "        except nx.NodeNotFound:\n",
    "            raise ValueError(f\"Graph does not have output node {output_node}\")\n",
    "        #print(f\"testing {i}->{j} {connected_to_input} {connected_to_output}\")\n",
    "        if connected_to_input and connected_to_output:\n",
    "            pruned_adj_mat[pos,i,j] = attr\n",
    "        else:\n",
    "            print(f\"pruning {pos} {i}->{j} {connected_to_input} {connected_to_output}\")\n",
    "    return pruned_adj_mat\n",
    "\n",
    "adj_mat = compute_adj_mat(edges_to_keep)\n",
    "pruned_adj_mat = better_prune_edges(adj_mat)\n",
    "\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", font_size=None, show=True, color_continuous_midpoint=0.0, **kwargs):\n",
    "    import plotly.express as px\n",
    "    import transformer_lens.utils as utils\n",
    "    fig = px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=color_continuous_midpoint, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs)\n",
    "    if not font_size is None:\n",
    "        if 'x' in kwargs and False:\n",
    "            fig.update_layout(\n",
    "              xaxis = dict(\n",
    "                tickmode='array',\n",
    "                tickvals = kwargs['x'],\n",
    "                ticktext = kwargs['x'], \n",
    "                ),\n",
    "               font=dict(size=font_size, color=\"black\"))\n",
    "        if 'y' in kwargs:\n",
    "            fig.update_layout(\n",
    "              yaxis = dict(\n",
    "                tickmode='array',\n",
    "                tickvals = kwargs['y'],\n",
    "                ticktext = kwargs['y'], \n",
    "                ),\n",
    "               font=dict(size=font_size, color=\"black\"))\n",
    "    plot_args = {\n",
    "        'width': 1200,\n",
    "        'height': 900,\n",
    "        \"autosize\": False,\n",
    "        'showlegend': True,\n",
    "        'margin': {\"l\":0,\"r\":0,\"t\":100,\"b\":0}\n",
    "    }\n",
    "    \n",
    "    fig.update_layout(**plot_args)\n",
    "    fig.update_layout(legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    ))\n",
    "    if show:\n",
    "        fig.show(renderer)\n",
    "    else:\n",
    "        return fig\n",
    "\n",
    "def to_str(lis):\n",
    "    return [str(x) for x in lis]\n",
    "x_labels = to_str([-x for x in range(model.cfg.d_conv)])\n",
    "toks = model.to_str_tokens(data.data[0])\n",
    "x_labels_all_pos = []\n",
    "for l in range(L):\n",
    "    x_labels_all_pos += [label + toks[l+int(label)] + \"_\" + str(l) for label in x_labels]\n",
    "print(x_labels, by_filters.size())\n",
    "print(len(x_labels_all_pos))\n",
    "title = f'which parts of layer using (conv filters, skip, and/or conv), capped to {-CAP}'\n",
    "if CAP == 1.0:\n",
    "    title = f'which parts of layer using (conv filters, skip, and/or conv)'\n",
    "print(x_labels_all_pos)\n",
    "imshow(by_filters.T, y=to_str(range(model.cfg.n_layers)), x=x_labels_all_pos, title=title, font_size=9)\n",
    "#for layer in range(model.cfg.n_layers):\n",
    "#    imshow(by_filters[:,layer:layer+1].T, y=to_str([layer]), x=x_labels_all_pos, title=str(layer), font_size=9)\n",
    "\n",
    "labels = ['embed'] + [str(x) for x in range(model.cfg.n_layers)] + ['output']\n",
    "if POSITIONS:\n",
    "    for position in range(L):\n",
    "        if torch.any(pruned_adj_mat[position] != 0):\n",
    "            imshow(pruned_adj_mat[position], y=labels, x=labels, title=f'pos {position} tok {toks[position]} adjacency matrix clamped to {CAP}', font_size=8)\n",
    "else:\n",
    "    imshow(pruned_adj_mat, y=labels, x=labels, title=f'adjacency matrix clamped to {CAP}', font_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6557bd1-5b65-47c2-92d9-b2c28467694c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "name_positions = [3,5,7,13,15]\n",
    "position_map = {}\n",
    "for l in range(L):\n",
    "    position_map[l] = f'pos{l}{toks[l]}'\n",
    "position_map[3] = 'n1'\n",
    "position_map[5] = 'n2'\n",
    "position_map[7] = 'n3'\n",
    "position_map[13] = 'n4'\n",
    "position_map[15] = 'n5'\n",
    "position_map[19] = 'out'\n",
    "\n",
    "\n",
    "\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    layer_attrs = []\n",
    "    for l in range(L):\n",
    "        for filter_i in range(model.cfg.d_conv):\n",
    "            filter_offset = -filter_i\n",
    "            index = l*model.cfg.d_conv + filter_i\n",
    "            attr = by_filters[index, layer]\n",
    "            if attr < 0:\n",
    "                layer_attrs.append((position_map[l+filter_offset], attr, filter_offset))\n",
    "    print(layer)\n",
    "    for tok, attr, filter_offset in layer_attrs:\n",
    "        attr = -attr*1000\n",
    "        print(f\"  {tok}[{filter_offset}] {attr:.5f}\")\n",
    "\n",
    "def map_layer(layer):\n",
    "    if layer == 0: return 'embed'\n",
    "    if layer == model.cfg.n_layers+1: return 'output'\n",
    "    else: return str(layer-1)\n",
    "\n",
    "for layer in range(model.cfg.n_layers+2):\n",
    "    layer_attrs = []\n",
    "    for other_layer in range(model.cfg.n_layers+2):\n",
    "        for pos in range(L):\n",
    "            if pruned_adj_mat[pos, layer, other_layer] != 0:\n",
    "                layer_attrs.append((position_map[pos], f\"->{map_layer(other_layer)}\", pruned_adj_mat[pos, layer, other_layer]))\n",
    "            if pruned_adj_mat[pos, other_layer, layer] != 0:\n",
    "                layer_attrs.append((position_map[pos], f\"{map_layer(other_layer)}->\", pruned_adj_mat[pos, other_layer, layer]))\n",
    "    print(map_layer(layer))\n",
    "    for tok, label, attr in layer_attrs:\n",
    "        attr = -attr*1000\n",
    "        print(f\"  {tok} {label} {attr:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92335cb9-9698-4f90-8121-7aa53c8af690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "def layer_to_i(layer):\n",
    "    if layer == 'embed': return 0\n",
    "    elif layer == 'output': return model.cfg.n_layers+1\n",
    "    else: return int(layer)+1\n",
    "\n",
    "forbidden = [0,1,40]\n",
    "pruned_adj_mat = better_prune_edges(adj_mat)\n",
    "for i in forbidden:\n",
    "    pruned_adj_mat[:,i,:] = 0\n",
    "    pruned_adj_mat[:,:,i] = 0\n",
    "present_layers = set()\n",
    "for i in range(model.cfg.n_layers):\n",
    "    for j in range(model.cfg.n_layers):\n",
    "        if not i+1 in forbidden and not j+1 in forbidden:\n",
    "            if torch.any(pruned_adj_mat[:,i+1,j+1] != 0):\n",
    "                present_layers.add(i)\n",
    "                present_layers.add(j)\n",
    "                #print(f\"present edge {i}->{j}\")\n",
    "print(present_layers)\n",
    "pruned_dot = graphviz.Digraph('graph')\n",
    "for edge in edges_to_keep:\n",
    "    is_between_layers, layer_input, layer_output = between_layers_info(edge)\n",
    "    if is_between_layers:\n",
    "        ini, outi = layer_to_i(layer_input), layer_to_i(layer_output)\n",
    "        pos = int(edge.label)\n",
    "        if pruned_adj_mat[pos, ini, outi] != 0:\n",
    "            if not ini in forbidden and not outi in forbidden:\n",
    "                pruned_dot.edge(edge.input_node, edge.output_node, label=position_map[int(edge.label)])\n",
    "    else:\n",
    "        layer = int(edge.input_node.split(\".\")[0])\n",
    "        if layer in present_layers: # don't display stuff that are disconnected\n",
    "            if '.ssm' in edge.output_node or '.skip' in edge.output_node or '.skip' in edge.input_node: # don't need all these since they are all forced on\n",
    "                if edge.label == 0 or edge.label == \"\" or edge.label is None or edge.label == \"0\":\n",
    "                    pruned_dot.edge(edge.input_node, edge.output_node)\n",
    "            elif '.conv' in edge.output_node:\n",
    "                label = edge.label\n",
    "                # replace pos: with the more helpful labels\n",
    "                for l in range(L):\n",
    "                    start_from_back = L-l-1 # start from biggest number work backwards because like 10 contains a 1 so we need to do 10 first\n",
    "                    prev_label = label\n",
    "                    label = label.replace(\"[\" + str(start_from_back) + \":\", \"[\" + position_map[start_from_back] + \":\")\n",
    "                    if label != prev_label:\n",
    "                        break\n",
    "                pruned_dot.edge(edge.input_node, edge.output_node, label=label)\n",
    "            else:\n",
    "                pruned_dot.edge(edge.input_node, edge.output_node, label=edge.label)\n",
    "output_name = f'pruned dot {ACC_THRESH}'\n",
    "pruned_dot.render(output_name, format=\"png\") # it automatically appends png\n",
    "display(Image(filename=output_name + \".png\"))\n",
    "display(FileLink(output_name + \".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "593bfde8-9bd8-4f43-8c08-cb8b3e0eda8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c843768-7705-4c53-8907-6f901d21ab61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
