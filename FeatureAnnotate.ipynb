{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044af21d-390b-4473-b01f-76ea4d29f181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "1\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry190.txtblocks.1.hook_out_proj/hook_blocks.1.hook_out_proj.pt\n",
      "2\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry191.txtblocks.2.hook_out_proj/hook_blocks.2.hook_out_proj.pt\n",
      "3\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry192.txtblocks.3.hook_out_proj/hook_blocks.3.hook_out_proj.pt\n",
      "4\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry193.txtblocks.4.hook_out_proj/hook_blocks.4.hook_out_proj.pt\n",
      "5\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry194.txtblocks.5.hook_out_proj/hook_blocks.5.hook_out_proj.pt\n",
      "6\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry195.txtblocks.6.hook_out_proj/hook_blocks.6.hook_out_proj.pt\n",
      "7\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry196.txtblocks.7.hook_out_proj/hook_blocks.7.hook_out_proj.pt\n",
      "8\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry200.txtblocks.8.hook_out_proj/hook_blocks.8.hook_out_proj.pt\n",
      "9\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry201.txtblocks.9.hook_out_proj/hook_blocks.9.hook_out_proj.pt\n",
      "10\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry202.txtblocks.10.hook_out_proj/hook_blocks.10.hook_out_proj.pt\n",
      "11\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry203.txtblocks.11.hook_out_proj/hook_blocks.11.hook_out_proj.pt\n",
      "12\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry204.txtblocks.12.hook_out_proj/hook_blocks.12.hook_out_proj.pt\n",
      "13\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry205.txtblocks.13.hook_out_proj/hook_blocks.13.hook_out_proj.pt\n",
      "14\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry206.txtblocks.14.hook_out_proj/hook_blocks.14.hook_out_proj.pt\n",
      "15\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry210.txtblocks.15.hook_out_proj/hook_blocks.15.hook_out_proj.pt\n",
      "16\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry211.txtblocks.16.hook_out_proj/hook_blocks.16.hook_out_proj.pt\n",
      "17\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry212.txtblocks.17.hook_out_proj/hook_blocks.17.hook_out_proj.pt\n",
      "18\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry213.txtblocks.18.hook_out_proj/hook_blocks.18.hook_out_proj.pt\n",
      "19\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry214.txtblocks.19.hook_out_proj/hook_blocks.19.hook_out_proj.pt\n",
      "20\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry215.txtblocks.20.hook_out_proj/hook_blocks.20.hook_out_proj.pt\n",
      "21\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry216.txtblocks.21.hook_out_proj/hook_blocks.21.hook_out_proj.pt\n",
      "using patching format\n",
      "ABC AB C\n",
      "ABC AC B\n",
      "\n",
      "ABC AB C\n",
      "ABC CB A\n",
      "\n",
      "ABC AB C\n",
      "ABD AB D\n",
      "\n",
      "ABC AC B\n",
      "ABC BC A\n",
      "\n",
      "ABC AC B\n",
      "ADC AC D\n",
      "\n",
      "ABC BA C\n",
      "ABC BC A\n",
      "\n",
      "ABC BA C\n",
      "ABC CA B\n",
      "\n",
      "ABC BA C\n",
      "ABD BA D\n",
      "\n",
      "ABC BC A\n",
      "DBC BC D\n",
      "\n",
      "ABC CA B\n",
      "ABC CB A\n",
      "\n",
      "ABC CA B\n",
      "ADC CA D\n",
      "\n",
      "ABC CB A\n",
      "DBC CB D\n",
      "\n",
      "using templates\n",
      "Then, [NAME], [NAME] and [NAME] went to the [PLACE]. [NAME] and [NAME] gave a [OBJECT] to\n",
      "with name positions (2, 4, 6, 12, 14)\n",
      "removed jesus\n",
      "['<|endoftext|>', 'Then', ',', ' Olivia', ',', ' Ian', ' and', ' Aaron', ' went', ' to', ' the', ' restaurant', '.', ' Aaron', ' and', ' Olivia', ' gave', ' a', ' computer', ' to']\n",
      "['<|endoftext|>', 'Then', ',', ' Olivia', ',', ' Ian', ' and', ' Aaron', ' went', ' to', ' the', ' restaurant', '.', ' Aaron', ' and', ' Ian', ' gave', ' a', ' computer', ' to']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", font_size=None, show=True, color_continuous_midpoint=0.0, **kwargs):\n",
    "    import plotly.express as px\n",
    "    import transformer_lens.utils as utils\n",
    "    fig = px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=color_continuous_midpoint, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs)\n",
    "    if not font_size is None:\n",
    "        if 'x' in kwargs:\n",
    "            fig.update_layout(\n",
    "              xaxis = dict(\n",
    "                tickmode='array',\n",
    "                tickvals = kwargs['x'],\n",
    "                ticktext = kwargs['x'], \n",
    "                ),\n",
    "               font=dict(size=font_size, color=\"black\"))\n",
    "        if 'y' in kwargs:\n",
    "            fig.update_layout(\n",
    "              yaxis = dict(\n",
    "                tickmode='array',\n",
    "                tickvals = kwargs['y'],\n",
    "                ticktext = kwargs['y'], \n",
    "                ),\n",
    "               font=dict(size=font_size, color=\"black\"))\n",
    "    plot_args = {\n",
    "        'width': 800,\n",
    "        'height': 600,\n",
    "        \"autosize\": False,\n",
    "        'showlegend': True,\n",
    "        'margin': {\"l\":0,\"r\":0,\"t\":100,\"b\":0}\n",
    "    }\n",
    "    \n",
    "    fig.update_layout(**plot_args)\n",
    "    fig.update_layout(legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    ))\n",
    "    if show:\n",
    "        fig.show(renderer)\n",
    "    else:\n",
    "        return fig\n",
    "\n",
    "\n",
    "# requires\n",
    "# pip install git+https://github.com/Phylliida/MambaLens.git\n",
    "\n",
    "from mamba_lens import HookedMamba # this will take a little while to import\n",
    "import torch\n",
    "model_path = \"state-spaces/mamba-370m\"\n",
    "\n",
    "# NOTE! We need to monkeypatch transformer lens to use register_full_backward_hook\n",
    "model = HookedMamba.from_pretrained(model_path, device='cuda')\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "if not \"/home/dev/sae-k-sparse-mamba/sae\" in sys.path:\n",
    "    sys.path.append(\"/home/dev/sae-k-sparse-mamba\")\n",
    "import os\n",
    "os.chdir('/home/dev/sae-k-sparse-mamba')\n",
    "saes = [None]\n",
    "from importlib import reload\n",
    "from sae.sae import Sae\n",
    "\n",
    "ckpt_dir = \"/home/dev/sae-k-sparse-mamba/\"\n",
    "for i in range(1,22):\n",
    "    print(i)\n",
    "    hook = f'blocks.{i}.hook_out_proj'\n",
    "    path = [ckpt_dir + f for f in sorted(list(os.listdir(ckpt_dir))) if hook in f][0] + \"/\" + f'hook_{hook}.pt'\n",
    "    #path = f'/home/dev/sae-k-sparse-mamba/blocks.{i}.hook_resid_pre/hook_blocks.{i}.hook_resid_pre.pt'\n",
    "    print(path)\n",
    "    #saes.append(Sae.load_from_disk(path, hook=f'blocks.{i}.hook_resid_pre', device=model.cfg.device))\n",
    "\n",
    "\n",
    "global PATCHING_FORMAT_I\n",
    "global patching_formats\n",
    "def make_data(num_patching_pairs, patching, template_i, seed, valid_seed):\n",
    "    constrain_to_answers = True\n",
    "    # this makes our data size 800, first 400 is each (a,b) pair, and then second 400 is each pair swapped to be (b,a)\n",
    "    has_symmetric_patching = True\n",
    "    \n",
    "    n1_patchings = [\"\"\"\n",
    "    ABC BC A\n",
    "    DBC BC D\"\"\",\n",
    "        \"\"\"\n",
    "    ABC CB A\n",
    "    DBC CB D\"\"\"]\n",
    "    \n",
    "    n2_patchings = [\"\"\"\n",
    "    ABC AC B\n",
    "    ADC AC D\"\"\",\n",
    "        \"\"\"\n",
    "    ABC CA B\n",
    "    ADC CA D\"\"\"]\n",
    "    \n",
    "    n3_patchings = [\"\"\"\n",
    "    ABC AB C\n",
    "    ABD AB D\"\"\",\n",
    "        \"\"\"\n",
    "    ABC BA C\n",
    "    ABD BA D\"\"\"]\n",
    "    \n",
    "    n4_patchings = [\"\"\"\n",
    "    ABC AC B\n",
    "    ABC BC A\"\"\",\n",
    "        \"\"\"\n",
    "    ABC AB C\n",
    "    ABC CB A\"\"\",\n",
    "        \"\"\"\n",
    "    ABC BA C\n",
    "    ABC CA B\"\"\"]\n",
    "    \n",
    "    n5_patchings = [\"\"\"\n",
    "    ABC CA B\n",
    "    ABC CB A\n",
    "    \"\"\",\n",
    "        \"\"\"\n",
    "    ABC BA C\n",
    "    ABC BC A\"\"\",\n",
    "        \"\"\"\n",
    "    ABC AB C\n",
    "    ABC AC B\"\"\"]\n",
    "    \n",
    "    patchings = {\n",
    "        'n1': n1_patchings,\n",
    "        'n2': n2_patchings,\n",
    "        'n3': n3_patchings,\n",
    "        'n4': n4_patchings,\n",
    "        'n5': n5_patchings\n",
    "    }\n",
    "    \n",
    "    all_patchings = []\n",
    "    for patching_set in patchings.values():\n",
    "        all_patchings += patching_set\n",
    "    all_patchings = sorted(all_patchings) # make deterministic \n",
    "    \n",
    "    patch_all_names = [\"\"\"\n",
    "    ABC AB C\n",
    "    DEF DE F\"\"\",\n",
    "        \"\"\"\n",
    "    ABC AC B\n",
    "    DEF DF E\"\"\",     \n",
    "        \"\"\"\n",
    "    ABC BA C\n",
    "    DEF ED F\"\"\",\n",
    "        \"\"\"\n",
    "    ABC BC A\n",
    "    DEF EF D\"\"\",\n",
    "        \"\"\"\n",
    "    ABC CA B\n",
    "    DEF FD E\"\"\",\n",
    "        \"\"\"\n",
    "    ABC CB A\n",
    "    DEF FE D\"\"\"]\n",
    "    \n",
    "    \n",
    "    patchings['all'] = all_patchings\n",
    "    patchings['allatonce'] = patch_all_names\n",
    "    from acdc.data.ioi import BABA_TEMPLATES, ABC_TEMPLATES\n",
    "    from acdc.data.ioi import ioi_data_generator, ABC_TEMPLATES, get_all_single_name_abc_patching_formats\n",
    "    from acdc.data.utils import generate_dataset\n",
    "    templates = ABC_TEMPLATES\n",
    "    #patching_formats = list(get_all_single_name_abc_patching_formats())\n",
    "    global PATCHING_FORMAT_I\n",
    "    global patching_formats\n",
    "    PATCHING_FORMAT_I = patching\n",
    "    patching_formats = [\"\\n\".join([line.strip() for line in x.split(\"\\n\")]).strip() for x in patchings[PATCHING_FORMAT_I]]\n",
    "    \n",
    "    print(\"using patching format\")\n",
    "    for patch in patching_formats:\n",
    "        print(patch)\n",
    "        print(\"\")\n",
    "    #print(patching_formats)\n",
    "    \n",
    "    \n",
    "    data = generate_dataset(model=model,\n",
    "                      data_generator=ioi_data_generator,\n",
    "                      num_patching_pairs=4,\n",
    "                      seed=seed,\n",
    "                      valid_seed=valid_seed,\n",
    "                      constrain_to_answers=constrain_to_answers,\n",
    "                      has_symmetric_patching=has_symmetric_patching, \n",
    "                      varying_data_lengths=True,\n",
    "                      templates=templates,\n",
    "                      patching_formats=patching_formats)\n",
    "    \n",
    "    \n",
    "    import acdc.data.ioi\n",
    "    from collections import defaultdict\n",
    "    name_positions_map = defaultdict(lambda: [])\n",
    "    for template in templates:\n",
    "        name = acdc.data.ioi.good_names[0]\n",
    "        template_filled_in = template.replace(\"[NAME]\", name)\n",
    "        template_filled_in = template_filled_in.replace(\"[PLACE]\", acdc.data.ioi.good_nouns['[PLACE]'][0])\n",
    "        template_filled_in = template_filled_in.replace(\"[OBJECT]\", acdc.data.ioi.good_nouns['[OBJECT]'][0])\n",
    "        # get the token positions of the [NAME] in the prompt\n",
    "        name_positions = tuple([(i) for (i,s) in enumerate(model.to_str_tokens(torch.tensor(model.tokenizer.encode(template_filled_in)))) if s == f' {name}'])\n",
    "        name_positions_map[name_positions].append(template)\n",
    "    sorted_by_frequency = sorted(list(name_positions_map.items()), key=lambda x: -len(x[1]))\n",
    "    most_frequent_name_positions, templates = sorted_by_frequency[0]\n",
    "    print(\"using templates\")\n",
    "    templates = [templates[0]]\n",
    "    for template in templates:\n",
    "        print(template)\n",
    "    print(f\"with name positions {most_frequent_name_positions}\")\n",
    "    import acdc.data.ioi\n",
    "    if 'Jesus' in acdc.data.ioi.good_names:\n",
    "        print(\"removed jesus\")\n",
    "        acdc.data.ioi.good_names.remove(\"Jesus\")\n",
    "    data = generate_dataset(model=model,\n",
    "                  data_generator=ioi_data_generator,\n",
    "                  num_patching_pairs=num_patching_pairs,\n",
    "                  seed=seed,\n",
    "                  valid_seed=valid_seed,\n",
    "                  constrain_to_answers=constrain_to_answers,\n",
    "                  has_symmetric_patching=has_symmetric_patching, \n",
    "                  varying_data_lengths=True,\n",
    "                  templates=templates,\n",
    "                  patching_formats=patching_formats)\n",
    "    \n",
    "    print(model.to_str_tokens(data.data[0]))\n",
    "    print(model.to_str_tokens(data.data[1]))\n",
    "    return data\n",
    "\n",
    "\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "# we do a hacky thing where this first hook clears the global storage\n",
    "# second hook stores all the hooks\n",
    "# then third hook computes the output (over all the hooks)\n",
    "# this avoids recomputing and so is much faster\n",
    "SAE_HOOKS = \"sae hooks\"\n",
    "SAE_BATCHES = \"sae batches\"\n",
    "SAE_OUTPUT = \"sae output\"\n",
    "def sae_patching_storage_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    sae_feature_i: int,\n",
    "    dummy: bool,\n",
    "    position: int,\n",
    "    layer: int,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    **kwargs,\n",
    "):\n",
    "    global sae_storage\n",
    "    if not SAE_HOOKS in sae_storage:\n",
    "        sae_storage[SAE_HOOKS] = [] # we can't do this above because it'll be emptied again on the next batch before this is called\n",
    "    sae_storage[SAE_OUTPUT] = None # clear output\n",
    "    sae_storage[SAE_HOOKS].append({\"position\": position, \"sae_feature_i\": sae_feature_i, \"dummy\": dummy})\n",
    "    #print(f\"sae feature i {sae_feature_i} position {position} layer {layer}\")\n",
    "    return x\n",
    "\n",
    "from jaxtyping import Float\n",
    "from einops import rearrange\n",
    "\n",
    "global sae_storage\n",
    "sae_storage = {}\n",
    "def sae_patching_hook(\n",
    "    x: Float[torch.Tensor, \"B L E\"],\n",
    "    hook: HookPoint,\n",
    "    input_hook_name: str,\n",
    "    layer: int,\n",
    "    **kwargs,\n",
    ") -> Float[torch.Tensor, \"B L E\"]:\n",
    "    global sae_storage\n",
    "    ### This is identical to what the conv is doing\n",
    "    # but we break it apart so we can patch on individual filters\n",
    "    # we have two input hooks, the second one is the one we want\n",
    "    input_hook_name = input_hook_name[1]\n",
    "    # don't recompute these if we don't need to\n",
    "    # because we stored all the hooks and batches in conv_storage, we can just do them all at once\n",
    "    \n",
    "    # they need to share an output because they write to the same output tensor\n",
    "    if sae_storage[SAE_OUTPUT] is None:\n",
    "        #print(f\"running for layer {layer}\")\n",
    "        K = saes[layer].cfg.k\n",
    "        sae = saes[layer]\n",
    "        #print(f\"layer {layer} storage {sae_storage}\")\n",
    "        sae_output = torch.zeros(x.size(), device=model.cfg.device)\n",
    "        #print(\"layer\", layer, \"keys\", conv_storage)\n",
    "        def get_filter_key(i):\n",
    "            return f'filter_{i}'\n",
    "        sae_input_uncorrupted = x[::2]\n",
    "        sae_input_corrupted = x[1::2]\n",
    "        B, L, D = sae_input_uncorrupted.size()\n",
    "        for l in range(L):\n",
    "            # [B, NFeatures]                             [B,D]\n",
    "            uncorrupted_features = sae.encode(sae_input_uncorrupted[:,l])\n",
    "            # [B, NFeatures]                             [B,D]\n",
    "            corrupted_features = sae.encode(sae_input_corrupted[:,l])\n",
    "            patched_features = corrupted_features.clone()\n",
    "            #patched_features = torch.zeros(corrupted_features.size(), device=model.cfg.device) # patch everything except the features we are keeping around\n",
    "            # apply hooks (one hook applies to a single feature)\n",
    "            #print(f\"{len(sae_storage[SAE_HOOKS])} hooks\")\n",
    "            for hook_data in sae_storage[SAE_HOOKS]:\n",
    "                position = hook_data['position']\n",
    "                sae_feature_i = hook_data['sae_feature_i']\n",
    "                dummy = hook_data['dummy']\n",
    "                if not dummy and (position == l or position is None): # position is None means all positions\n",
    "                    if copy_from_other:\n",
    "                        patched_features[:,sae_feature_i] = corrupted_features[:,sae_feature_i]\n",
    "                    else:\n",
    "                        patched_features[:,sae_feature_i] = uncorrupted_features[:,sae_feature_i]\n",
    "                    \n",
    "                    #print(f\"applying sae feature {sae_feature_i} to position {position} for layer {layer}\")\n",
    "                    #uncorrupted_features[:,sae_feature_i] = corrupted_features[:,sae_feature_i]\n",
    "            # compute sae outputs\n",
    "            patched_top_acts, patched_top_indices = patched_features.topk(K, sorted=False)\n",
    "            corrupted_top_acts, corrupted_top_indices = corrupted_features.topk(K, sorted=False)      \n",
    "            sae_output[::2,l] = sae.decode(patched_top_acts, patched_top_indices)     \n",
    "            sae_output[1::2,l] = sae.decode(corrupted_top_acts, corrupted_top_indices)\n",
    "        sae_storage = {} # clean up and prepare for next layer\n",
    "        sae_storage[SAE_OUTPUT] = sae_output # store the output\n",
    "    return sae_storage[SAE_OUTPUT]\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "@dataclass\n",
    "class SAEFeature:\n",
    "    \"\"\"Class for keeping track of an item in inventory.\"\"\"\n",
    "    layer: int\n",
    "    pos: int\n",
    "    feature_i: int\n",
    "    attr: float\n",
    "    records: list = field(default_factory=lambda: [])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.layer) + \" \" + str(self.pos) + \" \" + str(self.feature_i) + \" \" + str(self.attr)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "def get_name_counts(feature):\n",
    "    name_counts = {}\n",
    "    DATA_LEN = len(feature.records)\n",
    "    records_tensor = torch.tensor(feature.records)\n",
    "    non_zero_indices = torch.arange(DATA_LEN)[records_tensor!=0]\n",
    "    non_zero_tokens = data.data[non_zero_indices,feature.pos].cpu()\n",
    "    non_zero_records = records_tensor[non_zero_indices]\n",
    "    name_tokens = torch.unique(non_zero_tokens)\n",
    "    for name_token in name_tokens:\n",
    "        name_str = model.to_str_tokens(name_token.view(1,1))[0]\n",
    "        name_counts[name_str] = non_zero_records[non_zero_tokens==name_token.item()]\n",
    "    #for t,c in template_counts.items():\n",
    "    #    print(f\" template {t} with count {torch.mean(torch.tensor(c)).item()}\")\n",
    "    name_counts = sorted(list(name_counts.items()), key=lambda x: -torch.mean(x[1]).item())\n",
    "    return name_counts\n",
    "    #for n,c in name_counts[:100]:\n",
    "    #    print(f\" name {n} with avg {torch.mean(c).item()} min {torch.min(c).item()} max {torch.max(c).item()}\")\n",
    "\n",
    "data = make_data(num_patching_pairs=2, patching=\"all\", template_i=0, seed=24, valid_seed=23)\n",
    "\n",
    "toks = model.to_str_tokens(data.data[0])\n",
    "name_positions = [3,5,7,13,15]\n",
    "position_map = {}\n",
    "L = data.data.size()[1]\n",
    "for l in range(L):\n",
    "    position_map[l] = f'pos{l}{toks[l]}'\n",
    "position_map[3] = 'n1'\n",
    "position_map[5] = 'n2'\n",
    "position_map[7] = 'n3'\n",
    "position_map[13] = 'n4'\n",
    "position_map[15] = 'n5'\n",
    "position_map[19] = 'out'\n",
    "import pickle\n",
    "with open(\"cached_sae_feature_edges.pkl\", \"rb\") as f:\n",
    "    edges_to_keep = pickle.load(f)\n",
    "\n",
    "h = model.to_str_tokens(torch.arange(model.tokenizer.vocab_size))\n",
    "spaceThings = [(i, x) for (i, x) in enumerate(h) if x[0] == ' ' and len(x.strip()) > 0]\n",
    "prefix = data.data[0][:3].view(1,-1)\n",
    "new_data_toks = torch.tensor([tok for (tok,s) in spaceThings], device=model.cfg.device)\n",
    "data_for_all_tokens = torch.cat([prefix.repeat((len(new_data_toks),1)), new_data_toks.view(-1,1)], dim=1)\n",
    "data.data = data_for_all_tokens\n",
    "with open(\"layer_15_features_more_more.pkl\", \"rb\") as f:\n",
    "    data, features = pickle.load(f)\n",
    "\n",
    "'''\n",
    "#names = sorted(list(acdc.data.ioi.good_names))\n",
    "names = [x for (i,x) in spaceThings if len(x.strip()) > 0]\n",
    "NUM_NAMES = len(names)\n",
    "#name_to_i = dict([(\" \" + name, i) for (i, name) in enumerate(names)])\n",
    "name_to_i = dict([(name, i) for (i, name) in enumerate(names)])\n",
    "\n",
    "def get_name_vector(feature, feat_type):\n",
    "    name_vec = torch.zeros(NUM_NAMES)\n",
    "    for name,counts in get_name_counts(feature):\n",
    "        if feat_type == 'mean':\n",
    "            name_vec[name_to_i[name]] = torch.mean(counts)\n",
    "        elif feat_type == 'min':\n",
    "            name_vec[name_to_i[name]] = torch.min(counts)\n",
    "        elif feat_type == 'max':\n",
    "            name_vec[name_to_i[name]] = torch.max(counts)\n",
    "            \n",
    "    return name_vec\n",
    "'''\n",
    "features_sorted_by_feat_i = defaultdict(lambda: [])\n",
    "for feature in features:\n",
    "    features_sorted_by_feat_i[feature.feature_i].append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4902ccc-0a6f-49c6-b58a-86343d7b4167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 38989/38989 [00:39<00:00, 976.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 38989/38989 [00:40<00:00, 962.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 38989/38989 [00:40<00:00, 965.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 38989/38989 [00:40<00:00, 966.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 38989/38989 [00:40<00:00, 963.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 38989/38989 [00:40<00:00, 962.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 38989/38989 [00:40<00:00, 966.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 38989/38989 [00:40<00:00, 966.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 38989/38989 [00:40<00:00, 961.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 38989/38989 [00:40<00:00, 963.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 38989/38989 [00:40<00:00, 960.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 38989/38989 [00:40<00:00, 962.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 38989/38989 [00:40<00:00, 959.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 38989/38989 [00:40<00:00, 957.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 38989/38989 [00:40<00:00, 963.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 38989/38989 [00:40<00:00, 956.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 38989/38989 [00:40<00:00, 959.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 38989/38989 [00:40<00:00, 958.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 38989/38989 [00:40<00:00, 959.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████▎                              | 23216/38989 [00:24<00:16, 962.05it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m num_records \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m tqdm(features):\n\u001b[0;32m---> 17\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecords\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     18\u001b[0m     num_records \u001b[38;5;241m=\u001b[39m records\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     19\u001b[0m     top \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(records, K)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "all_features = []\n",
    "del features\n",
    "from collections import defaultdict\n",
    "K = 400\n",
    "feature_i_top_k_indices = defaultdict(lambda: torch.tensor([]))\n",
    "feature_i_top_k_values = defaultdict(lambda: torch.tensor([]))\n",
    "num_data_points_seen = 0\n",
    "for i in range(100, 5900, 100):\n",
    "    print(i)\n",
    "    path = f'/home/dev/sae-k-sparse-mamba/layer_15_features_on_large_data{i}.pkl'\n",
    "    with open(path, \"rb\") as f:\n",
    "        features = pickle.load(f)\n",
    "\n",
    "    num_records = 0\n",
    "    for feature in tqdm(features):\n",
    "        records = torch.tensor(feature.records).flatten()\n",
    "        num_records = records.size()[0]\n",
    "        top = torch.topk(records, K)\n",
    "        # offset indices by total num seen so far\n",
    "        top_indices = top.indices + num_data_points_seen\n",
    "        top_values = top.values\n",
    "        merged_top_indices = torch.concatenate([feature_i_top_k_indices[feature.feature_i], top_indices])\n",
    "        merged_top_values = torch.concatenate([feature_i_top_k_values[feature.feature_i], top_values])\n",
    "        merged_top = torch.topk(merged_top_values, K)\n",
    "        feature_i_top_k_indices[feature.feature_i] = merged_top_indices[merged_top.indices]\n",
    "        feature_i_top_k_values[feature.feature_i] = merged_top_values[merged_top.indices]\n",
    "        del feature.records\n",
    "    num_data_points_seen += num_records\n",
    "    del features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0739872-938b-4808-a78f-97eb14f606f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "all_features = []\n",
    "del features\n",
    "from collections import defaultdict\n",
    "K = 400\n",
    "feature_i_top_k_indices = defaultdict(lambda: torch.tensor([]))\n",
    "feature_i_top_k_values = defaultdict(lambda: torch.tensor([]))\n",
    "num_data_points_seen = 0\n",
    "for i in range(100, 5900, 100):\n",
    "    print(i)\n",
    "    path = f'/home/dev/sae-k-sparse-mamba/layer_15_features_on_large_data{i}.pkl'\n",
    "    with open(path, \"rb\") as f:\n",
    "        features = pickle.load(f)\n",
    "\n",
    "    num_records = 0\n",
    "    for feature in tqdm(features):\n",
    "        records = torch.tensor(feature.records).flatten()\n",
    "        num_records = records.size()[0]\n",
    "        top = torch.topk(records, K)\n",
    "        # offset indices by total num seen so far\n",
    "        top_indices = top.indices + num_data_points_seen\n",
    "        top_values = top.values\n",
    "        merged_top_indices = torch.concatenate([feature_i_top_k_indices[feature.feature_i], top_indices])\n",
    "        merged_top_values = torch.concatenate([feature_i_top_k_values[feature.feature_i], top_values])\n",
    "        merged_top = torch.topk(merged_top_values, K)\n",
    "        feature_i_top_k_indices[feature.feature_i] = merged_top_indices[merged_top.indices]\n",
    "        feature_i_top_k_values[feature.feature_i] = merged_top_values[merged_top.indices]\n",
    "        del feature.records\n",
    "    num_data_points_seen += num_records\n",
    "    del features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e588d9-9045-4010-9803-589aa6a45a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100, 5900, 100):\n",
    "    print(i)\n",
    "    path = f'/home/dev/sae-k-sparse-mamba/layer_15_features_on_large_data{i}.pkl'\n",
    "    with open(path + \"h\", \"rb\") as f:\n",
    "        features = pickle.load(f)\n",
    "\n",
    "    num_records = len(features[0].records)\n",
    "    data = torch.zeros(len(all_feature_i), num_records, 128, device='cuda')\n",
    "    for feature in tqdm.tqdm(features):\n",
    "        data[feature_to_storage_index[feature.feature_i], :, feature.pos] = torch.tensor(feature.records)\n",
    "        del feature.records\n",
    "    \n",
    "    with open(path + \"h\", \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(num_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34598beb-0b7d-406f-850f-349dc054e199",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"layer_15_top_act_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump((dict(feature_i_top_k_indices), dict(feature_i_top_k_values)), f)\n",
    "del features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72337b4b-0e36-4a55-9176-3c99e5a8f9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleanup features\n",
      "100\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1046.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1052.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1051.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1051.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:36<00:00, 1055.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1052.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1049.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1043.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1044.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1048.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1037.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1045.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1035.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1037.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1044.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1037.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1041.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1045.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1038.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1035.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1038.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1035.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1032.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1041.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1036.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1032.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1040.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1042.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1035.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1041.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1038.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1033.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1035.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1035.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1032.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1041.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1031.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1038.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1037.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1035.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1035.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1032.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1037.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1030.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1036.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1038.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1026.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1033.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1035.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1033.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1033.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1036.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1033.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1036.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1035.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1033.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 38989/38989 [00:37<00:00, 1034.19it/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "#K = 400\n",
    "if 'data' in globals():\n",
    "    del data\n",
    "if 'features' in globals():\n",
    "    print(\"cleanup features\")\n",
    "    for feature in features:\n",
    "        if hasattr(feature, 'records'):\n",
    "            del feature.records\n",
    "    del features\n",
    "with open(\"layer_15_top_act_data.pkl\", \"rb\") as f:\n",
    "    feature_i_top_k_indices, feature_i_top_k_values = pickle.load(f)\n",
    "\n",
    "\n",
    "all_feature_i = sorted(list(feature_i_top_k_indices.keys()))\n",
    "feature_to_storage_index = dict([(feat_i,index) for (index,feat_i) in enumerate(all_feature_i)])\n",
    "\n",
    "del feature_i_top_k_indices, feature_i_top_k_values\n",
    "\n",
    "#all_data = torch.zeros(len(all_feature_i), K, 128, device=torch.device(model.cfg.device))\n",
    "\n",
    "#num_data_points_seen = 0\n",
    "import tqdm\n",
    "data = torch.zeros(len(all_feature_i), num_records, 128, device='cuda')\n",
    "\n",
    "for i in range(100, 5900, 100):\n",
    "    print(i)\n",
    "    if os.path.exists(f'/home/dev/sae-k-sparse-mamba/layer_15_features_on_large_data{i}.pkl' + 'h'):\n",
    "        continue\n",
    "    path = f'/home/dev/sae-k-sparse-mamba/layer_15_features_on_large_data{i}.pkl'\n",
    "    with open(path, \"rb\") as f:\n",
    "        features = pickle.load(f)\n",
    "\n",
    "    num_records = len(features[0].records)\n",
    "    if data.size()[1] != num_records:\n",
    "        del data\n",
    "        data = torch.zeros(len(all_feature_i), num_records, 128, device='cuda')\n",
    "    for feature in tqdm.tqdm(features):\n",
    "        data[feature_to_storage_index[feature.feature_i], :, feature.pos] = torch.tensor(feature.records)\n",
    "        del feature.records\n",
    "    \n",
    "    with open(path + \"h\", \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    '''\n",
    "    for feature_i in tqdm.tqdm(all_feature_i):\n",
    "        feature_storage_index = feature_to_storage_index[feature_i]\n",
    "        indices = feature_i_top_k_indices[feature_i] - num_data_points_seen\n",
    "        indices_in_this_dataset = indices[indices < num_records and indices >= 0].nonzero().flatten()\n",
    "        if indicies_in_this_dataset.size()[0] > 0:\n",
    "            for feature in features:\n",
    "                if feature.feature_i == feature_i:\n",
    "                    for storage_index in indices_in_this_dataset:\n",
    "                        data_index = indices[storage_index]\n",
    "                        all_data[feature_storage_index, storage_index, feature.pos] = feature.records[data_index]\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ffa4d6-cbc4-437f-a016-09a0fe752fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 307/307 [00:00<00:00, 1167.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "#K = 400\n",
    "if 'data' in globals():\n",
    "    del data\n",
    "if 'features' in globals():\n",
    "    print(\"cleanup features\")\n",
    "    for feature in features:\n",
    "        if hasattr(feature, 'records'):\n",
    "            del feature.records\n",
    "    del features\n",
    "with open(\"layer_15_top_act_data.pkl\", \"rb\") as f:\n",
    "    feature_i_top_k_indices, feature_i_top_k_values = pickle.load(f)\n",
    "K = 400\n",
    "\n",
    "all_feature_i = sorted(list(feature_i_top_k_indices.keys()))\n",
    "feature_to_storage_index = dict([(feat_i,index) for (index,feat_i) in enumerate(all_feature_i)])\n",
    "\n",
    "\n",
    "all_data = torch.zeros(len(all_feature_i), K, 128, device=torch.device(model.cfg.device))\n",
    "\n",
    "#num_data_points_seen = 0\n",
    "import tqdm\n",
    "\n",
    "num_data_points_seen = 0\n",
    "for i in range(100, 5900, 100):\n",
    "    print(i)\n",
    "    path = f'/home/dev/sae-k-sparse-mamba/layer_15_features_on_large_data{i}.pklh'\n",
    "    if 'feature_data' in globals():\n",
    "        del feature_data\n",
    "    with open(path, \"rb\") as f:\n",
    "        feature_data = pickle.load(f)\n",
    "    num_records = feature_data.size()[1]\n",
    "    for feature_i in tqdm.tqdm(all_feature_i):\n",
    "        feature_storage_index = feature_to_storage_index[feature_i]\n",
    "        indices = feature_i_top_k_indices[feature_i] - num_data_points_seen\n",
    "        filter_index = torch.logical_and(0 <= indices, indices < num_records)\n",
    "        indices_in_this_dataset = torch.round(indices[filter_index]).long()\n",
    "        #print(indices_in_this_dataset)\n",
    "        storage_top_k_indices = filter_index.nonzero().flatten()\n",
    "        if indices_in_this_dataset.size()[0] > 0:\n",
    "            for index_in_this_dataset, storage_index in zip(indices_in_this_dataset, storage_top_k_indices):\n",
    "                all_data[feature_storage_index, storage_index, :] = feature_data[feature_storage_index, index_in_this_dataset, :]\n",
    "    num_data_points_seen += num_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "131d02b6-8fda-4a8d-8059-c05e9e56b372",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_15_data_topk.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab8bca58-f6cd-4e96-8e36-3c6dadb05239",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'feature_data' in globals():\n",
    "    del feature_data\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sae.data import chunk_and_tokenize\n",
    "dataset = load_dataset(\n",
    "    \"togethercomputer/RedPajama-Data-1T-Sample\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = model.tokenizer\n",
    "# too many processes crashes, probably memory issue\n",
    "tokenized = chunk_and_tokenize(dataset, tokenizer, num_proc=8)\n",
    "\n",
    "with open(\"all_15_data_topk.pkl\", \"rb\") as f:\n",
    "    top_k_data = pickle.load(f)\n",
    "\n",
    "\n",
    "with open(\"layer_15_top_act_data.pkl\", \"rb\") as f:\n",
    "    feature_i_top_k_indices, feature_i_top_k_values = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3ecef68-20d8-43a1-9aca-4f548c13eaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 307/307 [00:30<00:00,  9.96it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "K = 400\n",
    "\n",
    "all_feature_i = sorted(list(feature_i_top_k_indices.keys()))\n",
    "feature_to_storage_index = dict([(feat_i,index) for (index,feat_i) in enumerate(all_feature_i)])\n",
    "\n",
    "\n",
    "token_data = torch.zeros(len(all_feature_i), K, 128, device=torch.device(model.cfg.device), dtype=torch.long)\n",
    "\n",
    "import tqdm\n",
    "for feature_i in tqdm.tqdm(all_feature_i):\n",
    "    storage_index = feature_to_storage_index[feature_i]\n",
    "    indices = feature_i_top_k_indices[feature_i]\n",
    "    for k, index in enumerate(indices):\n",
    "        token_data[storage_index, k, :] = tokenized[torch.round(index).long().item()]['input_ids'][:128]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "508569b5-c266-4bbe-bda9-2a44885a1325",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_15_top_dataset_tokens.pkl\", \"wb\") as f:\n",
    "    pickle.dump(token_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fc726f56-30a5-48e1-a789-1ac8d86aaf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry190.txtblocks.1.hook_out_proj/hook_blocks.1.hook_out_proj.pt\n",
      "2\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry191.txtblocks.2.hook_out_proj/hook_blocks.2.hook_out_proj.pt\n",
      "3\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry192.txtblocks.3.hook_out_proj/hook_blocks.3.hook_out_proj.pt\n",
      "4\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry193.txtblocks.4.hook_out_proj/hook_blocks.4.hook_out_proj.pt\n",
      "5\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry194.txtblocks.5.hook_out_proj/hook_blocks.5.hook_out_proj.pt\n",
      "6\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry195.txtblocks.6.hook_out_proj/hook_blocks.6.hook_out_proj.pt\n",
      "7\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry196.txtblocks.7.hook_out_proj/hook_blocks.7.hook_out_proj.pt\n",
      "8\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry200.txtblocks.8.hook_out_proj/hook_blocks.8.hook_out_proj.pt\n",
      "9\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry201.txtblocks.9.hook_out_proj/hook_blocks.9.hook_out_proj.pt\n",
      "10\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry202.txtblocks.10.hook_out_proj/hook_blocks.10.hook_out_proj.pt\n",
      "11\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry203.txtblocks.11.hook_out_proj/hook_blocks.11.hook_out_proj.pt\n",
      "12\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry204.txtblocks.12.hook_out_proj/hook_blocks.12.hook_out_proj.pt\n",
      "13\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry205.txtblocks.13.hook_out_proj/hook_blocks.13.hook_out_proj.pt\n",
      "14\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry206.txtblocks.14.hook_out_proj/hook_blocks.14.hook_out_proj.pt\n",
      "15\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry210.txtblocks.15.hook_out_proj/hook_blocks.15.hook_out_proj.pt\n",
      "16\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry211.txtblocks.16.hook_out_proj/hook_blocks.16.hook_out_proj.pt\n",
      "17\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry212.txtblocks.17.hook_out_proj/hook_blocks.17.hook_out_proj.pt\n",
      "18\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry213.txtblocks.18.hook_out_proj/hook_blocks.18.hook_out_proj.pt\n",
      "19\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry214.txtblocks.19.hook_out_proj/hook_blocks.19.hook_out_proj.pt\n",
      "20\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry215.txtblocks.20.hook_out_proj/hook_blocks.20.hook_out_proj.pt\n",
      "21\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry216.txtblocks.21.hook_out_proj/hook_blocks.21.hook_out_proj.pt\n"
     ]
    }
   ],
   "source": [
    "saes = [None]\n",
    "from importlib import reload\n",
    "from sae.sae import Sae\n",
    "\n",
    "ckpt_dir = \"/home/dev/sae-k-sparse-mamba/\"\n",
    "for i in range(1,22):\n",
    "    print(i)\n",
    "    hook = f'blocks.{i}.hook_out_proj'\n",
    "    path = [ckpt_dir + f for f in sorted(list(os.listdir(ckpt_dir))) if hook in f][0] + \"/\" + f'hook_{hook}.pt'\n",
    "    #path = f'/home/dev/sae-k-sparse-mamba/blocks.{i}.hook_resid_pre/hook_blocks.{i}.hook_resid_pre.pt'\n",
    "    print(path)\n",
    "    saes.append(Sae.load_from_disk(path, hook=f'blocks.{i}.hook_resid_pre', device=model.cfg.device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe2bc6d-b8ed-40fc-9c17-2110ee777551",
   "metadata": {},
   "source": [
    "# Manual Test Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "45c5d1b4-93d5-4c23-8cc5-bbc56773391d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4930, 15877,   275,  ...,    94,   187,    61])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([4.2673, 4.0469, 4.0382, 3.9979, 3.9902, 3.9018, 3.7470, 3.7401, 3.7275,\n",
       "        3.6963, 3.6875, 3.6277, 3.5944, 3.5785, 3.5744, 3.5667, 3.5530, 3.5463,\n",
       "        3.5434, 3.5374, 3.5213, 3.5202, 3.5123, 3.4987, 3.4917, 3.4869, 3.4850,\n",
       "        3.4803, 3.4707, 3.4626, 3.4598, 3.4536, 3.4520, 3.4518, 3.4452, 3.4434,\n",
       "        3.4391, 3.4332, 3.4228, 3.4105, 3.4003, 3.3875, 3.3851, 3.3741, 3.3713,\n",
       "        3.3679, 3.3645, 3.3644, 3.3633, 3.3604, 3.3394, 3.3393, 3.3365, 3.3337,\n",
       "        3.3295, 3.3251, 3.3204, 3.3172, 3.3167, 3.3156, 3.3148, 3.3126, 3.3100,\n",
       "        3.3093, 3.3088, 3.2926, 3.2893, 3.2834, 3.2780, 3.2755, 3.2729, 3.2688,\n",
       "        3.2681, 3.2591, 3.2584, 3.2529, 3.2470, 3.2467, 3.2449, 3.2411, 3.2411,\n",
       "        3.2383, 3.2372, 3.2365, 3.2319, 3.2305, 3.2245, 3.2240, 3.2236, 3.2180,\n",
       "        3.2175, 3.2166, 3.2073, 3.2071, 3.2070, 3.2032, 3.2001, 3.1926, 3.1894,\n",
       "        3.1889, 3.1888, 3.1882, 3.1877, 3.1868, 3.1867, 3.1842, 3.1840, 3.1807,\n",
       "        3.1799, 3.1783, 3.1750, 3.1743, 3.1729, 3.1704, 3.1701, 3.1691, 3.1684,\n",
       "        3.1666, 3.1665, 3.1662, 3.1657, 3.1653, 3.1634, 3.1633, 3.1632, 3.1610,\n",
       "        3.1592, 3.1570, 3.1565, 3.1545, 3.1527, 3.1526, 3.1475, 3.1468, 3.1453,\n",
       "        3.1428, 3.1400, 3.1393, 3.1382, 3.1342, 3.1296, 3.1294, 3.1245, 3.1230,\n",
       "        3.1220, 3.1218, 3.1217, 3.1213, 3.1202, 3.1169, 3.1165, 3.1135, 3.1133,\n",
       "        3.1030, 3.1029, 3.1029, 3.1008, 3.0992, 3.0988, 3.0982, 3.0977, 3.0977,\n",
       "        3.0949, 3.0946, 3.0933, 3.0908, 3.0904, 3.0900, 3.0859, 3.0856, 3.0828,\n",
       "        3.0808, 3.0799, 3.0773, 3.0766, 3.0704, 3.0691, 3.0679, 3.0668, 3.0658,\n",
       "        3.0635, 3.0621, 3.0598, 3.0576, 3.0558, 3.0555, 3.0551, 3.0529, 3.0526,\n",
       "        3.0524, 3.0521, 3.0516, 3.0510, 3.0502, 3.0495, 3.0489, 3.0464, 3.0455,\n",
       "        3.0445, 3.0442, 3.0437, 3.0427, 3.0415, 3.0401, 3.0398, 3.0385, 3.0383,\n",
       "        3.0382, 3.0380, 3.0379, 3.0363, 3.0351, 3.0349, 3.0343, 3.0337, 3.0301,\n",
       "        3.0288, 3.0285, 3.0283, 3.0278, 3.0240, 3.0230, 3.0227, 3.0226, 3.0217,\n",
       "        3.0203, 3.0198, 3.0196, 3.0165, 3.0151, 3.0137, 3.0134, 3.0117, 3.0114,\n",
       "        3.0113, 3.0110, 3.0110, 3.0100, 3.0094, 3.0094, 3.0092, 3.0079, 3.0075,\n",
       "        3.0051, 3.0041, 3.0035, 3.0032, 3.0031, 3.0022, 3.0022, 3.0003, 2.9999,\n",
       "        2.9978, 2.9975, 2.9974, 2.9972, 2.9950, 2.9942, 2.9930, 2.9926, 2.9911,\n",
       "        2.9908, 2.9891, 2.9876, 2.9871, 2.9847, 2.9829, 2.9820, 2.9819, 2.9818,\n",
       "        2.9816, 2.9815, 2.9803, 2.9801, 2.9796, 2.9788, 2.9784, 2.9784, 2.9780,\n",
       "        2.9779, 2.9771, 2.9770, 2.9769, 2.9764, 2.9755, 2.9743, 2.9740, 2.9715,\n",
       "        2.9701, 2.9687, 2.9681, 2.9673, 2.9653, 2.9641, 2.9636, 2.9624, 2.9619,\n",
       "        2.9617, 2.9595, 2.9593, 2.9581, 2.9575, 2.9571, 2.9568, 2.9565, 2.9562,\n",
       "        2.9556, 2.9550, 2.9549, 2.9545, 2.9540, 2.9539, 2.9530, 2.9526, 2.9522,\n",
       "        2.9518, 2.9514, 2.9506, 2.9503, 2.9500, 2.9497, 2.9487, 2.9487, 2.9480,\n",
       "        2.9455, 2.9452, 2.9450, 2.9441, 2.9414, 2.9400, 2.9392, 2.9383, 2.9380,\n",
       "        2.9376, 2.9374, 2.9363, 2.9347, 2.9345, 2.9336, 2.9326, 2.9313, 2.9309,\n",
       "        2.9307, 2.9304, 2.9299, 2.9286, 2.9284, 2.9273, 2.9269, 2.9263, 2.9241,\n",
       "        2.9241, 2.9233, 2.9232, 2.9227, 2.9227, 2.9224, 2.9217, 2.9202, 2.9201,\n",
       "        2.9194, 2.9192, 2.9179, 2.9175, 2.9168, 2.9166, 2.9161, 2.9157, 2.9154,\n",
       "        2.9131, 2.9130, 2.9110, 2.9108, 2.9090, 2.9087, 2.9081, 2.9070, 2.9054,\n",
       "        2.9052, 2.9043, 2.9041, 2.9010, 2.8987, 2.8973, 2.8970, 2.8967, 2.8947,\n",
       "        2.8943, 2.8935, 2.8919, 2.8918, 2.8918, 2.8908, 2.8900, 2.8894, 2.8889,\n",
       "        2.8884, 2.8877, 2.8875, 2.8872])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "indices = torch.round(feature_i_top_k_indices[2380]).long()\n",
    "print(tokenized[indices[0].item()]['input_ids'])\n",
    "feature_i_top_k_values[2380]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "43b84fb9-c048-4fba-9961-24d0c9c60245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3870f03f87c04f748d8bee8746e2d6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', continuous_update=False, description='Test String', placeholder='Test String')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291525158eb546138a787a6e88b141a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', continuous_update=False, description='Feature Index', placeholder='Feature Index')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da1d74fba0543cbb7a609c8ebfe38d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', continuous_update=False, description='Eval Tokens', placeholder='Eval Tokens')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e9385bca8b44acbd33b953508e1193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Test', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8eca8f700d849a0a103c77b50180ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SAEFeature:\n",
    "    \"\"\"Class for keeping track of an item in inventory.\"\"\"\n",
    "    layer: int\n",
    "    pos: int\n",
    "    feature_i: int\n",
    "    attr: float\n",
    "    records: list = field(default_factory=lambda: [])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.layer) + \" \" + str(self.pos) + \" \" + str(self.feature_i) + \" \" + str(self.attr)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "\n",
    "with open(\"features_all_tokens_layer_15_with_collected_data.pkl\", \"rb\") as f:\n",
    "    features = pickle.load(f)\n",
    "\n",
    "\n",
    "def get_batched_index_into(indices):\n",
    "    '''\n",
    "    given data that is [B,N,V] and indicies that are [B,N,K] with each index being an index into the V space\n",
    "    this gives you indexes you can use to access your values\n",
    "    '''\n",
    "    first_axis = []\n",
    "    second_axis = []\n",
    "    third_axis = []\n",
    "    B, _, _ = indices.size()\n",
    "    for b in range(B):\n",
    "        second, third = get_index_into(indices[b])\n",
    "        first_axis.append(torch.full(second.size(), fill_value=b, device=model.cfg.device))\n",
    "        second_axis.append(second)\n",
    "        third_axis.append(third)\n",
    "\n",
    "    return torch.cat(first_axis), torch.cat(second_axis), torch.cat(third_axis)\n",
    "\n",
    "def get_index_into(indices):\n",
    "    '''\n",
    "    given data that is [N,V] and indicies that are [N,K] with each index being an index into the V space\n",
    "    this gives you indexes you can use to access your values\n",
    "    '''\n",
    "    num_data, num_per_data = indices.size()\n",
    "    # we want\n",
    "    # [0,0,0,...,] num per data of these\n",
    "    # [1,1,1,...,] num per data of these\n",
    "    # ...\n",
    "    # [num_data-1, num_data-1, ...]\n",
    "    first_axis_index = torch.arange(num_data, dtype=torch.long).view(num_data, 1)*torch.ones([num_data, num_per_data], dtype=torch.long)\n",
    "    # now we flatten it so it has an index for each term aligned with our indices\n",
    "    first_axis_index = first_axis_index.flatten()\n",
    "    second_axis_index = indices.flatten()\n",
    "    return first_axis_index, second_axis_index\n",
    "global buffer\n",
    "buffer = None\n",
    "global features_by_layer\n",
    "def sae_hook(\n",
    "    x,\n",
    "    hook,\n",
    "    layer,\n",
    "):\n",
    "    # s is [B,L,E]\n",
    "    K = saes[layer].cfg.k\n",
    "    sae = saes[layer]\n",
    "    B,L,D = x.size()\n",
    "    uncorrupted_features = sae.encode(x)\n",
    "    top_acts, top_indices = uncorrupted_features.topk(K, sorted=False)\n",
    "    buffer = torch.zeros(uncorrupted_features.size(), device=model.cfg.device)\n",
    "    global features_by_layer\n",
    "    # zero everything except the top k\n",
    "    buffer[get_batched_index_into(top_indices)] = top_acts.flatten()\n",
    "    for feature in features_by_layer[layer]:\n",
    "        if feature.pos < L: # sometimes prompt is too small to consider this feature\n",
    "            feature.records += buffer[:,feature.pos,feature.feature_i].tolist()\n",
    "    # kernel can't handle doing all token positions at same time by default\n",
    "    # but if we make it think B*L is a single batch index it works fine\n",
    "    top_acts_flattened = top_acts.flatten(start_dim=0, end_dim=1)\n",
    "    top_indices_flattened = top_indices.flatten(start_dim=0, end_dim=1)\n",
    "    sae_out = sae.decode(top_acts_flattened, top_indices_flattened)\n",
    "    sae_out = sae_out.unflatten(dim=0, sizes=(B,L))\n",
    "    return sae_out\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def forward_check_features(data, features):\n",
    "    \n",
    "    global features_by_layer\n",
    "    \n",
    "    #with open(\"layer_15_features_on_large_data.pkl\", \"rb\") as f:\n",
    "    #    features = pickle.load(f)\n",
    "    features_by_layer = defaultdict(lambda: [])\n",
    "    for feature in features:\n",
    "        feature.records = []\n",
    "        features_by_layer[feature.layer].append(feature)\n",
    "\n",
    "    # only bother with SAE on the layers we are checking\n",
    "    layers_to_apply_sae = sorted(list(features_by_layer.keys()))\n",
    "    hooks = [(f'blocks.{layer}.hook_out_proj', partial(sae_hook, layer=layer)) for layer in layers_to_apply_sae]\n",
    "    _ = model.run_with_hooks(input=data, fwd_hooks=hooks, fast_ssm=True, fast_conv=True)\n",
    "        \n",
    "import traceback\n",
    "def clicked(arg):\n",
    "    with outputTesting:\n",
    "        clear_output()\n",
    "        try:\n",
    "            if eval_data.value.strip() == \"\":\n",
    "                text = text_item.value\n",
    "                tokenized_input = torch.tensor([model.tokenizer.bos_token_id] + model.tokenizer.encode(text), device=model.cfg.device).reshape(1,-1)\n",
    "            else:\n",
    "                tokenized_input = eval(eval_data.value).reshape(1, -1)\n",
    "                print(\"eval to\", tokenized_input)\n",
    "            feature_i = int(feature_index.value)\n",
    "            L = tokenized_input.size()[1]\n",
    "            for feature in features:\n",
    "                if feature.feature_i == feature_i and feature.layer == 15:\n",
    "                    feat = feature\n",
    "            features_with_i = []\n",
    "            for i in range(1, L):\n",
    "                features_with_i.append(SAEFeature(layer=feat.layer, pos=i, feature_i=feature_i, attr=feat.attr))\n",
    "            forward_check_features(tokenized_input, features=features_with_i)\n",
    "            activations = torch.zeros(L)\n",
    "            for feature in features_with_i:\n",
    "                activations[feature.pos] = feature.records[0]\n",
    "            toks = model.to_str_tokens(tokenized_input[0])\n",
    "            print(toks)\n",
    "            token_pos = torch.argmax(activations).item()\n",
    "            if activations[token_pos] == 0.0:\n",
    "                token_pos = L\n",
    "            out_toks = []\n",
    "            print(activations)\n",
    "            for j,tok in enumerate(toks):\n",
    "                if len(tok.strip()) == 0:\n",
    "                    tok = repr(tok)\n",
    "                tok = tok.replace(\"\\n\", \"\\\\n\")\n",
    "                colored = f\"<span id='ayy'><font color='white'>{tok}</font></span>\"\n",
    "                if j < 1: continue\n",
    "                if j == token_pos:\n",
    "                    colored = f\"<span id='ayy'><font color='red'>{tok}</font></span>\"\n",
    "                elif activations[j].item() > 0.01:\n",
    "                    colored = f\"<span id='ayy'><font color='pink'>{tok}</font></span>\"\n",
    "                if activations[j].item() > 0.01:\n",
    "                    out_toks.append(f\"{colored}{activations[j].item():.3f}\")\n",
    "                else:\n",
    "                    out_toks.append(colored)\n",
    "            simpler = model.tokenizer.decode(tokenized_input[0,1:token_pos+1])\n",
    "            print(simpler)\n",
    "            if token_pos == L:\n",
    "                print(\"all zero\")\n",
    "            else:\n",
    "                display(HTML(toks[token_pos] + \"\\t||\\t\" + \"<span id='ayy'>\" + simpler + \"</span>\\t||\\t\" + \"\".join(out_toks))) \n",
    "        except:\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "                \n",
    "text_item = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Test String',\n",
    "    description='Test String',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    ")\n",
    "\n",
    "feature_index = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Feature Index',\n",
    "    description='Feature Index',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    ")\n",
    "eval_data = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Eval Tokens',\n",
    "    description='Eval Tokens',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    ")\n",
    "\n",
    "button_download = widgets.Button(description = 'Test')   \n",
    "button_download.on_click(clicked)\n",
    "\n",
    "outputTesting = widgets.Output()\n",
    "\n",
    "display(text_item)\n",
    "display(feature_index)\n",
    "display(eval_data)\n",
    "display(button_download)\n",
    "display(outputTesting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1823b8a0-2271-4641-9c58-91c6f1f2ea91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0587, 0.0000, 2.2101, 0.0623, 0.0815, 0.1905,\n",
       "        0.2061, 0.0000, 0.0000, 2.9549, 0.0000, 0.1316, 0.0923, 0.0353, 0.0729,\n",
       "        0.1159, 0.1034, 1.4236, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0722,\n",
       "        0.0818, 0.0000, 0.0846, 0.1015, 0.1813, 0.0664, 0.0000, 0.1172, 0.1185,\n",
       "        0.0000, 0.0000, 0.0831, 0.0509, 0.2250, 0.0000, 0.0000, 0.0000, 0.0944,\n",
       "        0.1050, 0.0000, 0.0000, 0.1076, 0.0000, 0.0000, 0.0755, 0.0982, 0.2454,\n",
       "        0.0363, 0.2620, 0.0663, 0.0319, 0.0000, 0.1723, 0.0000, 0.1357, 0.1387,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.6378, 0.0380, 0.0000, 0.0000, 0.0615,\n",
       "        0.0000, 0.0814, 0.0000, 0.0000, 0.0000, 0.1411, 0.1213, 0.0705, 0.1159,\n",
       "        0.0000, 0.0480, 0.0000, 0.1029, 0.0000, 0.0610, 0.0000, 0.0934, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1007, 0.1180, 0.0493, 0.0546,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0480, 0.0858,\n",
       "        0.0364, 0.0000, 0.1876, 0.0535, 0.1121, 0.1283, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.1557, 0.0000, 0.0000, 0.0766, 0.0971, 0.0000, 0.0328,\n",
       "        0.0000, 0.0000], device='cuda:0')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_data[feature_to_storage_index[2380], 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c697a8e7-927c-4f8a-afd9-e1cea7fd9626",
   "metadata": {},
   "source": [
    "# Display TopK Activations from Dataset subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fd409e77-f8b0-4524-8562-38ae30795ffb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='https://fonts.googleapis.com/css?family=Noto Sans' rel='stylesheet'>\n",
       "<style>\n",
       "/* Base Noto Sans and Serif for Latin, Greek, and Cyrillic */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans:wght@400;700&family=Noto+Serif:wght@400;700&display=swap');\n",
       "\n",
       "/* East Asian scripts */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@400;700&family=Noto+Sans+KR:wght@400;700&family=Noto+Sans+SC:wght@400;700&family=Noto+Sans+TC:wght@400;700&display=swap');\n",
       "\n",
       "/* South Asian scripts */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Devanagari:wght@400;700&family=Noto+Sans+Bengali:wght@400;700&family=Noto+Sans+Tamil:wght@400;700&display=swap');\n",
       "\n",
       "/* Middle Eastern scripts */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Arabic:wght@400;700&family=Noto+Sans+Hebrew:wght@400;700&display=swap');\n",
       "\n",
       "/* Other scripts */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Thai:wght@400;700&family=Noto+Sans+Ethiopic:wght@400;700&display=swap');\n",
       "\n",
       "/* Specialty fonts */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Mono:wght@400;700&family=Noto+Color+Emoji&display=swap');\n",
       "\n",
       "#ayy {\n",
       "  font-family: 'Noto Sans', 'Noto Sans JP', 'Noto Sans KR', 'Noto Sans SC', 'Noto Sans TC', \n",
       "               'Noto Sans Devanagari', 'Noto Sans Bengali', 'Noto Sans Tamil', \n",
       "               'Noto Sans Arabic', 'Noto Sans Hebrew', 'Noto Sans Thai', 'Noto Sans Ethiopic',\n",
       "               sans-serif;\n",
       "}\n",
       "\n",
       "/* Language-specific rules */\n",
       ":lang(ja) { font-family: 'Noto Sans JP', sans-serif; }\n",
       ":lang(ko) { font-family: 'Noto Sans KR', sans-serif; }\n",
       ":lang(zh-CN) { font-family: 'Noto Sans SC', sans-serif; }\n",
       ":lang(zh-TW) { font-family: 'Noto Sans TC', sans-serif; }\n",
       ":lang(hi) { font-family: 'Noto Sans Devanagari', sans-serif; }\n",
       ":lang(bn) { font-family: 'Noto Sans Bengali', sans-serif; }\n",
       ":lang(ta) { font-family: 'Noto Sans Tamil', sans-serif; }\n",
       ":lang(ar) { font-family: 'Noto Sans Arabic', sans-serif; }\n",
       ":lang(he) { font-family: 'Noto Sans Hebrew', sans-serif; }\n",
       ":lang(th) { font-family: 'Noto Sans Thai', sans-serif; }\n",
       ":lang(am), :lang(ti) { font-family: 'Noto Sans Ethiopic', sans-serif; }\n",
       "\n",
       "/* Emoji support */\n",
       ".emoji {\n",
       "  font-family: 'Noto Color Emoji', sans-serif;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a0d2b9f54243f38e809e8eff2466ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='ffff', continuous_update=False, description='String:', placeholder='Type something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db3e5143e9d4060b50e609ff7111243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with open(\"layer_15_top_act_data.pkl\", \"rb\") as f:\n",
    "    feature_i_top_k_indices, feature_i_top_k_values = pickle.load(f)\n",
    "\n",
    "with open(\"all_15_data_topk.pkl\", \"rb\") as f:\n",
    "    top_k_data = pickle.load(f)\n",
    "\n",
    "with open(\"all_15_top_dataset_tokens.pkl\", \"rb\") as f:\n",
    "    token_data = pickle.load(f)\n",
    "\n",
    "\n",
    "with open(\"layer_15_features.pkl\", \"rb\") as f:\n",
    "    feature_labels = pickle.load(f)\n",
    "with open(\"layer_15_features_take_two.pkl\", \"rb\") as f:\n",
    "    feature_labelsb = pickle.load(f)\n",
    "\n",
    "K = 400\n",
    "\n",
    "all_feature_i = sorted(list(feature_i_top_k_indices.keys()))\n",
    "feature_to_storage_index = dict([(feat_i,index) for (index,feat_i) in enumerate(all_feature_i)])\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, HTML\n",
    "global cur_feature_ind\n",
    "cur_feature_ind = None\n",
    "def display_unlabeled_feature():\n",
    "    global cur_feature_ind\n",
    "    global feature_labels\n",
    "    available_features = features_sorted_by_feat_i.keys() - feature_labelsb.keys()\n",
    "    for f in available_features:\n",
    "        if len(feature_labels[f].strip()) != 1: continue\n",
    "        if '?' in feature_labels[f]:\n",
    "           continue\n",
    "        print(f\"feature {f}\")\n",
    "        cur_feature_ind = f\n",
    "        text_item.value = feature_labels[f]\n",
    "        #if any([position_map[feat.pos][0] != 'n' for feat in feats]):\n",
    "            #print(f\"warning, feature {feat_i} has non name poses, all pos are {[position_map[f.pos] for f in feats]})\")\n",
    "        #    continue\n",
    "        display_feat(cur_feature_ind)\n",
    "        return\n",
    "    num_left = 0\n",
    "    for f in feature_labels.keys():\n",
    "        if \"?\" in feature_labels[f]:\n",
    "            num_left += 1\n",
    "    print(f\"num left {num_left}\")\n",
    "    for f in feature_labels.keys():\n",
    "        if \"?\" in feature_labels[f]:\n",
    "            cur_feature_ind = f\n",
    "            text_item.value = feature_labels[f]\n",
    "            display_feats(features_sorted_by_feat_i[f])\n",
    "            return\n",
    "\n",
    "display(HTML(\"\"\"<link href='https://fonts.googleapis.com/css?family=Noto Sans' rel='stylesheet'>\n",
    "<style>\n",
    "/* Base Noto Sans and Serif for Latin, Greek, and Cyrillic */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans:wght@400;700&family=Noto+Serif:wght@400;700&display=swap');\n",
    "\n",
    "/* East Asian scripts */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@400;700&family=Noto+Sans+KR:wght@400;700&family=Noto+Sans+SC:wght@400;700&family=Noto+Sans+TC:wght@400;700&display=swap');\n",
    "\n",
    "/* South Asian scripts */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Devanagari:wght@400;700&family=Noto+Sans+Bengali:wght@400;700&family=Noto+Sans+Tamil:wght@400;700&display=swap');\n",
    "\n",
    "/* Middle Eastern scripts */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Arabic:wght@400;700&family=Noto+Sans+Hebrew:wght@400;700&display=swap');\n",
    "\n",
    "/* Other scripts */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Thai:wght@400;700&family=Noto+Sans+Ethiopic:wght@400;700&display=swap');\n",
    "\n",
    "/* Specialty fonts */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Mono:wght@400;700&family=Noto+Color+Emoji&display=swap');\n",
    "\n",
    "#ayy {\n",
    "  font-family: 'Noto Sans', 'Noto Sans JP', 'Noto Sans KR', 'Noto Sans SC', 'Noto Sans TC', \n",
    "               'Noto Sans Devanagari', 'Noto Sans Bengali', 'Noto Sans Tamil', \n",
    "               'Noto Sans Arabic', 'Noto Sans Hebrew', 'Noto Sans Thai', 'Noto Sans Ethiopic',\n",
    "               sans-serif;\n",
    "}\n",
    "\n",
    "/* Language-specific rules */\n",
    ":lang(ja) { font-family: 'Noto Sans JP', sans-serif; }\n",
    ":lang(ko) { font-family: 'Noto Sans KR', sans-serif; }\n",
    ":lang(zh-CN) { font-family: 'Noto Sans SC', sans-serif; }\n",
    ":lang(zh-TW) { font-family: 'Noto Sans TC', sans-serif; }\n",
    ":lang(hi) { font-family: 'Noto Sans Devanagari', sans-serif; }\n",
    ":lang(bn) { font-family: 'Noto Sans Bengali', sans-serif; }\n",
    ":lang(ta) { font-family: 'Noto Sans Tamil', sans-serif; }\n",
    ":lang(ar) { font-family: 'Noto Sans Arabic', sans-serif; }\n",
    ":lang(he) { font-family: 'Noto Sans Hebrew', sans-serif; }\n",
    ":lang(th) { font-family: 'Noto Sans Thai', sans-serif; }\n",
    ":lang(am), :lang(ti) { font-family: 'Noto Sans Ethiopic', sans-serif; }\n",
    "\n",
    "/* Emoji support */\n",
    ".emoji {\n",
    "  font-family: 'Noto Color Emoji', sans-serif;\n",
    "}\n",
    "</style>\"\"\"))\n",
    "def display_feat(feature_i):\n",
    "    covered_already = set()\n",
    "    simpler_words = []\n",
    "    for k in range(K):\n",
    "        storage_index = feature_to_storage_index[feature_i]\n",
    "        activations = top_k_data[storage_index, k]\n",
    "        tokens = token_data[storage_index, k]\n",
    "        token_pos = torch.argmax(activations).item()\n",
    "        toks = model.to_str_tokens(tokens)\n",
    "        relevant_str = \"\".join(toks[:token_pos+1])\n",
    "        if relevant_str in covered_already:\n",
    "            continue\n",
    "        covered_already.add(relevant_str)\n",
    "        out_toks = []\n",
    "        colors = [''] + ['red', 'orange', 'yellow', 'green']*256\n",
    "        for j,tok in enumerate(toks):\n",
    "            if len(tok.strip()) == 0:\n",
    "                tok = repr(tok)\n",
    "            tok = tok.replace(\"\\n\", \"\\\\n\")\n",
    "            colored = f\"<span id='ayy'><font color='white'>{tok}</font></span>\"\n",
    "            if j < 1: continue\n",
    "            if j == token_pos:\n",
    "                colored = f\"<span id='ayy'><font color='red'>{tok}</font></span>\"\n",
    "            elif activations[j].item() > 0.01:\n",
    "                colored = f\"<span id='ayy'><font color='pink'>{tok}</font></span>\"\n",
    "            if activations[j].item() > 0.01:\n",
    "                out_toks.append(f\"{colored}{activations[j].item():.3f}\")\n",
    "            else:\n",
    "                out_toks.append(colored)\n",
    "        simpler = model.tokenizer.decode(tokens[1:token_pos+1])\n",
    "        simpler_words.append(simpler.strip())\n",
    "\n",
    "        display(HTML(toks[token_pos] + \"\\t||\\t\" + \"<span id='ayy'>\" + simpler + \"</span>\\t||\\t\" + \"\".join(out_toks)))\n",
    "        if len(simpler_words) == 20:\n",
    "            out_s = \"<span id='ayy'>What do \"\n",
    "            for s in simpler_words:\n",
    "                out_s += f'\"{s.strip()}\", '\n",
    "            out_s += \"have in common? Take a deep breath and think step by step.\"\n",
    "            display(HTML(out_s + \"</span>\"))\n",
    "    '''\n",
    "    feat_vecs = [get_name_vector(feat, 'mean') for feat in feats]\n",
    "    avg_vec = torch.stack(feat_vecs).mean(dim=0)\n",
    "    min_vec = torch.stack([get_name_vector(feat, 'min') for feat in feats]).min(dim=0).values\n",
    "    max_vec = torch.stack([get_name_vector(feat, 'max') for feat in feats]).max(dim=0).values\n",
    "    sorted_names = torch.argsort(-avg_vec)\n",
    "    #print(avg_vec, min_vec, max_vec, sorted_names)\n",
    "    for name_i in sorted_names[:100]:\n",
    "        #print(name_i)\n",
    "        print(f\" name {names[name_i]} with avg {avg_vec[name_i]} min {min_vec[name_i]} max {max_vec[name_i]}\")\n",
    "    '''\n",
    "    '''\n",
    "    for feat in feats:\n",
    "        if position_map[feat.pos][0] == 'n':\n",
    "            print(position_map[feat.pos], detect_single_letter(feat))\n",
    "            pretty_print_list_first_letter_info(list_first_letter_info(feat))\n",
    "            print(improved_first_letter(feat))\n",
    "    \n",
    "    diffs = torch.zeros(len(feats), len(feats))\n",
    "    for i,featv1 in enumerate(feat_vecs):\n",
    "        for j,featv2 in enumerate(feat_vecs):\n",
    "            diffs[i,j] = torch.mean(torch.abs(featv1-featv2))\n",
    "    labels = [position_map[feat.pos] for feat in feats]\n",
    "    imshow(diffs, x=labels, y=labels, font_size=9)\n",
    "    '''\n",
    "def save_labels():\n",
    "    with open(\"layer_15_features_take_two.pkl\", \"wb\") as f:\n",
    "        global feature_labelsb\n",
    "        pickle.dump(feature_labelsb, f)\n",
    "        print(f\"done saving {len(feature_labelsb)}\")\n",
    "import traceback\n",
    "out = widgets.Output()\n",
    "global cur_feature_ind\n",
    "global feature_labels\n",
    "def submitted(change):\n",
    "    global cur_feature_ind\n",
    "    global feature_labels\n",
    "    if len(text_item.value.strip()) > 0 and (text_item.value != feature_labels[cur_feature_ind]):\n",
    "        with out:\n",
    "            try:\n",
    "                times = 0\n",
    "                clear_output()\n",
    "                feature_labelsb[cur_feature_ind] = text_item.value\n",
    "                save_labels()\n",
    "                display_unlabeled_feature()\n",
    "            except:\n",
    "                print(traceback.format_exc())\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "text_item = widgets.Text(\n",
    "    value='ffff',\n",
    "    placeholder='Type something',\n",
    "    description='String:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    ")\n",
    "\n",
    "display(text_item)\n",
    "display(out)\n",
    "text_item.observe(submitted, names='value')\n",
    "\n",
    "with out:\n",
    "    display_unlabeled_feature()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15bea2e5-647d-4632-8b49-906000d6de5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cPickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcPickle\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtextlayer15.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     dat \u001b[38;5;241m=\u001b[39m cPickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cPickle'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with open(\"textlayer15.pkl\", \"rb\") as f:\n",
    "    dat = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ed54abf-41e1-4812-89ca-0516c05a4a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f55c75d-2756-4939-8f7c-e87377c24cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with open(\"textlayer15.pkl\", \"wb\") as f:\n",
    "    torch.save(features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f277460c-cc4a-423c-b14d-20075b82f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "global feature_labels\n",
    "import pickle\n",
    "with open(\"layer_15_features.pkl\", \"rb\") as f:\n",
    "    feature_labels = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7877047-a855-4c7e-8c5d-2c6e01710ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "global feature_labels\n",
    "import pickle\n",
    "with open(\"layer_15_features_take_two.pkl\", \"rb\") as f:\n",
    "    feature_labelsb = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b939cea7-2201-4894-97d1-d551b83d8754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{6146: 'M (also fires on final token of M words)', 4104: 'May, five, fifth ', 10252: 'R ', 12348: 'Famous Actors, actor portrayal, embodied, playing', 24649: \"? used to be 1800's and 1900's\", 22605: 'Repeated Token', 12365: '? ', 2129: 'Michigan Locations (previously MMA Wrestling and Sports)', 12371: 'Middle East Diplomacy (also haemostatic/thrombosis/punk rock/hematopoiesis)', 6227: 'Russian Names, 3 digit numbers, renal excretions, and escaped quotes', 2165: '? Muilti token phrases maybe?', 4214: 'quantum terminology ', 6266: 'me/you/your (previously interactions with webpage)', 20604: 'Kidney Function', 16512: '?, accountants, marriage and health (microbiome, antioxidants) related terms', 6272: 'Human Suffering (Cancer, Auschwitz, Guantanamo Bay, Divorce, Chernobyl) previously renal cancer terms/sports teams', 30887: 'denominator of/simplify sqrt(, previouslywine tasting', 14506: 'words put together with no space (prevoiusly all caps licensing/warranty terms)', 20651: 'Russia related terms (previously socialist countries/people, and escaped quotes', 16563: 'E and D words (mostly E, some F), token contains an e sound', 26824: 'D ', 6351: 'Russian Names (previously Cellular Signaling Pathways)', 8401: '? Opening Nested Brackets', 16598: '? Something about names', 6362: 'Communism Related Terms and locations', 16606: 'he/she/hes/hers/theirs pronoun ', 30976: 'S ', 6401: 'Q ', 22784: 'Compound nouns (two or more words)', 20736: 'driving/cars ', 20740: 'Designations of Military Units (previously, Japaneese or Chineese Terms)', 22790: 'B ', 24846: '? Contains an e', 22801: 'M ', 20764: 'measuring methods/instruments ', 2344: 'N ', 31017: '? ', 31021: 'A ', 12592: '? (maybe privacy/mind control related things?)', 28977: 'East Coast News and Sports (mostly New Jersey)', 28979: 'B ', 16702: '? dude/buddy/bro/mate/sir'}\n"
     ]
    }
   ],
   "source": [
    "print(feature_labelsb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a4dc57e-32f1-418c-af43-6c0984c129b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([389372, 5])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<link href='https://fonts.googleapis.com/css?family=Noto Sans' rel='stylesheet'>\n",
       "<style>\n",
       "/* Base Noto Sans and Serif for Latin, Greek, and Cyrillic */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans:wght@400;700&family=Noto+Serif:wght@400;700&display=swap');\n",
       "\n",
       "/* East Asian scripts */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@400;700&family=Noto+Sans+KR:wght@400;700&family=Noto+Sans+SC:wght@400;700&family=Noto+Sans+TC:wght@400;700&display=swap');\n",
       "\n",
       "/* South Asian scripts */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Devanagari:wght@400;700&family=Noto+Sans+Bengali:wght@400;700&family=Noto+Sans+Tamil:wght@400;700&display=swap');\n",
       "\n",
       "/* Middle Eastern scripts */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Arabic:wght@400;700&family=Noto+Sans+Hebrew:wght@400;700&display=swap');\n",
       "\n",
       "/* Other scripts */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Thai:wght@400;700&family=Noto+Sans+Ethiopic:wght@400;700&display=swap');\n",
       "\n",
       "/* Specialty fonts */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Mono:wght@400;700&family=Noto+Color+Emoji&display=swap');\n",
       "\n",
       "#ayy {\n",
       "  font-family: 'Noto Sans', 'Noto Sans JP', 'Noto Sans KR', 'Noto Sans SC', 'Noto Sans TC', \n",
       "               'Noto Sans Devanagari', 'Noto Sans Bengali', 'Noto Sans Tamil', \n",
       "               'Noto Sans Arabic', 'Noto Sans Hebrew', 'Noto Sans Thai', 'Noto Sans Ethiopic',\n",
       "               sans-serif;\n",
       "}\n",
       "\n",
       "/* Language-specific rules */\n",
       ":lang(ja) { font-family: 'Noto Sans JP', sans-serif; }\n",
       ":lang(ko) { font-family: 'Noto Sans KR', sans-serif; }\n",
       ":lang(zh-CN) { font-family: 'Noto Sans SC', sans-serif; }\n",
       ":lang(zh-TW) { font-family: 'Noto Sans TC', sans-serif; }\n",
       ":lang(hi) { font-family: 'Noto Sans Devanagari', sans-serif; }\n",
       ":lang(bn) { font-family: 'Noto Sans Bengali', sans-serif; }\n",
       ":lang(ta) { font-family: 'Noto Sans Tamil', sans-serif; }\n",
       ":lang(ar) { font-family: 'Noto Sans Arabic', sans-serif; }\n",
       ":lang(he) { font-family: 'Noto Sans Hebrew', sans-serif; }\n",
       ":lang(th) { font-family: 'Noto Sans Thai', sans-serif; }\n",
       ":lang(am), :lang(ti) { font-family: 'Noto Sans Ethiopic', sans-serif; }\n",
       "\n",
       "/* Emoji support */\n",
       ".emoji {\n",
       "  font-family: 'Noto Color Emoji', sans-serif;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f857796a6c7941ee8687af34d49ae2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='ffff', continuous_update=False, description='String:', placeholder='Type something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef099d78c4df477bac17f109afe67f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, HTML\n",
    "global cur_feature_ind\n",
    "cur_feature_ind = None\n",
    "def display_unlabeled_feature():\n",
    "    global cur_feature_ind\n",
    "    global feature_labels\n",
    "    available_features = features_sorted_by_feat_i.keys() - feature_labelsb.keys()\n",
    "    for f in available_features:\n",
    "        if not '?' in feature_labels[f]:\n",
    "            continue\n",
    "        print(f\"feature {f}\")\n",
    "        cur_feature_ind = f\n",
    "        feats = features_sorted_by_feat_i[f]\n",
    "        text_item.value = feature_labels[f]\n",
    "        #if any([position_map[feat.pos][0] != 'n' for feat in feats]):\n",
    "            #print(f\"warning, feature {feat_i} has non name poses, all pos are {[position_map[f.pos] for f in feats]})\")\n",
    "        #    continue\n",
    "        display_feats(feats)\n",
    "        return\n",
    "    num_left = 0\n",
    "    for f in feature_labels.keys():\n",
    "        if \"?\" in feature_labels[f]:\n",
    "            num_left += 1\n",
    "    print(f\"num left {num_left}\")\n",
    "    for f in feature_labels.keys():\n",
    "        if \"?\" in feature_labels[f]:\n",
    "            cur_feature_ind = f\n",
    "            text_item.value = feature_labels[f]\n",
    "            display_feats(features_sorted_by_feat_i[f])\n",
    "            return\n",
    "\n",
    "print(data.size())\n",
    "display(HTML(\"\"\"<link href='https://fonts.googleapis.com/css?family=Noto Sans' rel='stylesheet'>\n",
    "<style>\n",
    "/* Base Noto Sans and Serif for Latin, Greek, and Cyrillic */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans:wght@400;700&family=Noto+Serif:wght@400;700&display=swap');\n",
    "\n",
    "/* East Asian scripts */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@400;700&family=Noto+Sans+KR:wght@400;700&family=Noto+Sans+SC:wght@400;700&family=Noto+Sans+TC:wght@400;700&display=swap');\n",
    "\n",
    "/* South Asian scripts */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Devanagari:wght@400;700&family=Noto+Sans+Bengali:wght@400;700&family=Noto+Sans+Tamil:wght@400;700&display=swap');\n",
    "\n",
    "/* Middle Eastern scripts */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Arabic:wght@400;700&family=Noto+Sans+Hebrew:wght@400;700&display=swap');\n",
    "\n",
    "/* Other scripts */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Thai:wght@400;700&family=Noto+Sans+Ethiopic:wght@400;700&display=swap');\n",
    "\n",
    "/* Specialty fonts */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Mono:wght@400;700&family=Noto+Color+Emoji&display=swap');\n",
    "\n",
    "#ayy {\n",
    "  font-family: 'Noto Sans', 'Noto Sans JP', 'Noto Sans KR', 'Noto Sans SC', 'Noto Sans TC', \n",
    "               'Noto Sans Devanagari', 'Noto Sans Bengali', 'Noto Sans Tamil', \n",
    "               'Noto Sans Arabic', 'Noto Sans Hebrew', 'Noto Sans Thai', 'Noto Sans Ethiopic',\n",
    "               sans-serif;\n",
    "}\n",
    "\n",
    "/* Language-specific rules */\n",
    ":lang(ja) { font-family: 'Noto Sans JP', sans-serif; }\n",
    ":lang(ko) { font-family: 'Noto Sans KR', sans-serif; }\n",
    ":lang(zh-CN) { font-family: 'Noto Sans SC', sans-serif; }\n",
    ":lang(zh-TW) { font-family: 'Noto Sans TC', sans-serif; }\n",
    ":lang(hi) { font-family: 'Noto Sans Devanagari', sans-serif; }\n",
    ":lang(bn) { font-family: 'Noto Sans Bengali', sans-serif; }\n",
    ":lang(ta) { font-family: 'Noto Sans Tamil', sans-serif; }\n",
    ":lang(ar) { font-family: 'Noto Sans Arabic', sans-serif; }\n",
    ":lang(he) { font-family: 'Noto Sans Hebrew', sans-serif; }\n",
    ":lang(th) { font-family: 'Noto Sans Thai', sans-serif; }\n",
    ":lang(am), :lang(ti) { font-family: 'Noto Sans Ethiopic', sans-serif; }\n",
    "\n",
    "/* Emoji support */\n",
    ".emoji {\n",
    "  font-family: 'Noto Color Emoji', sans-serif;\n",
    "}\n",
    "</style>\"\"\"))\n",
    "def display_feats(feats):\n",
    "    all_records = []\n",
    "    feat_len = len(feats[0].records)\n",
    "    for feat in feats:\n",
    "        print(feat.pos)\n",
    "        all_records += feat.records\n",
    "    records = torch.tensor(all_records)\n",
    "    dats = [torch.tensor([1]).repeat(feat_len),torch.tensor([2]).repeat(feat_len),torch.tensor([3]).repeat(feat_len),torch.tensor([4]).repeat(feat_len)]\n",
    "    which_pos = torch.cat(dats)\n",
    "    print(records.size())\n",
    "    top_act_inds = torch.argsort(-records)\n",
    "    covered_already = set()\n",
    "    simpler_words = []\n",
    "    for i in range(500):\n",
    "        ind = top_act_inds[i]\n",
    "        token_pos = which_pos[ind]\n",
    "        data_pos = ind % feat_len\n",
    "        act = records[ind]\n",
    "        acts = [0]\n",
    "        for j in range(1,5):\n",
    "            acts.append([feat.records[data_pos] for feat in feats if feat.pos == j][0])\n",
    "        toks = model.to_str_tokens(data[data_pos])\n",
    "        relevant_str = \"\".join(toks[:token_pos+1])\n",
    "        if relevant_str in covered_already:\n",
    "            continue\n",
    "        covered_already.add(relevant_str)\n",
    "        out_toks = []\n",
    "        colors = ['', 'red', 'orange', 'yellow', 'green']\n",
    "        for j,tok in enumerate(toks):\n",
    "            if len(tok.strip()) == 0:\n",
    "                tok = repr(tok)\n",
    "            tok = tok.replace(\"\\n\", \"\\\\n\")\n",
    "            colored = f\"<span id='ayy'><font color='{colors[j]}'>{tok}</font></span>\"\n",
    "            if j < 1: continue\n",
    "            if j == token_pos:\n",
    "                colored = f\"<span id='ayy'><font color='pink'>{tok}</font</span>\"\n",
    "            if acts[j] > 0.01:\n",
    "                out_toks.append(f\"{colored}{acts[j]:.3f}\")\n",
    "            else:\n",
    "                out_toks.append(colored)\n",
    "        simpler = model.tokenizer.decode(data[data_pos][1:token_pos+1])\n",
    "        simpler_words.append(simpler.strip())\n",
    "\n",
    "        display(HTML(toks[token_pos] + \"\\t||\\t\" + \"<span id='ayy'>\" + simpler + \"</span>\\t||\\t\" + \"\".join(out_toks)))\n",
    "        if len(simpler_words) == 20:\n",
    "            out_s = \"<span id='ayy'>What do \"\n",
    "            for s in simpler_words:\n",
    "                out_s += f'\"{s.strip()}\", '\n",
    "            out_s += \"have in common? Take a deep breath and think step by step.\"\n",
    "            display(HTML(out_s + \"</span>\"))\n",
    "    '''\n",
    "    feat_vecs = [get_name_vector(feat, 'mean') for feat in feats]\n",
    "    avg_vec = torch.stack(feat_vecs).mean(dim=0)\n",
    "    min_vec = torch.stack([get_name_vector(feat, 'min') for feat in feats]).min(dim=0).values\n",
    "    max_vec = torch.stack([get_name_vector(feat, 'max') for feat in feats]).max(dim=0).values\n",
    "    sorted_names = torch.argsort(-avg_vec)\n",
    "    #print(avg_vec, min_vec, max_vec, sorted_names)\n",
    "    for name_i in sorted_names[:100]:\n",
    "        #print(name_i)\n",
    "        print(f\" name {names[name_i]} with avg {avg_vec[name_i]} min {min_vec[name_i]} max {max_vec[name_i]}\")\n",
    "    '''\n",
    "    '''\n",
    "    for feat in feats:\n",
    "        if position_map[feat.pos][0] == 'n':\n",
    "            print(position_map[feat.pos], detect_single_letter(feat))\n",
    "            pretty_print_list_first_letter_info(list_first_letter_info(feat))\n",
    "            print(improved_first_letter(feat))\n",
    "    \n",
    "    diffs = torch.zeros(len(feats), len(feats))\n",
    "    for i,featv1 in enumerate(feat_vecs):\n",
    "        for j,featv2 in enumerate(feat_vecs):\n",
    "            diffs[i,j] = torch.mean(torch.abs(featv1-featv2))\n",
    "    labels = [position_map[feat.pos] for feat in feats]\n",
    "    imshow(diffs, x=labels, y=labels, font_size=9)\n",
    "    '''\n",
    "def save_labels():\n",
    "    with open(\"layer_15_features_take_two.pkl\", \"wb\") as f:\n",
    "        global feature_labelsb\n",
    "        pickle.dump(feature_labelsb, f)\n",
    "        print(f\"done saving {len(feature_labelsb)}\")\n",
    "import traceback\n",
    "out = widgets.Output()\n",
    "global cur_feature_ind\n",
    "global feature_labels\n",
    "def submitted(change):\n",
    "    global cur_feature_ind\n",
    "    global feature_labels\n",
    "    if len(text_item.value.strip()) > 0 and (text_item.value != feature_labels[cur_feature_ind]):\n",
    "        with out:\n",
    "            try:\n",
    "                times = 0\n",
    "                clear_output()\n",
    "                feature_labelsb[cur_feature_ind] = text_item.value\n",
    "                save_labels()\n",
    "                display_unlabeled_feature()\n",
    "            except:\n",
    "                print(traceback.format_exc())\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "text_item = widgets.Text(\n",
    "    value='ffff',\n",
    "    placeholder='Type something',\n",
    "    description='String:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    ")\n",
    "\n",
    "display(text_item)\n",
    "display(out)\n",
    "text_item.observe(submitted, names='value')\n",
    "\n",
    "with out:\n",
    "    display_unlabeled_feature()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c72c3651-7d6e-4cab-8f22-15d7260fe014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='layer_15_features.pkl' target='_blank'>layer_15_features.pkl</a><br>"
      ],
      "text/plain": [
       "/home/dev/sae-k-sparse-mamba/layer_15_features.pkl"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, FileLink\n",
    "import os\n",
    "display(FileLink(\"layer_15_features.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d96ae99-0f7d-456f-a8be-365611fe5a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24649 1800's and 1900's MOREDATA: used to be 1800's and 1900's\n",
      "  attr -0.013689302893567401\n",
      "15582 1800-1999\n",
      "  attr -0.009123188330249832\n",
      "30568 1980's-1990s pop culture\n",
      "  attr -0.012585221676090441\n",
      "6227 3 digit numbers, renal excretions, and escaped quotes MOREDATA: Russian Names, 3 digit numbers, renal excretions, and escaped quotes\n",
      "  attr -0.008696446947169534\n",
      "8103 5/May/Five/Fifth\n",
      "  attr -0.01274918073158915\n",
      "32395 [ with some other symbols  [\", **[, etc.\n",
      "  attr -0.006224751563763675\n",
      "31021 A\n",
      "  attr -0.09784678883806919\n",
      "29892 A\n",
      "  attr -0.0719999106204341\n",
      "14819 about names\n",
      "  attr -0.009356041136925342\n",
      "1312 account/login terminology and oxidation/breathing\n",
      "  attr -0.017313731845206348\n",
      "12348 actor portrayal, embodied, playing MOREDATA: Famous Actors, actor portrayal, embodied, playing\n",
      "  attr -0.010314146090422582\n",
      "19724 aggressive/vigorously\n",
      "  attr -0.01069634652901641\n",
      "21271 Airline Brands and Punk Rock bands\n",
      "  attr -0.0069216288079587684\n",
      "15562 all caps hurricaine/military codename stuff\n",
      "  attr -0.015350925427355833\n",
      "14506 all caps licensing/warranty terms MOREDATA: words put together with no space  prevoiusly all caps licensing/warranty terms\n",
      "  attr -0.008297496336808763\n",
      "23646 assay chemical terms\n",
      "  attr -0.009188498152070679\n",
      "22790 B\n",
      "  attr -0.09131500174771645\n",
      "28979 B\n",
      "  attr -0.07274102425435558\n",
      "21593 blood related medical terminology\n",
      "  attr -0.007324386707068697\n",
      "28482 Canada related terms and names\n",
      "  attr -0.009112429528158827\n",
      "16927 Canada terms\n",
      "  attr -0.006526016593852546\n",
      "29448 Cancer research cell lines\n",
      "  attr -0.0069788733271707315\n",
      "9622 cancer treatment related terminology and wording: radiation related wording, chemotheraphy drugs, opiods\n",
      "  attr -0.008812440353722195\n",
      "18809 Carbon isotopes\n",
      "  attr -0.010045149520010455\n",
      "31591 cell biology\n",
      "  attr -0.00647655470220343\n",
      "11238 cell signaling\n",
      "  attr -0.006507405890260998\n",
      "6351 Cellular Signaling Pathways MOREDATA: Russian Names  previously Cellular Signaling Pathways\n",
      "  attr -0.006245518262403493\n",
      "30573 chemistry terms\n",
      "  attr -0.008322611262428836\n",
      "29391 circle related terms\n",
      "  attr -0.008665878058536691\n",
      "23218 Cities in Midwestern United States  mostly\n",
      "  attr -0.013883766508342887\n",
      "9660 clarifiers  seperately, equally, details, correspondingly, specifics, jointly\n",
      "  attr -0.011788112475187518\n",
      "9344 College Admission Terms  SAT, admissions, graduate, GRE\n",
      "  attr -0.006360145394864958\n",
      "29024 common denominator of\n",
      "  attr -0.007396902664368099\n",
      "6362 Communism Related Terms MOREDATA: Communism Related Terms and locations\n",
      "  attr -0.007367437463699389\n",
      "22784 Compound nouns  two or more words\n",
      "  attr -0.007089612839081383\n",
      "30030 contacting someone  call, telephone, email\n",
      "  attr -0.006253717307572515\n",
      "24846 Contains an e\n",
      "  attr -0.009616837047587978\n",
      "2836 contains subword within the word/phrase\n",
      "  attr -0.007383605534982962\n",
      "23306 court, protest and voting related news terms\n",
      "  attr -0.007382435433100909\n",
      "30041 cryptography terms\n",
      "  attr -0.008561787899452611\n",
      "19013 cybersecurity/security terms\n",
      "  attr -0.00771606135890579\n",
      "26824 D\n",
      "  attr -0.16980820387470885\n",
      "1349 D\n",
      "  attr -0.07883304957431392\n",
      "2517 denominator of\n",
      "  attr -0.006439173310184287\n",
      "7207 double L and also isotopes\n",
      "  attr -0.010156668427498516\n",
      "20736 driving/cars\n",
      "  attr -0.009892453394513723\n",
      "20089 driving/cars\n",
      "  attr -0.006260821712203324\n",
      "16702 dude/buddy/bro/mate/sir MOREDATA: dude/buddy/bro/mate/sir\n",
      "  attr -0.006263825359837938\n",
      "27758 E\n",
      "  attr -0.1747781486083113\n",
      "9371 E\n",
      "  attr -0.00970587123902078\n",
      "1444 E\n",
      "  attr -0.017846565402578562\n",
      "16563 E and D words  mostly E  MOREDATA: E and D words  mostly E, some F , token contains an e sound\n",
      "  attr -0.008854826909555413\n",
      "28977 East Coast News and Sports MOREDATA: East Coast News and Sports  mostly New Jersey\n",
      "  attr -0.01579690919766108\n",
      "9152 ends in ing  and less so ed\n",
      "  attr -0.0075197575945367134\n",
      "11188 Fighter/Bomber Wings\n",
      "  attr -0.013412890531981247\n",
      "24327 First/primary/january/1  also lesser, second\n",
      "  attr -0.006580403307452798\n",
      "32037 flying\n",
      "  attr -0.021024397003202466\n",
      "6674 Foreign language words  mostly russian but also japaneese\n",
      "  attr -0.016651658356181542\n",
      "31883 FORMAL LEGAL LANGUAGE IN ALL CAPS\n",
      "  attr -0.008560251996527768\n",
      "16138 G\n",
      "  attr -0.08049628931257757\n",
      "26556 G/H\n",
      "  attr -0.030817382530585746\n",
      "29097 Genocide and terrorism related countries, people, and organizations\n",
      "  attr -0.006131060118264031\n",
      "19433 genomics terms\n",
      "  attr -0.006696107015386588\n",
      "4463 german ends of words, and punk bands  Linkin Park, Slipknot, etc.\n",
      "  attr -0.006903969514269193\n",
      "23671 German words\n",
      "  attr -0.011143825632643711\n",
      "16606 he/she/hes/hers/theirs pronoun\n",
      "  attr -0.007248282881562318\n",
      "23385 HTTP terms\n",
      "  attr -0.007708824877681764\n",
      "16022 hunger/appetite/chewing\n",
      "  attr -0.014324363094146975\n",
      "8649 I\n",
      "  attr -0.033544364230692736\n",
      "17920 I\n",
      "  attr -0.022858697826450225\n",
      "1593 inflation\n",
      "  attr -0.006902257244973953\n",
      "6266 interactions with webpage MOREDATA: me/you/your  previously interactions with webpage\n",
      "  attr -0.006233607682247566\n",
      "5218 Ireland/Scottish names\n",
      "  attr -0.036398897432263766\n",
      "17703 irish/scottish\n",
      "  attr -0.006345220279399655\n",
      "25124 J/Y\n",
      "  attr -0.016267280905594816\n",
      "25903 J/Y\n",
      "  attr -0.02538938164434512\n",
      "20740 Japaneese or Chineese Terms MOREDATA: Designations of Military Units  previously, Japaneese or Chineese Terms\n",
      "  attr -0.008437638861096275\n",
      "21149 Javascript programming\n",
      "  attr -0.016306967873333633\n",
      "19977 Judicial proceedings\n",
      "  attr -0.009534094005857696\n",
      "1525 Judiciary news  packing, unconstitutional, sued, judiciary\n",
      "  attr -0.006557001974215382\n",
      "20604 Kidney Function\n",
      "  attr -0.008669543757378051\n",
      "21206 Kidney function terms\n",
      "  attr -0.044486247787062894\n",
      "17259 L\n",
      "  attr -0.09739969908878265\n",
      "21851 L\n",
      "  attr -0.008611365316937736\n",
      "32240 L\n",
      "  attr -0.02720839104949846\n",
      "20217 L\n",
      "  attr -0.007627729094110691\n",
      "12167 L\n",
      "  attr -0.033046428667148575\n",
      "29977 Late 1800's through 1990's, and notable figures of that time\n",
      "  attr -0.010183288915641242\n",
      "1429 Latex symbols and 3-digit numbers\n",
      "  attr -0.006139959426946007\n",
      "23490 lipids/cholesterol\n",
      "  attr -0.012551901419328715\n",
      "13212 liver disease/research on liver disease, or something about airlines\n",
      "  attr -0.010285173077136278\n",
      "6635 Locations  with hospitals\n",
      "  attr -0.019990712970866298\n",
      "22801 M\n",
      "  attr -0.045623418538525584\n",
      "6146 M MOREDATA: M  also fires on final token of M words\n",
      "  attr -0.09822651862123166\n",
      "9911 marajuana\n",
      "  attr -0.00798462220723195\n",
      "5740 marijuana\n",
      "  attr -0.006186059359151841\n",
      "16512 marriage and health  microbiome, antioxidants  related terms MOREDATA: , accountants, marriage and health  microbiome, antioxidants  related terms\n",
      "  attr -0.00977941253188419\n",
      "31070 marriage related terms\n",
      "  attr -0.006917106930359296\n",
      "17547 mate/sir/bro/dude/buddy\n",
      "  attr -0.010553130107382458\n",
      "12712 Math symbols\n",
      "  attr -0.006609869964904647\n",
      "23692 mathematical symbols\n",
      "  attr -0.006188363698811372\n",
      "4104 May, five, fifth\n",
      "  attr -0.009138316909229616\n",
      "27417 maybe contains c and/or h\n",
      "  attr -0.008059936456447758\n",
      "1231 maybe contains symbols\n",
      "  attr -0.007072995650787561\n",
      "15112 maybe double letters\n",
      "  attr -0.006430887430042276\n",
      "25370 maybe ends in a vowel\n",
      "  attr -0.006850492354715243\n",
      "17257 maybe partial phrase activation\n",
      "  attr -0.006392213238541444\n",
      "12592 maybe privacy/mind control related things\n",
      "  attr -0.020322264046171767\n",
      "20764 measuring methods/instruments\n",
      "  attr -0.007428856332012401\n",
      "6667 Medical surgery terms\n",
      "  attr -0.00754842196147365\n",
      "14859 Medical terms related to kidney function\n",
      "  attr -0.00621000153478235\n",
      "23657 Michigan cities, words, and teams\n",
      "  attr -0.016552651359234005\n",
      "12371 Middle East Diplomacy  also haemostatic/thrombosis/punk rock/hematopoiesis\n",
      "  attr -0.008280414368300626\n",
      "2129 MMA Wrestling and Sports MOREDATA: Michigan Locations  previously MMA Wrestling and Sports\n",
      "  attr -0.01086678510546335\n",
      "17165 mostly all caps locations or military code names  Troipical storm names  Not sure\n",
      "  attr -0.008432961922153481\n",
      "27844 Mostly duplicated phrases, but also some short phrases\n",
      "  attr -0.010690764611354098\n",
      "14934 Mostly punctuation\n",
      "  attr -0.019670318491989747\n",
      "32518 Mostly Tex related symbols\n",
      "  attr -0.008426743361269473\n",
      "2165 Muilti token phrases maybe\n",
      "  attr -0.00660846277685323\n",
      "2344 N\n",
      "  attr -0.042718175682239234\n",
      "8113 N\n",
      "  attr -0.07060992660626653\n",
      "8600 Name words like brother, mate, master, accountant\n",
      "  attr -0.015843032270822732\n",
      "17015 names\n",
      "  attr -0.012270050730080584\n",
      "21471 names\n",
      "  attr -0.00676922697948612\n",
      "5151 names\n",
      "  attr -0.022999352184797317\n",
      "19710 names\n",
      "  attr -0.006678973795146703\n",
      "30460 names\n",
      "  attr -0.00836467627982529\n",
      "5290 Names MOREDATA: Names, mostly fictional\n",
      "  attr -0.008184253030549371\n",
      "19851 Natural Disasters and also mate/sir/dude/bro/sis\n",
      "  attr -0.006872525849189515\n",
      "13434 negotiations  advocate, persuade, marketing, lobbying\n",
      "  attr -0.011237540364618326\n",
      "32156 News MOREDATA: Some bands  News\n",
      "  attr -0.009065301725968311\n",
      "8520 Notable events in 1850-1950\n",
      "  attr -0.00819831061903642\n",
      "29913 notification messages  coming soon/screen name, syntax error on, /**, some german words\n",
      "  attr -0.006672907451047649\n",
      "27326 occupations\n",
      "  attr -0.009404679599924748\n",
      "19210 oh my gosh but also some other stuff idk\n",
      "  attr -0.009025092790352574\n",
      "8401 Opening Nested Brackets MOREDATA: Opening Nested Brackets\n",
      "  attr -0.006932958481002061\n",
      "15921 P\n",
      "  attr -0.026116897555766627\n",
      "29400 partly/largely/portion\n",
      "  attr -0.007081040903358371\n",
      "2480 past tense verbs\n",
      "  attr -0.010681397359803668\n",
      "19119 patents/invention/innovation\n",
      "  attr -0.011078803682721627\n",
      "2438 phone calls\n",
      "  attr -0.019688784060690523\n",
      "4059 poem/verse/lyrics/musicians/playwright\n",
      "  attr -0.006929941609996604\n",
      "25584 Pop culture  polarizing topics   MOREDATA: Pop culture  polarizing topics    maybe duplicated letters\n",
      "  attr -0.00792502611375312\n",
      "24408 Pop culture from 1970s\n",
      "  attr -0.006387471017660573\n",
      "1560 Pop culture News\n",
      "  attr -0.006976166669574013\n",
      "25867 Pop culture terms and names\n",
      "  attr -0.009792545894811155\n",
      "9832 Popular Culture  mostly sports, but also honesty/lying related\n",
      "  attr -0.013338867276843303\n",
      "26388 portray/actors/embodied  and names of actors\n",
      "  attr -0.007027864370456882\n",
      "31773 possibly duplicated letters  Or having the word 2 or maybe typos  Not sure\n",
      "  attr -0.006422075800344373\n",
      "27945 prefixes of scientific terms\n",
      "  attr -0.006160893262517675\n",
      "6014 present tense verb\n",
      "  attr -0.013679897524980333\n",
      "19868 Probably copyright code comment related\n",
      "  attr -0.007609032425534679\n",
      "12210 probably greek symbols\n",
      "  attr -0.00959057461568591\n",
      "23448 probably the * symbol  Used for censoring maybe\n",
      "  attr -0.009247681292663401\n",
      "5066 programming languages/IEEE\n",
      "  attr -0.014873993666242313\n",
      "29937 programming related initialization/zero terms\n",
      "  attr -0.009706658512072863\n",
      "31304 proportion/percentage, acres, miles\n",
      "  attr -0.007669238376593057\n",
      "6401 Q\n",
      "  attr -0.06049906851876585\n",
      "7210 Q\n",
      "  attr -0.03905940388722229\n",
      "15762 Q\n",
      "  attr -0.006868912230402202\n",
      "11918 Q\n",
      "  attr -0.009669136925367638\n",
      "9856 quantum resonance/oscillation/vibration related terms\n",
      "  attr -0.008641311014798703\n",
      "4214 quantum terminology\n",
      "  attr -0.013722597336709441\n",
      "26187 quiz/assignments/exams, also nudity/uncovered/covered\n",
      "  attr -0.00627296765878782\n",
      "25010 quotations about sports teams and celebrities\n",
      "  attr -0.011194733123375045\n",
      "10252 R\n",
      "  attr -0.03757084527751431\n",
      "8935 R\n",
      "  attr -0.062377776339417323\n",
      "7436 R\n",
      "  attr -0.042790833933395334\n",
      "25933 R  and sometimes P, but mostly R\n",
      "  attr -0.06427513889138936\n",
      "13336 relations of people  parents, families, mothers, investors, parents, farmers\n",
      "  attr -0.009604730643332005\n",
      "11644 Relatives  dad, mom, grandfather, cousin\n",
      "  attr -0.008568098613068287\n",
      "6272 renal cancer terms/sports teams MOREDATA: Human Suffering  Cancer, Auschwitz, Guantanamo Bay, Divorce, Chernobyl  previously renal cancer terms/sports teams\n",
      "  attr -0.020549124503304483\n",
      "22605 Repeated Token\n",
      "  attr -0.016981449274680926\n",
      "2713 repeating a letter  or sound  twice\n",
      "  attr -0.02596145577626885\n",
      "9370 right leaning or corporate news terms  Scripture, MSM, aired, shareholders, investors, advertise\n",
      "  attr -0.011689641587963706\n",
      "15421 rotating objects\n",
      "  attr -0.013939963814436851\n",
      "8607 Russian symbols\n",
      "  attr -0.008129017246574222\n",
      "30976 S\n",
      "  attr -0.03369896866388444\n",
      "11839 S\n",
      "  attr -0.12674832851189421\n",
      "2404 salt related chemistry\n",
      "  attr -0.021208943013334647\n",
      "9772 Scottish/Irish related terms\n",
      "  attr -0.061268554543858045\n",
      "21909 second to last token in word\n",
      "  attr -0.011395784979413293\n",
      "21333 second to last token on a word, maybe\n",
      "  attr -0.008520465583387704\n",
      "3888 second/ii/double/february\n",
      "  attr -0.029396268168966344\n",
      "9474 second/ii/secondly and also seperately/supposedly related   MOREDATA: there's no doubt/denying and other stuff , previously second/ii/secondly and also seperately/supposedly related\n",
      "  attr -0.006523636489873752\n",
      "5334 sign related terminology  art stuff about how they are made, what put on, when used, etc.\n",
      "  attr -0.007916668177472275\n",
      "10175 sir/mate/bro also sports teams\n",
      "  attr -0.011609393237677068\n",
      "3765 sir/mate/bro/buddy/dude/bretheren\n",
      "  attr -0.009207125831153462\n",
      "10143 sir/mate/bro/happy faces, also programming terms\n",
      "  attr -0.009580517507856712\n",
      "9070 smell and taste related terms  also mitochondria\n",
      "  attr -0.006382021861895737\n",
      "20651 socialist countries/people, and escaped quotes MOREDATA: Russia related terms  previously socialist countries/people, and escaped quotes\n",
      "  attr -0.00942766465755085\n",
      "29875 some \"musings on life/thoughts on life/up to date on\" sorts of phrases\n",
      "  attr -0.00798489862995666\n",
      "29495 some name related, likes Mc\n",
      "  attr -0.0066899425416977465\n",
      "16598 Something about names\n",
      "  attr -0.0065409953317612235\n",
      "27600 spooky\n",
      "  attr -0.012488319768635847\n",
      "25299 Sports teams\n",
      "  attr -0.00725484013401001\n",
      "1925 Sports teams\n",
      "  attr -0.007098740324522623\n",
      "24468 Sports Teams\n",
      "  attr -0.02388748936459706\n",
      "1993 Sports Teams\n",
      "  attr -0.006114820432912893\n",
      "25307 Sports teams and drugs\n",
      "  attr -0.00787552531255642\n",
      "19260 sports teams, some medical terms  Not clearly double letters\n",
      "  attr -0.006481513251628712\n",
      "2765 sqrt, also\n",
      "  attr -0.01010312067887753\n",
      "28510 superhero terms  and cellular signaling\n",
      "  attr -0.017119744939918746\n",
      "28428 Symbols\n",
      "  attr -0.0072473992016171\n",
      "9187 T\n",
      "  attr -0.016009828459573328\n",
      "25771 T\n",
      "  attr -0.1365963689131604\n",
      "28222 T\n",
      "  attr -0.06432983783452073\n",
      "7315 tempest all caps related stuff\n",
      "  attr -0.0064665828858778696\n",
      "29260 things that people are  player, footballer, rapper, astronaut\n",
      "  attr -0.012476729858008184\n",
      "27463 Third/III/3\n",
      "  attr -0.01072121935976611\n",
      "8885 Third/III/3/March  also 4 a little\n",
      "  attr -0.00814137204997678\n",
      "5441 three digit numbers and determined/resolved/finishes MOREDATA: code notation  previously three digit numbers and determined/resolved/finishes\n",
      "  attr -0.02206722709161113\n",
      "23031 Three or two letter acronyms\n",
      "  attr -0.01146313421531886\n",
      "26276 tickets/permits/coupons/booking also signaling molocules\n",
      "  attr -0.014432658348027871\n",
      "9793 Tourism/Museum related terms\n",
      "  attr -0.007516761312444942\n",
      "23254 trademark/invention/lawyer related terms\n",
      "  attr -0.008407365721154747\n",
      "7153 traversing building words  upstairs, downstairs, browsing, directories, corridors, etc.\n",
      "  attr -0.019046447800519672\n",
      "21217 Tribalism/Polarising topics\n",
      "  attr -0.007970399494183766\n",
      "5164 Two letter acronyms\n",
      "  attr -0.007334597855788161\n",
      "2923 U\n",
      "  attr -0.05607139528365224\n",
      "9746 U\n",
      "  attr -0.0126178362279461\n",
      "3214 v\n",
      "  attr -0.008973046828941733\n",
      "7440 V\n",
      "  attr -0.05792993160139304\n",
      "7976 V\n",
      "  attr -0.006432281166325993\n",
      "25129 vacation/holidy trip terms\n",
      "  attr -0.008725606308871647\n",
      "23771 vegetarian/eco firendly related stuff, maybe\n",
      "  attr -0.014947573205859044\n",
      "21957 verbs\n",
      "  attr -0.013124387892958111\n",
      "2380 W\n",
      "  attr -0.04605771117894619\n",
      "28185 W\n",
      "  attr -0.03094202431384474\n",
      "29676 war related years and entities\n",
      "  attr -0.00757064356707815\n",
      "17946 web browser related tech terms\n",
      "  attr -0.006128787816805925\n",
      "28503 web development / some medical terms\n",
      "  attr -0.009363260717464073\n",
      "2370 wine\n",
      "  attr -0.014538607297254202\n",
      "29039 Wine and opiod related chemical properties\n",
      "  attr -0.01113584420522784\n",
      "11527 wine and vine related terms\n",
      "  attr -0.007252043727021373\n",
      "30887 wine tasting MOREDATA: denominator of/simplify sqrt , previouslywine tasting\n",
      "  attr -0.009843360778631904\n",
      "11431 X  and less so, Y  and roman numerals\n",
      "  attr -0.09644712582303328\n",
      "32344 X/crosses, also some y\n",
      "  attr -0.027355561614967883\n",
      "3481 Y/J and related sounds\n",
      "  attr -0.008252387309767073\n",
      "8860 Years  0-2000\n",
      "  attr -0.010908834899737485\n",
      "15551 Years  0-2000\n",
      "  attr -0.010188623953581555\n",
      "4516 z\n",
      "  attr -0.012184076008452394\n",
      "27951 Z\n",
      "  attr -0.006935771954204029\n",
      "unknown 12365 with attr -0.008066546563895827\n",
      "unknown 31017 with attr -0.006213662440359258\n",
      "unknown 8556 with attr -0.01649465186710586\n",
      "unknown 14726 with attr -0.006957205181606696\n",
      "unknown 6539 with attr -0.010329634168328994\n",
      "unknown 422 with attr -0.006121505353689827\n",
      "unknown 27074 with attr -0.00620498292278171\n",
      "unknown 8713 with attr -0.007830667949065173\n",
      "unknown 593 with attr -0.012529574942618638\n",
      "unknown 29281 with attr -0.009626295634461712\n",
      "unknown 17010 with attr -0.006254309687449222\n",
      "unknown 710 with attr -0.00834142514031555\n",
      "unknown 25316 with attr -0.007214687834846245\n",
      "unknown 25327 with attr -0.008484647296427283\n",
      "unknown 819 with attr -0.008638471323365593\n",
      "unknown 17207 with attr -0.00777073866356659\n",
      "unknown 27638 with attr -0.019120284184737102\n",
      "unknown 1021 with attr -0.011518310732981263\n",
      "unknown 21510 with attr -0.006987265755071803\n",
      "unknown 3099 with attr -0.007184532264091104\n",
      "unknown 31774 with attr -0.007802040568549273\n",
      "unknown 5333 with attr -0.006951492146072269\n",
      "unknown 9636 with attr -0.01230801921337843\n",
      "unknown 7590 with attr -0.012472123598854523\n",
      "unknown 28108 with attr -0.007399879723379854\n",
      "unknown 3563 with attr -0.006175605278485818\n",
      "unknown 30215 with attr -0.0073036741368923686\n",
      "unknown 11822 with attr -0.008759976565102079\n",
      "unknown 1582 with attr -0.006295407222978611\n",
      "unknown 28237 with attr -0.007709657324539876\n",
      "unknown 32334 with attr -0.013396876420983972\n",
      "unknown 13907 with attr -0.012828784556631945\n",
      "unknown 5728 with attr -0.01917508570750215\n",
      "unknown 5761 with attr -0.012286140343690022\n",
      "unknown 20115 with attr -0.008976714616437675\n",
      "unknown 7831 with attr -0.009134389562859724\n",
      "unknown 5804 with attr -0.013965484222353552\n",
      "unknown 1747 with attr -0.009762119346532927\n",
      "unknown 7913 with attr -0.012339023232925683\n",
      "unknown 22257 with attr -0.007414371207801196\n",
      "unknown 32520 with attr -0.016671571551000852\n",
      "unknown 18193 with attr -0.013261100954764515\n",
      "unknown 24347 with attr -0.0065045992357681826\n",
      "unknown 16157 with attr -0.011161787535002077\n",
      "unknown 28457 with attr -0.010792451323595742\n",
      "unknown 1841 with attr -0.0071572935667063575\n",
      "unknown 22331 with attr -0.011890978936321517\n",
      "unknown 3901 with attr -0.006199716004061884\n",
      "unknown 22342 with attr -0.01298939757680273\n",
      "unknown 10063 with attr -0.007461147328513107\n",
      "unknown 12133 with attr -0.006802125586546026\n",
      "unknown 30579 with attr -0.010818241202059653\n",
      "unknown 30581 with attr -0.011055106613184762\n",
      "unknown 12169 with attr -0.00801742154931162\n",
      "unknown 12192 with attr -0.015830506024940405\n",
      "unknown 24490 with attr -0.009734261439007241\n",
      "unknown 18358 with attr -0.01017975765535084\n",
      "unknown 28613 with attr -0.011815485472197906\n",
      "unknown 12230 with attr -0.008391123122919453\n",
      "unknown 30666 with attr -0.008407235308141026\n",
      "unknown 12244 with attr -0.010579588298924136\n",
      "unknown 28647 with attr -0.006654217308096122\n",
      "unknown 22516 with attr -0.019402021218184018\n",
      "unknown 26618 with attr -0.00804275281552691\n",
      "27758 E\n",
      "  attr -0.1747781486083113\n",
      "26824 D\n",
      "  attr -0.16980820387470885\n",
      "25771 T\n",
      "  attr -0.1365963689131604\n",
      "11839 S\n",
      "  attr -0.12674832851189421\n",
      "6146 M MOREDATA: M  also fires on final token of M words\n",
      "  attr -0.09822651862123166\n",
      "31021 A\n",
      "  attr -0.09784678883806919\n",
      "17259 L\n",
      "  attr -0.09739969908878265\n",
      "11431 X  and less so, Y  and roman numerals\n",
      "  attr -0.09644712582303328\n",
      "22790 B\n",
      "  attr -0.09131500174771645\n",
      "16138 G\n",
      "  attr -0.08049628931257757\n",
      "1349 D\n",
      "  attr -0.07883304957431392\n",
      "28979 B\n",
      "  attr -0.07274102425435558\n",
      "29892 A\n",
      "  attr -0.0719999106204341\n",
      "8113 N\n",
      "  attr -0.07060992660626653\n",
      "28222 T\n",
      "  attr -0.06432983783452073\n",
      "25933 R  and sometimes P, but mostly R\n",
      "  attr -0.06427513889138936\n",
      "8935 R\n",
      "  attr -0.062377776339417323\n",
      "9772 Scottish/Irish related terms\n",
      "  attr -0.061268554543858045\n",
      "6401 Q\n",
      "  attr -0.06049906851876585\n",
      "7440 V\n",
      "  attr -0.05792993160139304\n",
      "2923 U\n",
      "  attr -0.05607139528365224\n",
      "2380 W\n",
      "  attr -0.04605771117894619\n",
      "22801 M\n",
      "  attr -0.045623418538525584\n",
      "21206 Kidney function terms\n",
      "  attr -0.044486247787062894\n",
      "7436 R\n",
      "  attr -0.042790833933395334\n",
      "2344 N\n",
      "  attr -0.042718175682239234\n",
      "7210 Q\n",
      "  attr -0.03905940388722229\n",
      "10252 R\n",
      "  attr -0.03757084527751431\n",
      "5218 Ireland/Scottish names\n",
      "  attr -0.036398897432263766\n",
      "30976 S\n",
      "  attr -0.03369896866388444\n",
      "8649 I\n",
      "  attr -0.033544364230692736\n",
      "12167 L\n",
      "  attr -0.033046428667148575\n",
      "28185 W\n",
      "  attr -0.03094202431384474\n",
      "26556 G/H\n",
      "  attr -0.030817382530585746\n",
      "3888 second/ii/double/february\n",
      "  attr -0.029396268168966344\n",
      "32344 X/crosses, also some y\n",
      "  attr -0.027355561614967883\n",
      "32240 L\n",
      "  attr -0.02720839104949846\n",
      "15921 P\n",
      "  attr -0.026116897555766627\n",
      "2713 repeating a letter  or sound  twice\n",
      "  attr -0.02596145577626885\n",
      "25903 J/Y\n",
      "  attr -0.02538938164434512\n",
      "24468 Sports Teams\n",
      "  attr -0.02388748936459706\n",
      "5151 names\n",
      "  attr -0.022999352184797317\n",
      "17920 I\n",
      "  attr -0.022858697826450225\n",
      "5441 three digit numbers and determined/resolved/finishes MOREDATA: code notation  previously three digit numbers and determined/resolved/finishes\n",
      "  attr -0.02206722709161113\n",
      "2404 salt related chemistry\n",
      "  attr -0.021208943013334647\n",
      "32037 flying\n",
      "  attr -0.021024397003202466\n",
      "6272 renal cancer terms/sports teams MOREDATA: Human Suffering  Cancer, Auschwitz, Guantanamo Bay, Divorce, Chernobyl  previously renal cancer terms/sports teams\n",
      "  attr -0.020549124503304483\n",
      "12592 maybe privacy/mind control related things\n",
      "  attr -0.020322264046171767\n",
      "6635 Locations  with hospitals\n",
      "  attr -0.019990712970866298\n",
      "2438 phone calls\n",
      "  attr -0.019688784060690523\n",
      "14934 Mostly punctuation\n",
      "  attr -0.019670318491989747\n",
      "22516 ?\n",
      "  attr -0.019402021218184018\n",
      "5728 ?\n",
      "  attr -0.01917508570750215\n",
      "27638 ?\n",
      "  attr -0.019120284184737102\n",
      "7153 traversing building words  upstairs, downstairs, browsing, directories, corridors, etc.\n",
      "  attr -0.019046447800519672\n",
      "1444 E\n",
      "  attr -0.017846565402578562\n",
      "1312 account/login terminology and oxidation/breathing\n",
      "  attr -0.017313731845206348\n",
      "28510 superhero terms  and cellular signaling\n",
      "  attr -0.017119744939918746\n",
      "22605 Repeated Token\n",
      "  attr -0.016981449274680926\n",
      "32520 ?\n",
      "  attr -0.016671571551000852\n",
      "6674 Foreign language words  mostly russian but also japaneese\n",
      "  attr -0.016651658356181542\n",
      "23657 Michigan cities, words, and teams\n",
      "  attr -0.016552651359234005\n",
      "8556 ?\n",
      "  attr -0.01649465186710586\n",
      "21149 Javascript programming\n",
      "  attr -0.016306967873333633\n",
      "25124 J/Y\n",
      "  attr -0.016267280905594816\n",
      "9187 T\n",
      "  attr -0.016009828459573328\n",
      "8600 Name words like brother, mate, master, accountant\n",
      "  attr -0.015843032270822732\n",
      "12192 ?\n",
      "  attr -0.015830506024940405\n",
      "28977 East Coast News and Sports MOREDATA: East Coast News and Sports  mostly New Jersey\n",
      "  attr -0.01579690919766108\n",
      "15562 all caps hurricaine/military codename stuff\n",
      "  attr -0.015350925427355833\n",
      "23771 vegetarian/eco firendly related stuff, maybe\n",
      "  attr -0.014947573205859044\n",
      "5066 programming languages/IEEE\n",
      "  attr -0.014873993666242313\n",
      "2370 wine\n",
      "  attr -0.014538607297254202\n",
      "26276 tickets/permits/coupons/booking also signaling molocules\n",
      "  attr -0.014432658348027871\n",
      "16022 hunger/appetite/chewing\n",
      "  attr -0.014324363094146975\n",
      "5804 ?\n",
      "  attr -0.013965484222353552\n",
      "15421 rotating objects\n",
      "  attr -0.013939963814436851\n",
      "23218 Cities in Midwestern United States  mostly\n",
      "  attr -0.013883766508342887\n",
      "4214 quantum terminology\n",
      "  attr -0.013722597336709441\n",
      "24649 1800's and 1900's MOREDATA: used to be 1800's and 1900's\n",
      "  attr -0.013689302893567401\n",
      "6014 present tense verb\n",
      "  attr -0.013679897524980333\n",
      "11188 Fighter/Bomber Wings\n",
      "  attr -0.013412890531981247\n",
      "32334 ?\n",
      "  attr -0.013396876420983972\n",
      "9832 Popular Culture  mostly sports, but also honesty/lying related\n",
      "  attr -0.013338867276843303\n",
      "18193 ?\n",
      "  attr -0.013261100954764515\n",
      "21957 verbs\n",
      "  attr -0.013124387892958111\n",
      "22342 ?\n",
      "  attr -0.01298939757680273\n",
      "13907 ?\n",
      "  attr -0.012828784556631945\n",
      "8103 5/May/Five/Fifth\n",
      "  attr -0.01274918073158915\n",
      "9746 U\n",
      "  attr -0.0126178362279461\n",
      "30568 1980's-1990s pop culture\n",
      "  attr -0.012585221676090441\n",
      "23490 lipids/cholesterol\n",
      "  attr -0.012551901419328715\n",
      "593 ?\n",
      "  attr -0.012529574942618638\n",
      "27600 spooky\n",
      "  attr -0.012488319768635847\n",
      "29260 things that people are  player, footballer, rapper, astronaut\n",
      "  attr -0.012476729858008184\n",
      "7590 ?\n",
      "  attr -0.012472123598854523\n",
      "7913 ?\n",
      "  attr -0.012339023232925683\n",
      "9636 ?\n",
      "  attr -0.01230801921337843\n",
      "5761 ?\n",
      "  attr -0.012286140343690022\n",
      "17015 names\n",
      "  attr -0.012270050730080584\n",
      "4516 z\n",
      "  attr -0.012184076008452394\n",
      "22331 ?\n",
      "  attr -0.011890978936321517\n",
      "28613 ?\n",
      "  attr -0.011815485472197906\n",
      "9660 clarifiers  seperately, equally, details, correspondingly, specifics, jointly\n",
      "  attr -0.011788112475187518\n",
      "9370 right leaning or corporate news terms  Scripture, MSM, aired, shareholders, investors, advertise\n",
      "  attr -0.011689641587963706\n",
      "10175 sir/mate/bro also sports teams\n",
      "  attr -0.011609393237677068\n",
      "1021 ?\n",
      "  attr -0.011518310732981263\n",
      "23031 Three or two letter acronyms\n",
      "  attr -0.01146313421531886\n",
      "21909 second to last token in word\n",
      "  attr -0.011395784979413293\n",
      "13434 negotiations  advocate, persuade, marketing, lobbying\n",
      "  attr -0.011237540364618326\n",
      "25010 quotations about sports teams and celebrities\n",
      "  attr -0.011194733123375045\n",
      "16157 ?\n",
      "  attr -0.011161787535002077\n",
      "23671 German words\n",
      "  attr -0.011143825632643711\n",
      "29039 Wine and opiod related chemical properties\n",
      "  attr -0.01113584420522784\n",
      "19119 patents/invention/innovation\n",
      "  attr -0.011078803682721627\n",
      "30581 ?\n",
      "  attr -0.011055106613184762\n",
      "8860 Years  0-2000\n",
      "  attr -0.010908834899737485\n",
      "2129 MMA Wrestling and Sports MOREDATA: Michigan Locations  previously MMA Wrestling and Sports\n",
      "  attr -0.01086678510546335\n",
      "30579 ?\n",
      "  attr -0.010818241202059653\n",
      "28457 ?\n",
      "  attr -0.010792451323595742\n",
      "27463 Third/III/3\n",
      "  attr -0.01072121935976611\n",
      "19724 aggressive/vigorously\n",
      "  attr -0.01069634652901641\n",
      "27844 Mostly duplicated phrases, but also some short phrases\n",
      "  attr -0.010690764611354098\n",
      "2480 past tense verbs\n",
      "  attr -0.010681397359803668\n",
      "12244 ?\n",
      "  attr -0.010579588298924136\n",
      "17547 mate/sir/bro/dude/buddy\n",
      "  attr -0.010553130107382458\n",
      "6539 ?\n",
      "  attr -0.010329634168328994\n",
      "12348 actor portrayal, embodied, playing MOREDATA: Famous Actors, actor portrayal, embodied, playing\n",
      "  attr -0.010314146090422582\n",
      "13212 liver disease/research on liver disease, or something about airlines\n",
      "  attr -0.010285173077136278\n",
      "15551 Years  0-2000\n",
      "  attr -0.010188623953581555\n",
      "29977 Late 1800's through 1990's, and notable figures of that time\n",
      "  attr -0.010183288915641242\n",
      "18358 ?\n",
      "  attr -0.01017975765535084\n",
      "7207 double L and also isotopes\n",
      "  attr -0.010156668427498516\n",
      "2765 sqrt, also\n",
      "  attr -0.01010312067887753\n",
      "18809 Carbon isotopes\n",
      "  attr -0.010045149520010455\n",
      "20736 driving/cars\n",
      "  attr -0.009892453394513723\n",
      "30887 wine tasting MOREDATA: denominator of/simplify sqrt , previouslywine tasting\n",
      "  attr -0.009843360778631904\n",
      "25867 Pop culture terms and names\n",
      "  attr -0.009792545894811155\n",
      "16512 marriage and health  microbiome, antioxidants  related terms MOREDATA: , accountants, marriage and health  microbiome, antioxidants  related terms\n",
      "  attr -0.00977941253188419\n",
      "1747 ?\n",
      "  attr -0.009762119346532927\n",
      "24490 ?\n",
      "  attr -0.009734261439007241\n",
      "29937 programming related initialization/zero terms\n",
      "  attr -0.009706658512072863\n",
      "9371 E\n",
      "  attr -0.00970587123902078\n",
      "11918 Q\n",
      "  attr -0.009669136925367638\n",
      "29281 ?\n",
      "  attr -0.009626295634461712\n",
      "24846 Contains an e\n",
      "  attr -0.009616837047587978\n",
      "13336 relations of people  parents, families, mothers, investors, parents, farmers\n",
      "  attr -0.009604730643332005\n",
      "12210 probably greek symbols\n",
      "  attr -0.00959057461568591\n",
      "10143 sir/mate/bro/happy faces, also programming terms\n",
      "  attr -0.009580517507856712\n",
      "19977 Judicial proceedings\n",
      "  attr -0.009534094005857696\n",
      "20651 socialist countries/people, and escaped quotes MOREDATA: Russia related terms  previously socialist countries/people, and escaped quotes\n",
      "  attr -0.00942766465755085\n",
      "27326 occupations\n",
      "  attr -0.009404679599924748\n",
      "28503 web development / some medical terms\n",
      "  attr -0.009363260717464073\n",
      "14819 about names\n",
      "  attr -0.009356041136925342\n",
      "23448 probably the * symbol  Used for censoring maybe\n",
      "  attr -0.009247681292663401\n",
      "3765 sir/mate/bro/buddy/dude/bretheren\n",
      "  attr -0.009207125831153462\n",
      "23646 assay chemical terms\n",
      "  attr -0.009188498152070679\n",
      "4104 May, five, fifth\n",
      "  attr -0.009138316909229616\n",
      "7831 ?\n",
      "  attr -0.009134389562859724\n",
      "15582 1800-1999\n",
      "  attr -0.009123188330249832\n",
      "28482 Canada related terms and names\n",
      "  attr -0.009112429528158827\n",
      "32156 News MOREDATA: Some bands  News\n",
      "  attr -0.009065301725968311\n",
      "19210 oh my gosh but also some other stuff idk\n",
      "  attr -0.009025092790352574\n",
      "20115 ?\n",
      "  attr -0.008976714616437675\n",
      "3214 v\n",
      "  attr -0.008973046828941733\n",
      "16563 E and D words  mostly E  MOREDATA: E and D words  mostly E, some F , token contains an e sound\n",
      "  attr -0.008854826909555413\n",
      "9622 cancer treatment related terminology and wording: radiation related wording, chemotheraphy drugs, opiods\n",
      "  attr -0.008812440353722195\n",
      "11822 ?\n",
      "  attr -0.008759976565102079\n",
      "25129 vacation/holidy trip terms\n",
      "  attr -0.008725606308871647\n",
      "6227 3 digit numbers, renal excretions, and escaped quotes MOREDATA: Russian Names, 3 digit numbers, renal excretions, and escaped quotes\n",
      "  attr -0.008696446947169534\n",
      "20604 Kidney Function\n",
      "  attr -0.008669543757378051\n",
      "29391 circle related terms\n",
      "  attr -0.008665878058536691\n",
      "9856 quantum resonance/oscillation/vibration related terms\n",
      "  attr -0.008641311014798703\n",
      "819 ?\n",
      "  attr -0.008638471323365593\n",
      "21851 L\n",
      "  attr -0.008611365316937736\n",
      "11644 Relatives  dad, mom, grandfather, cousin\n",
      "  attr -0.008568098613068287\n",
      "30041 cryptography terms\n",
      "  attr -0.008561787899452611\n",
      "31883 FORMAL LEGAL LANGUAGE IN ALL CAPS\n",
      "  attr -0.008560251996527768\n",
      "21333 second to last token on a word, maybe\n",
      "  attr -0.008520465583387704\n",
      "25327 ?\n",
      "  attr -0.008484647296427283\n",
      "20740 Japaneese or Chineese Terms MOREDATA: Designations of Military Units  previously, Japaneese or Chineese Terms\n",
      "  attr -0.008437638861096275\n",
      "17165 mostly all caps locations or military code names  Troipical storm names  Not sure\n",
      "  attr -0.008432961922153481\n",
      "32518 Mostly Tex related symbols\n",
      "  attr -0.008426743361269473\n",
      "23254 trademark/invention/lawyer related terms\n",
      "  attr -0.008407365721154747\n",
      "30666 ?\n",
      "  attr -0.008407235308141026\n",
      "12230 ?\n",
      "  attr -0.008391123122919453\n",
      "30460 names\n",
      "  attr -0.00836467627982529\n",
      "710 ?\n",
      "  attr -0.00834142514031555\n",
      "30573 chemistry terms\n",
      "  attr -0.008322611262428836\n",
      "14506 all caps licensing/warranty terms MOREDATA: words put together with no space  prevoiusly all caps licensing/warranty terms\n",
      "  attr -0.008297496336808763\n",
      "12371 Middle East Diplomacy  also haemostatic/thrombosis/punk rock/hematopoiesis\n",
      "  attr -0.008280414368300626\n",
      "3481 Y/J and related sounds\n",
      "  attr -0.008252387309767073\n",
      "8520 Notable events in 1850-1950\n",
      "  attr -0.00819831061903642\n",
      "5290 Names MOREDATA: Names, mostly fictional\n",
      "  attr -0.008184253030549371\n",
      "8885 Third/III/3/March  also 4 a little\n",
      "  attr -0.00814137204997678\n",
      "8607 Russian symbols\n",
      "  attr -0.008129017246574222\n",
      "12365 ?\n",
      "  attr -0.008066546563895827\n",
      "27417 maybe contains c and/or h\n",
      "  attr -0.008059936456447758\n",
      "26618 ?\n",
      "  attr -0.00804275281552691\n",
      "12169 ?\n",
      "  attr -0.00801742154931162\n",
      "29875 some \"musings on life/thoughts on life/up to date on\" sorts of phrases\n",
      "  attr -0.00798489862995666\n",
      "9911 marajuana\n",
      "  attr -0.00798462220723195\n",
      "21217 Tribalism/Polarising topics\n",
      "  attr -0.007970399494183766\n",
      "25584 Pop culture  polarizing topics   MOREDATA: Pop culture  polarizing topics    maybe duplicated letters\n",
      "  attr -0.00792502611375312\n",
      "5334 sign related terminology  art stuff about how they are made, what put on, when used, etc.\n",
      "  attr -0.007916668177472275\n",
      "25307 Sports teams and drugs\n",
      "  attr -0.00787552531255642\n",
      "8713 ?\n",
      "  attr -0.007830667949065173\n",
      "31774 ?\n",
      "  attr -0.007802040568549273\n",
      "17207 ?\n",
      "  attr -0.00777073866356659\n",
      "19013 cybersecurity/security terms\n",
      "  attr -0.00771606135890579\n",
      "28237 ?\n",
      "  attr -0.007709657324539876\n",
      "23385 HTTP terms\n",
      "  attr -0.007708824877681764\n",
      "31304 proportion/percentage, acres, miles\n",
      "  attr -0.007669238376593057\n",
      "20217 L\n",
      "  attr -0.007627729094110691\n",
      "19868 Probably copyright code comment related\n",
      "  attr -0.007609032425534679\n",
      "29676 war related years and entities\n",
      "  attr -0.00757064356707815\n",
      "6667 Medical surgery terms\n",
      "  attr -0.00754842196147365\n",
      "9152 ends in ing  and less so ed\n",
      "  attr -0.0075197575945367134\n",
      "9793 Tourism/Museum related terms\n",
      "  attr -0.007516761312444942\n",
      "10063 ?\n",
      "  attr -0.007461147328513107\n",
      "20764 measuring methods/instruments\n",
      "  attr -0.007428856332012401\n",
      "22257 ?\n",
      "  attr -0.007414371207801196\n",
      "28108 ?\n",
      "  attr -0.007399879723379854\n",
      "29024 common denominator of\n",
      "  attr -0.007396902664368099\n",
      "2836 contains subword within the word/phrase\n",
      "  attr -0.007383605534982962\n",
      "23306 court, protest and voting related news terms\n",
      "  attr -0.007382435433100909\n",
      "6362 Communism Related Terms MOREDATA: Communism Related Terms and locations\n",
      "  attr -0.007367437463699389\n",
      "5164 Two letter acronyms\n",
      "  attr -0.007334597855788161\n",
      "21593 blood related medical terminology\n",
      "  attr -0.007324386707068697\n",
      "30215 ?\n",
      "  attr -0.0073036741368923686\n",
      "25299 Sports teams\n",
      "  attr -0.00725484013401001\n",
      "11527 wine and vine related terms\n",
      "  attr -0.007252043727021373\n",
      "16606 he/she/hes/hers/theirs pronoun\n",
      "  attr -0.007248282881562318\n",
      "28428 Symbols\n",
      "  attr -0.0072473992016171\n",
      "25316 ?\n",
      "  attr -0.007214687834846245\n",
      "3099 ?\n",
      "  attr -0.007184532264091104\n",
      "1841 ?\n",
      "  attr -0.0071572935667063575\n",
      "1925 Sports teams\n",
      "  attr -0.007098740324522623\n",
      "22784 Compound nouns  two or more words\n",
      "  attr -0.007089612839081383\n",
      "29400 partly/largely/portion\n",
      "  attr -0.007081040903358371\n",
      "1231 maybe contains symbols\n",
      "  attr -0.007072995650787561\n",
      "26388 portray/actors/embodied  and names of actors\n",
      "  attr -0.007027864370456882\n",
      "21510 ?\n",
      "  attr -0.006987265755071803\n",
      "29448 Cancer research cell lines\n",
      "  attr -0.0069788733271707315\n",
      "1560 Pop culture News\n",
      "  attr -0.006976166669574013\n",
      "14726 ?\n",
      "  attr -0.006957205181606696\n",
      "5333 ?\n",
      "  attr -0.006951492146072269\n",
      "27951 Z\n",
      "  attr -0.006935771954204029\n",
      "8401 Opening Nested Brackets MOREDATA: Opening Nested Brackets\n",
      "  attr -0.006932958481002061\n",
      "4059 poem/verse/lyrics/musicians/playwright\n",
      "  attr -0.006929941609996604\n",
      "21271 Airline Brands and Punk Rock bands\n",
      "  attr -0.0069216288079587684\n",
      "31070 marriage related terms\n",
      "  attr -0.006917106930359296\n",
      "4463 german ends of words, and punk bands  Linkin Park, Slipknot, etc.\n",
      "  attr -0.006903969514269193\n",
      "1593 inflation\n",
      "  attr -0.006902257244973953\n",
      "19851 Natural Disasters and also mate/sir/dude/bro/sis\n",
      "  attr -0.006872525849189515\n",
      "15762 Q\n",
      "  attr -0.006868912230402202\n",
      "25370 maybe ends in a vowel\n",
      "  attr -0.006850492354715243\n",
      "12133 ?\n",
      "  attr -0.006802125586546026\n",
      "21471 names\n",
      "  attr -0.00676922697948612\n",
      "19433 genomics terms\n",
      "  attr -0.006696107015386588\n",
      "29495 some name related, likes Mc\n",
      "  attr -0.0066899425416977465\n",
      "19710 names\n",
      "  attr -0.006678973795146703\n",
      "29913 notification messages  coming soon/screen name, syntax error on, /**, some german words\n",
      "  attr -0.006672907451047649\n",
      "28647 ?\n",
      "  attr -0.006654217308096122\n",
      "12712 Math symbols\n",
      "  attr -0.006609869964904647\n",
      "2165 Muilti token phrases maybe\n",
      "  attr -0.00660846277685323\n",
      "24327 First/primary/january/1  also lesser, second\n",
      "  attr -0.006580403307452798\n",
      "1525 Judiciary news  packing, unconstitutional, sued, judiciary\n",
      "  attr -0.006557001974215382\n",
      "16598 Something about names\n",
      "  attr -0.0065409953317612235\n",
      "16927 Canada terms\n",
      "  attr -0.006526016593852546\n",
      "9474 second/ii/secondly and also seperately/supposedly related   MOREDATA: there's no doubt/denying and other stuff , previously second/ii/secondly and also seperately/supposedly related\n",
      "  attr -0.006523636489873752\n",
      "11238 cell signaling\n",
      "  attr -0.006507405890260998\n",
      "24347 ?\n",
      "  attr -0.0065045992357681826\n",
      "19260 sports teams, some medical terms  Not clearly double letters\n",
      "  attr -0.006481513251628712\n",
      "31591 cell biology\n",
      "  attr -0.00647655470220343\n",
      "7315 tempest all caps related stuff\n",
      "  attr -0.0064665828858778696\n",
      "2517 denominator of\n",
      "  attr -0.006439173310184287\n",
      "7976 V\n",
      "  attr -0.006432281166325993\n",
      "15112 maybe double letters\n",
      "  attr -0.006430887430042276\n",
      "31773 possibly duplicated letters  Or having the word 2 or maybe typos  Not sure\n",
      "  attr -0.006422075800344373\n",
      "17257 maybe partial phrase activation\n",
      "  attr -0.006392213238541444\n",
      "24408 Pop culture from 1970s\n",
      "  attr -0.006387471017660573\n",
      "9070 smell and taste related terms  also mitochondria\n",
      "  attr -0.006382021861895737\n",
      "9344 College Admission Terms  SAT, admissions, graduate, GRE\n",
      "  attr -0.006360145394864958\n",
      "17703 irish/scottish\n",
      "  attr -0.006345220279399655\n",
      "1582 ?\n",
      "  attr -0.006295407222978611\n",
      "26187 quiz/assignments/exams, also nudity/uncovered/covered\n",
      "  attr -0.00627296765878782\n",
      "16702 dude/buddy/bro/mate/sir MOREDATA: dude/buddy/bro/mate/sir\n",
      "  attr -0.006263825359837938\n",
      "20089 driving/cars\n",
      "  attr -0.006260821712203324\n",
      "17010 ?\n",
      "  attr -0.006254309687449222\n",
      "30030 contacting someone  call, telephone, email\n",
      "  attr -0.006253717307572515\n",
      "6351 Cellular Signaling Pathways MOREDATA: Russian Names  previously Cellular Signaling Pathways\n",
      "  attr -0.006245518262403493\n",
      "6266 interactions with webpage MOREDATA: me/you/your  previously interactions with webpage\n",
      "  attr -0.006233607682247566\n",
      "32395 [ with some other symbols  [\", **[, etc.\n",
      "  attr -0.006224751563763675\n",
      "31017 ?\n",
      "  attr -0.006213662440359258\n",
      "14859 Medical terms related to kidney function\n",
      "  attr -0.00621000153478235\n",
      "27074 ?\n",
      "  attr -0.00620498292278171\n",
      "3901 ?\n",
      "  attr -0.006199716004061884\n",
      "23692 mathematical symbols\n",
      "  attr -0.006188363698811372\n",
      "5740 marijuana\n",
      "  attr -0.006186059359151841\n",
      "3563 ?\n",
      "  attr -0.006175605278485818\n",
      "27945 prefixes of scientific terms\n",
      "  attr -0.006160893262517675\n",
      "1429 Latex symbols and 3-digit numbers\n",
      "  attr -0.006139959426946007\n",
      "29097 Genocide and terrorism related countries, people, and organizations\n",
      "  attr -0.006131060118264031\n",
      "17946 web browser related tech terms\n",
      "  attr -0.006128787816805925\n",
      "422 ?\n",
      "  attr -0.006121505353689827\n",
      "1993 Sports Teams\n",
      "  attr -0.006114820432912893\n",
      "total num features identified 307\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "combined = {}\n",
    "for f in feature_labels.keys():\n",
    "    if f in feature_labelsb:\n",
    "        combined[f] = feature_labels[f] + \" MOREDATA: \" + feature_labelsb[f].replace(\"?\", \" \").strip()\n",
    "    else:\n",
    "        combined[f] = feature_labels[f]\n",
    "    \n",
    "    if f in feature_labelsb and feature_labelsb[f].strip()  == feature_labels[f].strip():\n",
    "        combined[f] = feature_labels[f]\n",
    "    \n",
    "    if feature_labels[f].replace(\"?\", \"\").strip() == \"\" and f in feature_labelsb:\n",
    "        combined[f] = feature_labelsb[f]\n",
    "    combined[f] = combined[f].replace(\"?\", \" \").replace(\"(\", \" \").replace(\")\", \" \").strip()\n",
    "    if combined[f].strip() == \"\" or combined[f].strip() == '||':\n",
    "        del combined[f]\n",
    "labs = sorted(list(combined.items()), key=lambda x: x[1].lower().strip())\n",
    "for f,l in labs:\n",
    "    #if not '?' in l:\n",
    "    if l.strip() != \"\":\n",
    "        print(f, l)\n",
    "        print(f\"  attr {features_sorted_by_feat_i[f][0].attr}\")\n",
    "for f in feature_labels.keys():\n",
    "    if len([(f2,l) for (f2,l) in labs if f2 == f]) == 0:\n",
    "        print(f\"unknown {f} with attr {features_sorted_by_feat_i[f][0].attr}\")\n",
    "        combined[f] = feature_labels[f]\n",
    "labs = sorted(list(combined.items()), key=lambda x: features_sorted_by_feat_i[x[0]][0].attr)\n",
    "for f,l in labs:\n",
    "    #if not '?' in l:\n",
    "    if l.strip() != \"\":\n",
    "        print(f, l)\n",
    "        print(f\"  attr {features_sorted_by_feat_i[f][0].attr}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"total num features identified {len(labs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e2f5262-ec13-4c97-a16d-1368f35319c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6146: 'M (also fires on final token of M words)',\n",
       " 4104: 'May, five, fifth ',\n",
       " 10252: 'R ',\n",
       " 12348: 'Famous Actors, actor portrayal, embodied, playing',\n",
       " 24649: \"? used to be 1800's and 1900's\",\n",
       " 22605: 'Repeated Token'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_labelsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e63e1a39-917a-4f2c-84e6-19de4c31ddaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Purushottam'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" Purushottam\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0176123f-744f-4217-a337-7e0c83911678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
