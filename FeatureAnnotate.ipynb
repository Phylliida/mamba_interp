{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044af21d-390b-4473-b01f-76ea4d29f181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "1\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry190.txtblocks.1.hook_out_proj/hook_blocks.1.hook_out_proj.pt\n",
      "2\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry191.txtblocks.2.hook_out_proj/hook_blocks.2.hook_out_proj.pt\n",
      "3\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry192.txtblocks.3.hook_out_proj/hook_blocks.3.hook_out_proj.pt\n",
      "4\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry193.txtblocks.4.hook_out_proj/hook_blocks.4.hook_out_proj.pt\n",
      "5\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry194.txtblocks.5.hook_out_proj/hook_blocks.5.hook_out_proj.pt\n",
      "6\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry195.txtblocks.6.hook_out_proj/hook_blocks.6.hook_out_proj.pt\n",
      "7\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry196.txtblocks.7.hook_out_proj/hook_blocks.7.hook_out_proj.pt\n",
      "8\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry200.txtblocks.8.hook_out_proj/hook_blocks.8.hook_out_proj.pt\n",
      "9\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry201.txtblocks.9.hook_out_proj/hook_blocks.9.hook_out_proj.pt\n",
      "10\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry202.txtblocks.10.hook_out_proj/hook_blocks.10.hook_out_proj.pt\n",
      "11\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry203.txtblocks.11.hook_out_proj/hook_blocks.11.hook_out_proj.pt\n",
      "12\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry204.txtblocks.12.hook_out_proj/hook_blocks.12.hook_out_proj.pt\n",
      "13\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry205.txtblocks.13.hook_out_proj/hook_blocks.13.hook_out_proj.pt\n",
      "14\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry206.txtblocks.14.hook_out_proj/hook_blocks.14.hook_out_proj.pt\n",
      "15\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry210.txtblocks.15.hook_out_proj/hook_blocks.15.hook_out_proj.pt\n",
      "16\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry211.txtblocks.16.hook_out_proj/hook_blocks.16.hook_out_proj.pt\n",
      "17\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry212.txtblocks.17.hook_out_proj/hook_blocks.17.hook_out_proj.pt\n",
      "18\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry213.txtblocks.18.hook_out_proj/hook_blocks.18.hook_out_proj.pt\n",
      "19\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry214.txtblocks.19.hook_out_proj/hook_blocks.19.hook_out_proj.pt\n",
      "20\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry215.txtblocks.20.hook_out_proj/hook_blocks.20.hook_out_proj.pt\n",
      "21\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry216.txtblocks.21.hook_out_proj/hook_blocks.21.hook_out_proj.pt\n",
      "using patching format\n",
      "ABC AB C\n",
      "ABC AC B\n",
      "\n",
      "ABC AB C\n",
      "ABC CB A\n",
      "\n",
      "ABC AB C\n",
      "ABD AB D\n",
      "\n",
      "ABC AC B\n",
      "ABC BC A\n",
      "\n",
      "ABC AC B\n",
      "ADC AC D\n",
      "\n",
      "ABC BA C\n",
      "ABC BC A\n",
      "\n",
      "ABC BA C\n",
      "ABC CA B\n",
      "\n",
      "ABC BA C\n",
      "ABD BA D\n",
      "\n",
      "ABC BC A\n",
      "DBC BC D\n",
      "\n",
      "ABC CA B\n",
      "ABC CB A\n",
      "\n",
      "ABC CA B\n",
      "ADC CA D\n",
      "\n",
      "ABC CB A\n",
      "DBC CB D\n",
      "\n",
      "using templates\n",
      "Then, [NAME], [NAME] and [NAME] went to the [PLACE]. [NAME] and [NAME] gave a [OBJECT] to\n",
      "with name positions (2, 4, 6, 12, 14)\n",
      "removed jesus\n",
      "['<|endoftext|>', 'Then', ',', ' Olivia', ',', ' Ian', ' and', ' Aaron', ' went', ' to', ' the', ' restaurant', '.', ' Aaron', ' and', ' Olivia', ' gave', ' a', ' computer', ' to']\n",
      "['<|endoftext|>', 'Then', ',', ' Olivia', ',', ' Ian', ' and', ' Aaron', ' went', ' to', ' the', ' restaurant', '.', ' Aaron', ' and', ' Ian', ' gave', ' a', ' computer', ' to']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", font_size=None, show=True, color_continuous_midpoint=0.0, **kwargs):\n",
    "    import plotly.express as px\n",
    "    import transformer_lens.utils as utils\n",
    "    fig = px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=color_continuous_midpoint, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs)\n",
    "    if not font_size is None:\n",
    "        if 'x' in kwargs:\n",
    "            fig.update_layout(\n",
    "              xaxis = dict(\n",
    "                tickmode='array',\n",
    "                tickvals = kwargs['x'],\n",
    "                ticktext = kwargs['x'], \n",
    "                ),\n",
    "               font=dict(size=font_size, color=\"black\"))\n",
    "        if 'y' in kwargs:\n",
    "            fig.update_layout(\n",
    "              yaxis = dict(\n",
    "                tickmode='array',\n",
    "                tickvals = kwargs['y'],\n",
    "                ticktext = kwargs['y'], \n",
    "                ),\n",
    "               font=dict(size=font_size, color=\"black\"))\n",
    "    plot_args = {\n",
    "        'width': 800,\n",
    "        'height': 600,\n",
    "        \"autosize\": False,\n",
    "        'showlegend': True,\n",
    "        'margin': {\"l\":0,\"r\":0,\"t\":100,\"b\":0}\n",
    "    }\n",
    "    \n",
    "    fig.update_layout(**plot_args)\n",
    "    fig.update_layout(legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    ))\n",
    "    if show:\n",
    "        fig.show(renderer)\n",
    "    else:\n",
    "        return fig\n",
    "\n",
    "\n",
    "# requires\n",
    "# pip install git+https://github.com/Phylliida/MambaLens.git\n",
    "\n",
    "from mamba_lens import HookedMamba # this will take a little while to import\n",
    "import torch\n",
    "model_path = \"state-spaces/mamba-370m\"\n",
    "\n",
    "# NOTE! We need to monkeypatch transformer lens to use register_full_backward_hook\n",
    "model = HookedMamba.from_pretrained(model_path, device='cuda')\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "if not \"/home/dev/sae-k-sparse-mamba/sae\" in sys.path:\n",
    "    sys.path.append(\"/home/dev/sae-k-sparse-mamba\")\n",
    "import os\n",
    "os.chdir('/home/dev/sae-k-sparse-mamba')\n",
    "saes = [None]\n",
    "from importlib import reload\n",
    "from sae.sae import Sae\n",
    "\n",
    "ckpt_dir = \"/home/dev/sae-k-sparse-mamba/\"\n",
    "for i in range(1,22):\n",
    "    print(i)\n",
    "    hook = f'blocks.{i}.hook_out_proj'\n",
    "    path = [ckpt_dir + f for f in sorted(list(os.listdir(ckpt_dir))) if hook in f][0] + \"/\" + f'hook_{hook}.pt'\n",
    "    #path = f'/home/dev/sae-k-sparse-mamba/blocks.{i}.hook_resid_pre/hook_blocks.{i}.hook_resid_pre.pt'\n",
    "    print(path)\n",
    "    #saes.append(Sae.load_from_disk(path, hook=f'blocks.{i}.hook_resid_pre', device=model.cfg.device))\n",
    "\n",
    "\n",
    "global PATCHING_FORMAT_I\n",
    "global patching_formats\n",
    "def make_data(num_patching_pairs, patching, template_i, seed, valid_seed):\n",
    "    constrain_to_answers = True\n",
    "    # this makes our data size 800, first 400 is each (a,b) pair, and then second 400 is each pair swapped to be (b,a)\n",
    "    has_symmetric_patching = True\n",
    "    \n",
    "    n1_patchings = [\"\"\"\n",
    "    ABC BC A\n",
    "    DBC BC D\"\"\",\n",
    "        \"\"\"\n",
    "    ABC CB A\n",
    "    DBC CB D\"\"\"]\n",
    "    \n",
    "    n2_patchings = [\"\"\"\n",
    "    ABC AC B\n",
    "    ADC AC D\"\"\",\n",
    "        \"\"\"\n",
    "    ABC CA B\n",
    "    ADC CA D\"\"\"]\n",
    "    \n",
    "    n3_patchings = [\"\"\"\n",
    "    ABC AB C\n",
    "    ABD AB D\"\"\",\n",
    "        \"\"\"\n",
    "    ABC BA C\n",
    "    ABD BA D\"\"\"]\n",
    "    \n",
    "    n4_patchings = [\"\"\"\n",
    "    ABC AC B\n",
    "    ABC BC A\"\"\",\n",
    "        \"\"\"\n",
    "    ABC AB C\n",
    "    ABC CB A\"\"\",\n",
    "        \"\"\"\n",
    "    ABC BA C\n",
    "    ABC CA B\"\"\"]\n",
    "    \n",
    "    n5_patchings = [\"\"\"\n",
    "    ABC CA B\n",
    "    ABC CB A\n",
    "    \"\"\",\n",
    "        \"\"\"\n",
    "    ABC BA C\n",
    "    ABC BC A\"\"\",\n",
    "        \"\"\"\n",
    "    ABC AB C\n",
    "    ABC AC B\"\"\"]\n",
    "    \n",
    "    patchings = {\n",
    "        'n1': n1_patchings,\n",
    "        'n2': n2_patchings,\n",
    "        'n3': n3_patchings,\n",
    "        'n4': n4_patchings,\n",
    "        'n5': n5_patchings\n",
    "    }\n",
    "    \n",
    "    all_patchings = []\n",
    "    for patching_set in patchings.values():\n",
    "        all_patchings += patching_set\n",
    "    all_patchings = sorted(all_patchings) # make deterministic \n",
    "    \n",
    "    patch_all_names = [\"\"\"\n",
    "    ABC AB C\n",
    "    DEF DE F\"\"\",\n",
    "        \"\"\"\n",
    "    ABC AC B\n",
    "    DEF DF E\"\"\",     \n",
    "        \"\"\"\n",
    "    ABC BA C\n",
    "    DEF ED F\"\"\",\n",
    "        \"\"\"\n",
    "    ABC BC A\n",
    "    DEF EF D\"\"\",\n",
    "        \"\"\"\n",
    "    ABC CA B\n",
    "    DEF FD E\"\"\",\n",
    "        \"\"\"\n",
    "    ABC CB A\n",
    "    DEF FE D\"\"\"]\n",
    "    \n",
    "    \n",
    "    patchings['all'] = all_patchings\n",
    "    patchings['allatonce'] = patch_all_names\n",
    "    from acdc.data.ioi import BABA_TEMPLATES, ABC_TEMPLATES\n",
    "    from acdc.data.ioi import ioi_data_generator, ABC_TEMPLATES, get_all_single_name_abc_patching_formats\n",
    "    from acdc.data.utils import generate_dataset\n",
    "    templates = ABC_TEMPLATES\n",
    "    #patching_formats = list(get_all_single_name_abc_patching_formats())\n",
    "    global PATCHING_FORMAT_I\n",
    "    global patching_formats\n",
    "    PATCHING_FORMAT_I = patching\n",
    "    patching_formats = [\"\\n\".join([line.strip() for line in x.split(\"\\n\")]).strip() for x in patchings[PATCHING_FORMAT_I]]\n",
    "    \n",
    "    print(\"using patching format\")\n",
    "    for patch in patching_formats:\n",
    "        print(patch)\n",
    "        print(\"\")\n",
    "    #print(patching_formats)\n",
    "    \n",
    "    \n",
    "    data = generate_dataset(model=model,\n",
    "                      data_generator=ioi_data_generator,\n",
    "                      num_patching_pairs=4,\n",
    "                      seed=seed,\n",
    "                      valid_seed=valid_seed,\n",
    "                      constrain_to_answers=constrain_to_answers,\n",
    "                      has_symmetric_patching=has_symmetric_patching, \n",
    "                      varying_data_lengths=True,\n",
    "                      templates=templates,\n",
    "                      patching_formats=patching_formats)\n",
    "    \n",
    "    \n",
    "    import acdc.data.ioi\n",
    "    from collections import defaultdict\n",
    "    name_positions_map = defaultdict(lambda: [])\n",
    "    for template in templates:\n",
    "        name = acdc.data.ioi.good_names[0]\n",
    "        template_filled_in = template.replace(\"[NAME]\", name)\n",
    "        template_filled_in = template_filled_in.replace(\"[PLACE]\", acdc.data.ioi.good_nouns['[PLACE]'][0])\n",
    "        template_filled_in = template_filled_in.replace(\"[OBJECT]\", acdc.data.ioi.good_nouns['[OBJECT]'][0])\n",
    "        # get the token positions of the [NAME] in the prompt\n",
    "        name_positions = tuple([(i) for (i,s) in enumerate(model.to_str_tokens(torch.tensor(model.tokenizer.encode(template_filled_in)))) if s == f' {name}'])\n",
    "        name_positions_map[name_positions].append(template)\n",
    "    sorted_by_frequency = sorted(list(name_positions_map.items()), key=lambda x: -len(x[1]))\n",
    "    most_frequent_name_positions, templates = sorted_by_frequency[0]\n",
    "    print(\"using templates\")\n",
    "    templates = [templates[0]]\n",
    "    for template in templates:\n",
    "        print(template)\n",
    "    print(f\"with name positions {most_frequent_name_positions}\")\n",
    "    import acdc.data.ioi\n",
    "    if 'Jesus' in acdc.data.ioi.good_names:\n",
    "        print(\"removed jesus\")\n",
    "        acdc.data.ioi.good_names.remove(\"Jesus\")\n",
    "    data = generate_dataset(model=model,\n",
    "                  data_generator=ioi_data_generator,\n",
    "                  num_patching_pairs=num_patching_pairs,\n",
    "                  seed=seed,\n",
    "                  valid_seed=valid_seed,\n",
    "                  constrain_to_answers=constrain_to_answers,\n",
    "                  has_symmetric_patching=has_symmetric_patching, \n",
    "                  varying_data_lengths=True,\n",
    "                  templates=templates,\n",
    "                  patching_formats=patching_formats)\n",
    "    \n",
    "    print(model.to_str_tokens(data.data[0]))\n",
    "    print(model.to_str_tokens(data.data[1]))\n",
    "    return data\n",
    "\n",
    "\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "# we do a hacky thing where this first hook clears the global storage\n",
    "# second hook stores all the hooks\n",
    "# then third hook computes the output (over all the hooks)\n",
    "# this avoids recomputing and so is much faster\n",
    "SAE_HOOKS = \"sae hooks\"\n",
    "SAE_BATCHES = \"sae batches\"\n",
    "SAE_OUTPUT = \"sae output\"\n",
    "def sae_patching_storage_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    sae_feature_i: int,\n",
    "    dummy: bool,\n",
    "    position: int,\n",
    "    layer: int,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    **kwargs,\n",
    "):\n",
    "    global sae_storage\n",
    "    if not SAE_HOOKS in sae_storage:\n",
    "        sae_storage[SAE_HOOKS] = [] # we can't do this above because it'll be emptied again on the next batch before this is called\n",
    "    sae_storage[SAE_OUTPUT] = None # clear output\n",
    "    sae_storage[SAE_HOOKS].append({\"position\": position, \"sae_feature_i\": sae_feature_i, \"dummy\": dummy})\n",
    "    #print(f\"sae feature i {sae_feature_i} position {position} layer {layer}\")\n",
    "    return x\n",
    "\n",
    "from jaxtyping import Float\n",
    "from einops import rearrange\n",
    "\n",
    "global sae_storage\n",
    "sae_storage = {}\n",
    "def sae_patching_hook(\n",
    "    x: Float[torch.Tensor, \"B L E\"],\n",
    "    hook: HookPoint,\n",
    "    input_hook_name: str,\n",
    "    layer: int,\n",
    "    **kwargs,\n",
    ") -> Float[torch.Tensor, \"B L E\"]:\n",
    "    global sae_storage\n",
    "    ### This is identical to what the conv is doing\n",
    "    # but we break it apart so we can patch on individual filters\n",
    "    # we have two input hooks, the second one is the one we want\n",
    "    input_hook_name = input_hook_name[1]\n",
    "    # don't recompute these if we don't need to\n",
    "    # because we stored all the hooks and batches in conv_storage, we can just do them all at once\n",
    "    \n",
    "    # they need to share an output because they write to the same output tensor\n",
    "    if sae_storage[SAE_OUTPUT] is None:\n",
    "        #print(f\"running for layer {layer}\")\n",
    "        K = saes[layer].cfg.k\n",
    "        sae = saes[layer]\n",
    "        #print(f\"layer {layer} storage {sae_storage}\")\n",
    "        sae_output = torch.zeros(x.size(), device=model.cfg.device)\n",
    "        #print(\"layer\", layer, \"keys\", conv_storage)\n",
    "        def get_filter_key(i):\n",
    "            return f'filter_{i}'\n",
    "        sae_input_uncorrupted = x[::2]\n",
    "        sae_input_corrupted = x[1::2]\n",
    "        B, L, D = sae_input_uncorrupted.size()\n",
    "        for l in range(L):\n",
    "            # [B, NFeatures]                             [B,D]\n",
    "            uncorrupted_features = sae.encode(sae_input_uncorrupted[:,l])\n",
    "            # [B, NFeatures]                             [B,D]\n",
    "            corrupted_features = sae.encode(sae_input_corrupted[:,l])\n",
    "            patched_features = corrupted_features.clone()\n",
    "            #patched_features = torch.zeros(corrupted_features.size(), device=model.cfg.device) # patch everything except the features we are keeping around\n",
    "            # apply hooks (one hook applies to a single feature)\n",
    "            #print(f\"{len(sae_storage[SAE_HOOKS])} hooks\")\n",
    "            for hook_data in sae_storage[SAE_HOOKS]:\n",
    "                position = hook_data['position']\n",
    "                sae_feature_i = hook_data['sae_feature_i']\n",
    "                dummy = hook_data['dummy']\n",
    "                if not dummy and (position == l or position is None): # position is None means all positions\n",
    "                    if copy_from_other:\n",
    "                        patched_features[:,sae_feature_i] = corrupted_features[:,sae_feature_i]\n",
    "                    else:\n",
    "                        patched_features[:,sae_feature_i] = uncorrupted_features[:,sae_feature_i]\n",
    "                    \n",
    "                    #print(f\"applying sae feature {sae_feature_i} to position {position} for layer {layer}\")\n",
    "                    #uncorrupted_features[:,sae_feature_i] = corrupted_features[:,sae_feature_i]\n",
    "            # compute sae outputs\n",
    "            patched_top_acts, patched_top_indices = patched_features.topk(K, sorted=False)\n",
    "            corrupted_top_acts, corrupted_top_indices = corrupted_features.topk(K, sorted=False)      \n",
    "            sae_output[::2,l] = sae.decode(patched_top_acts, patched_top_indices)     \n",
    "            sae_output[1::2,l] = sae.decode(corrupted_top_acts, corrupted_top_indices)\n",
    "        sae_storage = {} # clean up and prepare for next layer\n",
    "        sae_storage[SAE_OUTPUT] = sae_output # store the output\n",
    "    return sae_storage[SAE_OUTPUT]\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "@dataclass\n",
    "class SAEFeature:\n",
    "    \"\"\"Class for keeping track of an item in inventory.\"\"\"\n",
    "    layer: int\n",
    "    pos: int\n",
    "    feature_i: int\n",
    "    attr: float\n",
    "    records: list = field(default_factory=lambda: [])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.layer) + \" \" + str(self.pos) + \" \" + str(self.feature_i) + \" \" + str(self.attr)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "def get_name_counts(feature):\n",
    "    name_counts = {}\n",
    "    DATA_LEN = len(feature.records)\n",
    "    records_tensor = torch.tensor(feature.records)\n",
    "    non_zero_indices = torch.arange(DATA_LEN)[records_tensor!=0]\n",
    "    non_zero_tokens = data.data[non_zero_indices,feature.pos].cpu()\n",
    "    non_zero_records = records_tensor[non_zero_indices]\n",
    "    name_tokens = torch.unique(non_zero_tokens)\n",
    "    for name_token in name_tokens:\n",
    "        name_str = model.to_str_tokens(name_token.view(1,1))[0]\n",
    "        name_counts[name_str] = non_zero_records[non_zero_tokens==name_token.item()]\n",
    "    #for t,c in template_counts.items():\n",
    "    #    print(f\" template {t} with count {torch.mean(torch.tensor(c)).item()}\")\n",
    "    name_counts = sorted(list(name_counts.items()), key=lambda x: -torch.mean(x[1]).item())\n",
    "    return name_counts\n",
    "    #for n,c in name_counts[:100]:\n",
    "    #    print(f\" name {n} with avg {torch.mean(c).item()} min {torch.min(c).item()} max {torch.max(c).item()}\")\n",
    "\n",
    "data = make_data(num_patching_pairs=2, patching=\"all\", template_i=0, seed=24, valid_seed=23)\n",
    "\n",
    "toks = model.to_str_tokens(data.data[0])\n",
    "name_positions = [3,5,7,13,15]\n",
    "position_map = {}\n",
    "L = data.data.size()[1]\n",
    "for l in range(L):\n",
    "    position_map[l] = f'pos{l}{toks[l]}'\n",
    "position_map[3] = 'n1'\n",
    "position_map[5] = 'n2'\n",
    "position_map[7] = 'n3'\n",
    "position_map[13] = 'n4'\n",
    "position_map[15] = 'n5'\n",
    "position_map[19] = 'out'\n",
    "import pickle\n",
    "with open(\"cached_sae_feature_edges.pkl\", \"rb\") as f:\n",
    "    edges_to_keep = pickle.load(f)\n",
    "\n",
    "h = model.to_str_tokens(torch.arange(model.tokenizer.vocab_size))\n",
    "spaceThings = [(i, x) for (i, x) in enumerate(h) if x[0] == ' ' and len(x.strip()) > 0]\n",
    "prefix = data.data[0][:3].view(1,-1)\n",
    "new_data_toks = torch.tensor([tok for (tok,s) in spaceThings], device=model.cfg.device)\n",
    "data_for_all_tokens = torch.cat([prefix.repeat((len(new_data_toks),1)), new_data_toks.view(-1,1)], dim=1)\n",
    "data.data = data_for_all_tokens\n",
    "with open(\"layer_15_features_more_more.pkl\", \"rb\") as f:\n",
    "    data, features = pickle.load(f)\n",
    "\n",
    "'''\n",
    "#names = sorted(list(acdc.data.ioi.good_names))\n",
    "names = [x for (i,x) in spaceThings if len(x.strip()) > 0]\n",
    "NUM_NAMES = len(names)\n",
    "#name_to_i = dict([(\" \" + name, i) for (i, name) in enumerate(names)])\n",
    "name_to_i = dict([(name, i) for (i, name) in enumerate(names)])\n",
    "\n",
    "def get_name_vector(feature, feat_type):\n",
    "    name_vec = torch.zeros(NUM_NAMES)\n",
    "    for name,counts in get_name_counts(feature):\n",
    "        if feat_type == 'mean':\n",
    "            name_vec[name_to_i[name]] = torch.mean(counts)\n",
    "        elif feat_type == 'min':\n",
    "            name_vec[name_to_i[name]] = torch.min(counts)\n",
    "        elif feat_type == 'max':\n",
    "            name_vec[name_to_i[name]] = torch.max(counts)\n",
    "            \n",
    "    return name_vec\n",
    "'''\n",
    "features_sorted_by_feat_i = defaultdict(lambda: [])\n",
    "for feature in features:\n",
    "    features_sorted_by_feat_i[feature.feature_i].append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f277460c-cc4a-423c-b14d-20075b82f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "global feature_labels\n",
    "import pickle\n",
    "with open(\"layer_15_features.pkl\", \"rb\") as f:\n",
    "    feature_labels = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7877047-a855-4c7e-8c5d-2c6e01710ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "global feature_labels\n",
    "import pickle\n",
    "with open(\"layer_15_features_take_two.pkl\", \"rb\") as f:\n",
    "    feature_labelsb = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b939cea7-2201-4894-97d1-d551b83d8754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{6146: 'M (also fires on final token of M words)', 4104: 'May, five, fifth ', 10252: 'R ', 12348: 'Famous Actors, actor portrayal, embodied, playing', 24649: \"? used to be 1800's and 1900's\", 22605: 'Repeated Token', 12365: '? ', 2129: 'Michigan Locations (previously MMA Wrestling and Sports)', 12371: 'Middle East Diplomacy (also haemostatic/thrombosis/punk rock/hematopoiesis)', 6227: 'Russian Names, 3 digit numbers, renal excretions, and escaped quotes', 2165: '? Muilti token phrases maybe?', 4214: 'quantum terminology ', 6266: 'me/you/your (previously interactions with webpage)', 20604: 'Kidney Function', 16512: '?, accountants, marriage and health (microbiome, antioxidants) related terms', 6272: 'Human Suffering (Cancer, Auschwitz, Guantanamo Bay, Divorce, Chernobyl) previously renal cancer terms/sports teams', 30887: 'denominator of/simplify sqrt(, previouslywine tasting', 14506: 'words put together with no space (prevoiusly all caps licensing/warranty terms)', 20651: 'Russia related terms (previously socialist countries/people, and escaped quotes', 16563: 'E and D words (mostly E, some F), token contains an e sound', 26824: 'D ', 6351: 'Russian Names (previously Cellular Signaling Pathways)', 8401: '? Opening Nested Brackets', 16598: '? Something about names', 6362: 'Communism Related Terms and locations', 16606: 'he/she/hes/hers/theirs pronoun ', 30976: 'S ', 6401: 'Q ', 22784: 'Compound nouns (two or more words)', 20736: 'driving/cars ', 20740: 'Designations of Military Units (previously, Japaneese or Chineese Terms)', 22790: 'B ', 24846: '? Contains an e', 22801: 'M ', 20764: 'measuring methods/instruments ', 2344: 'N ', 31017: '? ', 31021: 'A ', 12592: '? (maybe privacy/mind control related things?)', 28977: 'East Coast News and Sports (mostly New Jersey)', 28979: 'B ', 16702: '? dude/buddy/bro/mate/sir'}\n"
     ]
    }
   ],
   "source": [
    "print(feature_labelsb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a4dc57e-32f1-418c-af43-6c0984c129b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([389372, 5])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<link href='https://fonts.googleapis.com/css?family=Noto Sans' rel='stylesheet'>\n",
       "<style>\n",
       "/* Base Noto Sans and Serif for Latin, Greek, and Cyrillic */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans:wght@400;700&family=Noto+Serif:wght@400;700&display=swap');\n",
       "\n",
       "/* East Asian scripts */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@400;700&family=Noto+Sans+KR:wght@400;700&family=Noto+Sans+SC:wght@400;700&family=Noto+Sans+TC:wght@400;700&display=swap');\n",
       "\n",
       "/* South Asian scripts */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Devanagari:wght@400;700&family=Noto+Sans+Bengali:wght@400;700&family=Noto+Sans+Tamil:wght@400;700&display=swap');\n",
       "\n",
       "/* Middle Eastern scripts */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Arabic:wght@400;700&family=Noto+Sans+Hebrew:wght@400;700&display=swap');\n",
       "\n",
       "/* Other scripts */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Thai:wght@400;700&family=Noto+Sans+Ethiopic:wght@400;700&display=swap');\n",
       "\n",
       "/* Specialty fonts */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Mono:wght@400;700&family=Noto+Color+Emoji&display=swap');\n",
       "\n",
       "#ayy {\n",
       "  font-family: 'Noto Sans', 'Noto Sans JP', 'Noto Sans KR', 'Noto Sans SC', 'Noto Sans TC', \n",
       "               'Noto Sans Devanagari', 'Noto Sans Bengali', 'Noto Sans Tamil', \n",
       "               'Noto Sans Arabic', 'Noto Sans Hebrew', 'Noto Sans Thai', 'Noto Sans Ethiopic',\n",
       "               sans-serif;\n",
       "}\n",
       "\n",
       "/* Language-specific rules */\n",
       ":lang(ja) { font-family: 'Noto Sans JP', sans-serif; }\n",
       ":lang(ko) { font-family: 'Noto Sans KR', sans-serif; }\n",
       ":lang(zh-CN) { font-family: 'Noto Sans SC', sans-serif; }\n",
       ":lang(zh-TW) { font-family: 'Noto Sans TC', sans-serif; }\n",
       ":lang(hi) { font-family: 'Noto Sans Devanagari', sans-serif; }\n",
       ":lang(bn) { font-family: 'Noto Sans Bengali', sans-serif; }\n",
       ":lang(ta) { font-family: 'Noto Sans Tamil', sans-serif; }\n",
       ":lang(ar) { font-family: 'Noto Sans Arabic', sans-serif; }\n",
       ":lang(he) { font-family: 'Noto Sans Hebrew', sans-serif; }\n",
       ":lang(th) { font-family: 'Noto Sans Thai', sans-serif; }\n",
       ":lang(am), :lang(ti) { font-family: 'Noto Sans Ethiopic', sans-serif; }\n",
       "\n",
       "/* Emoji support */\n",
       ".emoji {\n",
       "  font-family: 'Noto Color Emoji', sans-serif;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f857796a6c7941ee8687af34d49ae2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='ffff', continuous_update=False, description='String:', placeholder='Type something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef099d78c4df477bac17f109afe67f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, HTML\n",
    "global cur_feature_ind\n",
    "cur_feature_ind = None\n",
    "def display_unlabeled_feature():\n",
    "    global cur_feature_ind\n",
    "    global feature_labels\n",
    "    available_features = features_sorted_by_feat_i.keys() - feature_labelsb.keys()\n",
    "    for f in available_features:\n",
    "        if not '?' in feature_labels[f]:\n",
    "            continue\n",
    "        print(f\"feature {f}\")\n",
    "        cur_feature_ind = f\n",
    "        feats = features_sorted_by_feat_i[f]\n",
    "        text_item.value = feature_labels[f]\n",
    "        #if any([position_map[feat.pos][0] != 'n' for feat in feats]):\n",
    "            #print(f\"warning, feature {feat_i} has non name poses, all pos are {[position_map[f.pos] for f in feats]})\")\n",
    "        #    continue\n",
    "        display_feats(feats)\n",
    "        return\n",
    "    num_left = 0\n",
    "    for f in feature_labels.keys():\n",
    "        if \"?\" in feature_labels[f]:\n",
    "            num_left += 1\n",
    "    print(f\"num left {num_left}\")\n",
    "    for f in feature_labels.keys():\n",
    "        if \"?\" in feature_labels[f]:\n",
    "            cur_feature_ind = f\n",
    "            text_item.value = feature_labels[f]\n",
    "            display_feats(features_sorted_by_feat_i[f])\n",
    "            return\n",
    "\n",
    "print(data.size())\n",
    "display(HTML(\"\"\"<link href='https://fonts.googleapis.com/css?family=Noto Sans' rel='stylesheet'>\n",
    "<style>\n",
    "/* Base Noto Sans and Serif for Latin, Greek, and Cyrillic */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans:wght@400;700&family=Noto+Serif:wght@400;700&display=swap');\n",
    "\n",
    "/* East Asian scripts */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@400;700&family=Noto+Sans+KR:wght@400;700&family=Noto+Sans+SC:wght@400;700&family=Noto+Sans+TC:wght@400;700&display=swap');\n",
    "\n",
    "/* South Asian scripts */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Devanagari:wght@400;700&family=Noto+Sans+Bengali:wght@400;700&family=Noto+Sans+Tamil:wght@400;700&display=swap');\n",
    "\n",
    "/* Middle Eastern scripts */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Arabic:wght@400;700&family=Noto+Sans+Hebrew:wght@400;700&display=swap');\n",
    "\n",
    "/* Other scripts */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Thai:wght@400;700&family=Noto+Sans+Ethiopic:wght@400;700&display=swap');\n",
    "\n",
    "/* Specialty fonts */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Mono:wght@400;700&family=Noto+Color+Emoji&display=swap');\n",
    "\n",
    "#ayy {\n",
    "  font-family: 'Noto Sans', 'Noto Sans JP', 'Noto Sans KR', 'Noto Sans SC', 'Noto Sans TC', \n",
    "               'Noto Sans Devanagari', 'Noto Sans Bengali', 'Noto Sans Tamil', \n",
    "               'Noto Sans Arabic', 'Noto Sans Hebrew', 'Noto Sans Thai', 'Noto Sans Ethiopic',\n",
    "               sans-serif;\n",
    "}\n",
    "\n",
    "/* Language-specific rules */\n",
    ":lang(ja) { font-family: 'Noto Sans JP', sans-serif; }\n",
    ":lang(ko) { font-family: 'Noto Sans KR', sans-serif; }\n",
    ":lang(zh-CN) { font-family: 'Noto Sans SC', sans-serif; }\n",
    ":lang(zh-TW) { font-family: 'Noto Sans TC', sans-serif; }\n",
    ":lang(hi) { font-family: 'Noto Sans Devanagari', sans-serif; }\n",
    ":lang(bn) { font-family: 'Noto Sans Bengali', sans-serif; }\n",
    ":lang(ta) { font-family: 'Noto Sans Tamil', sans-serif; }\n",
    ":lang(ar) { font-family: 'Noto Sans Arabic', sans-serif; }\n",
    ":lang(he) { font-family: 'Noto Sans Hebrew', sans-serif; }\n",
    ":lang(th) { font-family: 'Noto Sans Thai', sans-serif; }\n",
    ":lang(am), :lang(ti) { font-family: 'Noto Sans Ethiopic', sans-serif; }\n",
    "\n",
    "/* Emoji support */\n",
    ".emoji {\n",
    "  font-family: 'Noto Color Emoji', sans-serif;\n",
    "}\n",
    "</style>\"\"\"))\n",
    "def display_feats(feats):\n",
    "    all_records = []\n",
    "    feat_len = len(feats[0].records)\n",
    "    for feat in feats:\n",
    "        print(feat.pos)\n",
    "        all_records += feat.records\n",
    "    records = torch.tensor(all_records)\n",
    "    dats = [torch.tensor([1]).repeat(feat_len),torch.tensor([2]).repeat(feat_len),torch.tensor([3]).repeat(feat_len),torch.tensor([4]).repeat(feat_len)]\n",
    "    which_pos = torch.cat(dats)\n",
    "    print(records.size())\n",
    "    top_act_inds = torch.argsort(-records)\n",
    "    covered_already = set()\n",
    "    simpler_words = []\n",
    "    for i in range(500):\n",
    "        ind = top_act_inds[i]\n",
    "        token_pos = which_pos[ind]\n",
    "        data_pos = ind % feat_len\n",
    "        act = records[ind]\n",
    "        acts = [0]\n",
    "        for j in range(1,5):\n",
    "            acts.append([feat.records[data_pos] for feat in feats if feat.pos == j][0])\n",
    "        toks = model.to_str_tokens(data[data_pos])\n",
    "        relevant_str = \"\".join(toks[:token_pos+1])\n",
    "        if relevant_str in covered_already:\n",
    "            continue\n",
    "        covered_already.add(relevant_str)\n",
    "        out_toks = []\n",
    "        colors = ['', 'red', 'orange', 'yellow', 'green']\n",
    "        for j,tok in enumerate(toks):\n",
    "            if len(tok.strip()) == 0:\n",
    "                tok = repr(tok)\n",
    "            tok = tok.replace(\"\\n\", \"\\\\n\")\n",
    "            colored = f\"<span id='ayy'><font color='{colors[j]}'>{tok}</font></span>\"\n",
    "            if j < 1: continue\n",
    "            if j == token_pos:\n",
    "                colored = f\"<span id='ayy'><font color='pink'>{tok}</font</span>\"\n",
    "            if acts[j] > 0.01:\n",
    "                out_toks.append(f\"{colored}{acts[j]:.3f}\")\n",
    "            else:\n",
    "                out_toks.append(colored)\n",
    "        simpler = model.tokenizer.decode(data[data_pos][1:token_pos+1])\n",
    "        simpler_words.append(simpler.strip())\n",
    "\n",
    "        display(HTML(toks[token_pos] + \"\\t||\\t\" + \"<span id='ayy'>\" + simpler + \"</span>\\t||\\t\" + \"\".join(out_toks)))\n",
    "        if len(simpler_words) == 20:\n",
    "            out_s = \"<span id='ayy'>What do \"\n",
    "            for s in simpler_words:\n",
    "                out_s += f'\"{s.strip()}\", '\n",
    "            out_s += \"have in common? Take a deep breath and think step by step.\"\n",
    "            display(HTML(out_s + \"</span>\"))\n",
    "    '''\n",
    "    feat_vecs = [get_name_vector(feat, 'mean') for feat in feats]\n",
    "    avg_vec = torch.stack(feat_vecs).mean(dim=0)\n",
    "    min_vec = torch.stack([get_name_vector(feat, 'min') for feat in feats]).min(dim=0).values\n",
    "    max_vec = torch.stack([get_name_vector(feat, 'max') for feat in feats]).max(dim=0).values\n",
    "    sorted_names = torch.argsort(-avg_vec)\n",
    "    #print(avg_vec, min_vec, max_vec, sorted_names)\n",
    "    for name_i in sorted_names[:100]:\n",
    "        #print(name_i)\n",
    "        print(f\" name {names[name_i]} with avg {avg_vec[name_i]} min {min_vec[name_i]} max {max_vec[name_i]}\")\n",
    "    '''\n",
    "    '''\n",
    "    for feat in feats:\n",
    "        if position_map[feat.pos][0] == 'n':\n",
    "            print(position_map[feat.pos], detect_single_letter(feat))\n",
    "            pretty_print_list_first_letter_info(list_first_letter_info(feat))\n",
    "            print(improved_first_letter(feat))\n",
    "    \n",
    "    diffs = torch.zeros(len(feats), len(feats))\n",
    "    for i,featv1 in enumerate(feat_vecs):\n",
    "        for j,featv2 in enumerate(feat_vecs):\n",
    "            diffs[i,j] = torch.mean(torch.abs(featv1-featv2))\n",
    "    labels = [position_map[feat.pos] for feat in feats]\n",
    "    imshow(diffs, x=labels, y=labels, font_size=9)\n",
    "    '''\n",
    "def save_labels():\n",
    "    with open(\"layer_15_features_take_two.pkl\", \"wb\") as f:\n",
    "        global feature_labelsb\n",
    "        pickle.dump(feature_labelsb, f)\n",
    "        print(f\"done saving {len(feature_labelsb)}\")\n",
    "import traceback\n",
    "out = widgets.Output()\n",
    "global cur_feature_ind\n",
    "global feature_labels\n",
    "def submitted(change):\n",
    "    global cur_feature_ind\n",
    "    global feature_labels\n",
    "    if len(text_item.value.strip()) > 0 and (text_item.value != feature_labels[cur_feature_ind]):\n",
    "        with out:\n",
    "            try:\n",
    "                times = 0\n",
    "                clear_output()\n",
    "                feature_labelsb[cur_feature_ind] = text_item.value\n",
    "                save_labels()\n",
    "                display_unlabeled_feature()\n",
    "            except:\n",
    "                print(traceback.format_exc())\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "text_item = widgets.Text(\n",
    "    value='ffff',\n",
    "    placeholder='Type something',\n",
    "    description='String:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    ")\n",
    "\n",
    "display(text_item)\n",
    "display(out)\n",
    "text_item.observe(submitted, names='value')\n",
    "\n",
    "with out:\n",
    "    display_unlabeled_feature()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c72c3651-7d6e-4cab-8f22-15d7260fe014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='layer_15_features.pkl' target='_blank'>layer_15_features.pkl</a><br>"
      ],
      "text/plain": [
       "/home/dev/sae-k-sparse-mamba/layer_15_features.pkl"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, FileLink\n",
    "import os\n",
    "display(FileLink(\"layer_15_features.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d96ae99-0f7d-456f-a8be-365611fe5a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24649 1800's and 1900's MOREDATA: used to be 1800's and 1900's\n",
      "15582 1800-1999\n",
      "30568 1980's-1990s pop culture\n",
      "6227 3 digit numbers, renal excretions, and escaped quotes MOREDATA: Russian Names, 3 digit numbers, renal excretions, and escaped quotes\n",
      "8103 5/May/Five/Fifth\n",
      "32395 [ with some other symbols  [\", **[, etc.\n",
      "31021 A\n",
      "29892 A\n",
      "14819 about names\n",
      "1312 account/login terminology and oxidation/breathing\n",
      "12348 actor portrayal, embodied, playing MOREDATA: Famous Actors, actor portrayal, embodied, playing\n",
      "19724 aggressive/vigorously\n",
      "21271 Airline Brands and Punk Rock bands\n",
      "15562 all caps hurricaine/military codename stuff\n",
      "14506 all caps licensing/warranty terms MOREDATA: words put together with no space  prevoiusly all caps licensing/warranty terms\n",
      "23646 assay chemical terms\n",
      "22790 B\n",
      "28979 B\n",
      "21593 blood related medical terminology\n",
      "28482 Canada related terms and names\n",
      "16927 Canada terms\n",
      "29448 Cancer research cell lines\n",
      "9622 cancer treatment related terminology and wording: radiation related wording, chemotheraphy drugs, opiods\n",
      "18809 Carbon isotopes\n",
      "31591 cell biology\n",
      "11238 cell signaling\n",
      "6351 Cellular Signaling Pathways MOREDATA: Russian Names  previously Cellular Signaling Pathways\n",
      "30573 chemistry terms\n",
      "29391 circle related terms\n",
      "23218 Cities in Midwestern United States  mostly\n",
      "9660 clarifiers  seperately, equally, details, correspondingly, specifics, jointly\n",
      "9344 College Admission Terms  SAT, admissions, graduate, GRE\n",
      "29024 common denominator of\n",
      "6362 Communism Related Terms MOREDATA: Communism Related Terms and locations\n",
      "22784 Compound nouns  two or more words\n",
      "30030 contacting someone  call, telephone, email\n",
      "24846 Contains an e\n",
      "2836 contains subword within the word/phrase\n",
      "23306 court, protest and voting related news terms\n",
      "30041 cryptography terms\n",
      "19013 cybersecurity/security terms\n",
      "26824 D\n",
      "1349 D\n",
      "2517 denominator of\n",
      "7207 double L and also isotopes\n",
      "20736 driving/cars\n",
      "20089 driving/cars\n",
      "16702 dude/buddy/bro/mate/sir MOREDATA: dude/buddy/bro/mate/sir\n",
      "27758 E\n",
      "9371 E\n",
      "1444 E\n",
      "16563 E and D words  mostly E  MOREDATA: E and D words  mostly E, some F , token contains an e sound\n",
      "28977 East Coast News and Sports MOREDATA: East Coast News and Sports  mostly New Jersey\n",
      "9152 ends in ing  and less so ed\n",
      "11188 Fighter/Bomber Wings\n",
      "24327 First/primary/january/1  also lesser, second\n",
      "32037 flying\n",
      "6674 Foreign language words  mostly russian but also japaneese\n",
      "31883 FORMAL LEGAL LANGUAGE IN ALL CAPS\n",
      "16138 G\n",
      "26556 G/H\n",
      "29097 Genocide and terrorism related countries, people, and organizations\n",
      "19433 genomics terms\n",
      "4463 german ends of words, and punk bands  Linkin Park, Slipknot, etc.\n",
      "23671 German words\n",
      "16606 he/she/hes/hers/theirs pronoun\n",
      "23385 HTTP terms\n",
      "16022 hunger/appetite/chewing\n",
      "8649 I\n",
      "17920 I\n",
      "1593 inflation\n",
      "6266 interactions with webpage MOREDATA: me/you/your  previously interactions with webpage\n",
      "5218 Ireland/Scottish names\n",
      "17703 irish/scottish\n",
      "25124 J/Y\n",
      "25903 J/Y\n",
      "20740 Japaneese or Chineese Terms MOREDATA: Designations of Military Units  previously, Japaneese or Chineese Terms\n",
      "21149 Javascript programming\n",
      "19977 Judicial proceedings\n",
      "1525 Judiciary news  packing, unconstitutional, sued, judiciary\n",
      "20604 Kidney Function\n",
      "21206 Kidney function terms\n",
      "17259 L\n",
      "21851 L\n",
      "32240 L\n",
      "20217 L\n",
      "12167 L\n",
      "29977 Late 1800's through 1990's, and notable figures of that time\n",
      "1429 Latex symbols and 3-digit numbers\n",
      "23490 lipids/cholesterol\n",
      "13212 liver disease/research on liver disease, or something about airlines\n",
      "6635 Locations  with hospitals\n",
      "22801 M\n",
      "6146 M MOREDATA: M  also fires on final token of M words\n",
      "9911 marajuana\n",
      "5740 marijuana\n",
      "16512 marriage and health  microbiome, antioxidants  related terms MOREDATA: , accountants, marriage and health  microbiome, antioxidants  related terms\n",
      "31070 marriage related terms\n",
      "17547 mate/sir/bro/dude/buddy\n",
      "12712 Math symbols\n",
      "23692 mathematical symbols\n",
      "4104 May, five, fifth\n",
      "27417 maybe contains c and/or h\n",
      "1231 maybe contains symbols\n",
      "15112 maybe double letters\n",
      "25370 maybe ends in a vowel\n",
      "17257 maybe partial phrase activation\n",
      "12592 maybe privacy/mind control related things\n",
      "20764 measuring methods/instruments\n",
      "6667 Medical surgery terms\n",
      "14859 Medical terms related to kidney function\n",
      "23657 Michigan cities, words, and teams\n",
      "12371 Middle East Diplomacy  also haemostatic/thrombosis/punk rock/hematopoiesis\n",
      "2129 MMA Wrestling and Sports MOREDATA: Michigan Locations  previously MMA Wrestling and Sports\n",
      "17165 mostly all caps locations or military code names  Troipical storm names  Not sure\n",
      "27844 Mostly duplicated phrases, but also some short phrases\n",
      "14934 Mostly punctuation\n",
      "32518 Mostly Tex related symbols\n",
      "2165 Muilti token phrases maybe\n",
      "2344 N\n",
      "8113 N\n",
      "8600 Name words like brother, mate, master, accountant\n",
      "17015 names\n",
      "21471 names\n",
      "5151 names\n",
      "19710 names\n",
      "30460 names\n",
      "5290 Names MOREDATA: Names, mostly fictional\n",
      "19851 Natural Disasters and also mate/sir/dude/bro/sis\n",
      "13434 negotiations  advocate, persuade, marketing, lobbying\n",
      "32156 News MOREDATA: Some bands  News\n",
      "8520 Notable events in 1850-1950\n",
      "29913 notification messages  coming soon/screen name, syntax error on, /**, some german words\n",
      "27326 occupations\n",
      "19210 oh my gosh but also some other stuff idk\n",
      "8401 Opening Nested Brackets MOREDATA: Opening Nested Brackets\n",
      "15921 P\n",
      "29400 partly/largely/portion\n",
      "2480 past tense verbs\n",
      "19119 patents/invention/innovation\n",
      "2438 phone calls\n",
      "4059 poem/verse/lyrics/musicians/playwright\n",
      "25584 Pop culture  polarizing topics   MOREDATA: Pop culture  polarizing topics    maybe duplicated letters\n",
      "24408 Pop culture from 1970s\n",
      "1560 Pop culture News\n",
      "25867 Pop culture terms and names\n",
      "9832 Popular Culture  mostly sports, but also honesty/lying related\n",
      "26388 portray/actors/embodied  and names of actors\n",
      "31773 possibly duplicated letters  Or having the word 2 or maybe typos  Not sure\n",
      "27945 prefixes of scientific terms\n",
      "6014 present tense verb\n",
      "19868 Probably copyright code comment related\n",
      "12210 probably greek symbols\n",
      "23448 probably the * symbol  Used for censoring maybe\n",
      "5066 programming languages/IEEE\n",
      "29937 programming related initialization/zero terms\n",
      "31304 proportion/percentage, acres, miles\n",
      "6401 Q\n",
      "7210 Q\n",
      "15762 Q\n",
      "11918 Q\n",
      "9856 quantum resonance/oscillation/vibration related terms\n",
      "4214 quantum terminology\n",
      "26187 quiz/assignments/exams, also nudity/uncovered/covered\n",
      "25010 quotations about sports teams and celebrities\n",
      "10252 R\n",
      "8935 R\n",
      "7436 R\n",
      "25933 R  and sometimes P, but mostly R\n",
      "13336 relations of people  parents, families, mothers, investors, parents, farmers\n",
      "11644 Relatives  dad, mom, grandfather, cousin\n",
      "6272 renal cancer terms/sports teams MOREDATA: Human Suffering  Cancer, Auschwitz, Guantanamo Bay, Divorce, Chernobyl  previously renal cancer terms/sports teams\n",
      "22605 Repeated Token\n",
      "2713 repeating a letter  or sound  twice\n",
      "9370 right leaning or corporate news terms  Scripture, MSM, aired, shareholders, investors, advertise\n",
      "15421 rotating objects\n",
      "8607 Russian symbols\n",
      "30976 S\n",
      "11839 S\n",
      "2404 salt related chemistry\n",
      "9772 Scottish/Irish related terms\n",
      "21909 second to last token in word\n",
      "21333 second to last token on a word, maybe\n",
      "3888 second/ii/double/february\n",
      "9474 second/ii/secondly and also seperately/supposedly related   MOREDATA: there's no doubt/denying and other stuff , previously second/ii/secondly and also seperately/supposedly related\n",
      "5334 sign related terminology  art stuff about how they are made, what put on, when used, etc.\n",
      "10175 sir/mate/bro also sports teams\n",
      "3765 sir/mate/bro/buddy/dude/bretheren\n",
      "10143 sir/mate/bro/happy faces, also programming terms\n",
      "9070 smell and taste related terms  also mitochondria\n",
      "20651 socialist countries/people, and escaped quotes MOREDATA: Russia related terms  previously socialist countries/people, and escaped quotes\n",
      "29875 some \"musings on life/thoughts on life/up to date on\" sorts of phrases\n",
      "29495 some name related, likes Mc\n",
      "16598 Something about names\n",
      "27600 spooky\n",
      "25299 Sports teams\n",
      "1925 Sports teams\n",
      "24468 Sports Teams\n",
      "1993 Sports Teams\n",
      "25307 Sports teams and drugs\n",
      "19260 sports teams, some medical terms  Not clearly double letters\n",
      "2765 sqrt, also\n",
      "28510 superhero terms  and cellular signaling\n",
      "28428 Symbols\n",
      "9187 T\n",
      "25771 T\n",
      "28222 T\n",
      "7315 tempest all caps related stuff\n",
      "29260 things that people are  player, footballer, rapper, astronaut\n",
      "27463 Third/III/3\n",
      "8885 Third/III/3/March  also 4 a little\n",
      "5441 three digit numbers and determined/resolved/finishes MOREDATA: code notation  previously three digit numbers and determined/resolved/finishes\n",
      "23031 Three or two letter acronyms\n",
      "26276 tickets/permits/coupons/booking also signaling molocules\n",
      "9793 Tourism/Museum related terms\n",
      "23254 trademark/invention/lawyer related terms\n",
      "7153 traversing building words  upstairs, downstairs, browsing, directories, corridors, etc.\n",
      "21217 Tribalism/Polarising topics\n",
      "5164 Two letter acronyms\n",
      "2923 U\n",
      "9746 U\n",
      "3214 v\n",
      "7440 V\n",
      "7976 V\n",
      "25129 vacation/holidy trip terms\n",
      "23771 vegetarian/eco firendly related stuff, maybe\n",
      "21957 verbs\n",
      "2380 W\n",
      "28185 W\n",
      "29676 war related years and entities\n",
      "17946 web browser related tech terms\n",
      "28503 web development / some medical terms\n",
      "2370 wine\n",
      "29039 Wine and opiod related chemical properties\n",
      "11527 wine and vine related terms\n",
      "30887 wine tasting MOREDATA: denominator of/simplify sqrt , previouslywine tasting\n",
      "11431 X  and less so, Y  and roman numerals\n",
      "32344 X/crosses, also some y\n",
      "3481 Y/J and related sounds\n",
      "8860 Years  0-2000\n",
      "15551 Years  0-2000\n",
      "4516 z\n",
      "27951 Z\n",
      "total num features identified 243\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "combined = {}\n",
    "for f in feature_labels.keys():\n",
    "    if f in feature_labelsb:\n",
    "        combined[f] = feature_labels[f] + \" MOREDATA: \" + feature_labelsb[f].replace(\"?\", \" \").strip()\n",
    "    else:\n",
    "        combined[f] = feature_labels[f]\n",
    "    \n",
    "    if f in feature_labelsb and feature_labelsb[f].strip()  == feature_labels[f].strip():\n",
    "        combined[f] = feature_labels[f]\n",
    "    \n",
    "    if feature_labels[f].replace(\"?\", \"\").strip() == \"\" and f in feature_labelsb:\n",
    "        combined[f] = feature_labelsb[f]\n",
    "    combined[f] = combined[f].replace(\"?\", \" \").replace(\"(\", \" \").replace(\")\", \" \").strip()\n",
    "    if combined[f].strip() == \"\" or combined[f].strip() == '||':\n",
    "        del combined[f]\n",
    "labs = sorted(list(combined.items()), key=lambda x: x[1].lower().strip())\n",
    "for f,l in labs:\n",
    "    #if not '?' in l:\n",
    "    if l.strip() != \"\":\n",
    "        print(f, l)\n",
    "print(f\"total num features identified {len(labs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e2f5262-ec13-4c97-a16d-1368f35319c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6146: 'M (also fires on final token of M words)',\n",
       " 4104: 'May, five, fifth ',\n",
       " 10252: 'R ',\n",
       " 12348: 'Famous Actors, actor portrayal, embodied, playing',\n",
       " 24649: \"? used to be 1800's and 1900's\",\n",
       " 22605: 'Repeated Token'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_labelsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e63e1a39-917a-4f2c-84e6-19de4c31ddaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Purushottam'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" Purushottam\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0176123f-744f-4217-a337-7e0c83911678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
