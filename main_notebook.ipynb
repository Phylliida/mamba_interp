{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Notebook\n",
    "\n",
    "This notebook contains all the mech interp tools I've developed, centered around doing circuit analysis for any task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports/General Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires install of mamba lens\n",
    "# https://github.com/Phylliida/MambaLens\n",
    "# pip install git+https://github.com/Phylliida/MambaLens.git\n",
    "# and also my implementation of ACDC (used for dataset managment)\n",
    "# pip install git+https://github.com/Phylliida/ACDC.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import transformer_lens.utils as utils\n",
    "from mamba_lens import HookedMamba\n",
    "from acdc.data.utils import generate_dataset\n",
    "from tqdm import tqdm\n",
    "from acdc import get_pad_token\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# from transformer lens\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        # Parse the PyTorch version to check if it's below version 2.0\n",
    "        major_version = int(torch.__version__.split(\".\")[0])\n",
    "        if major_version >= 2:\n",
    "            return torch.device(\"mps\")\n",
    "\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "# modified from neel nanda's examples\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", font_size=None, show=True, color_continuous_midpoint=0.0, fix_size=False, **kwargs):\n",
    "    fig = px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=color_continuous_midpoint, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs)\n",
    "    if not font_size is None:\n",
    "        if 'x' in kwargs:\n",
    "            fig.update_layout(\n",
    "              xaxis = dict(\n",
    "                tickmode='array',\n",
    "                tickvals = kwargs['x'],\n",
    "                ticktext = kwargs['x'], \n",
    "                ),\n",
    "               font=dict(size=font_size, color=\"black\"))\n",
    "        if 'y' in kwargs:\n",
    "            fig.update_layout(\n",
    "              yaxis = dict(\n",
    "                tickmode='array',\n",
    "                tickvals = kwargs['y'],\n",
    "                ticktext = kwargs['y'], \n",
    "                ),\n",
    "               font=dict(size=font_size, color=\"black\"))\n",
    "    if fix_size:\n",
    "        # default settings aren't very good, these are better\n",
    "        plot_args = {\n",
    "            'width': 800,\n",
    "            'height': 600,\n",
    "            \"autosize\": False,\n",
    "            'showlegend': True,\n",
    "            'margin': {\"l\":0,\"r\":0,\"t\":100,\"b\":0}\n",
    "        }\n",
    "        if model.cfg.n_layers < len(kwargs['y']):\n",
    "            plot_args['height'] *= model.cfg.D_conv\n",
    "        \n",
    "        fig.update_layout(**plot_args)\n",
    "        fig.update_layout(legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=0.01\n",
    "        ))\n",
    "    if show:\n",
    "        fig.show(renderer)\n",
    "    else:\n",
    "        return fig\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)\n",
    "\n",
    "def bar_chart(data, x_labels, y_label, title, font_size=None):\n",
    "    # it requires a pandas dict with the columns and rows named, annoying\n",
    "    # by default rows and columns are named with ints so we relabel them accordingly\n",
    "    renames = dict([(i, x_labels[i]) for i in range(len(x_labels))])\n",
    "    ps = pd.DataFrame(data.cpu().numpy()).rename(renames, axis='rows').rename({0: y_label}, axis='columns')\n",
    "    fig = px.bar(ps, y=y_label, x=x_labels, title=title)\n",
    "    if not font_size is None:\n",
    "        fig.update_layout(\n",
    "          xaxis = dict(\n",
    "            tickmode='array',\n",
    "            tickvals = x_labels,\n",
    "            ticktext = x_labels, \n",
    "            ),\n",
    "           font=dict(size=font_size, color=\"black\"))\n",
    "    fig.show()\n",
    "\n",
    "def get_batched_index_into(indices):\n",
    "    '''\n",
    "    given data that is [B,N,V] and indicies that are [B,N,K] with each index being an index into the V space\n",
    "    this gives you indexes you can use to access your values\n",
    "    '''\n",
    "    first_axis = []\n",
    "    second_axis = []\n",
    "    third_axis = []\n",
    "    B, _, _ = indices.size()\n",
    "    for b in range(B):\n",
    "        second, third = get_index_into(indices[b])\n",
    "        first_axis.append(torch.full(second.size(), fill_value=b, device=model.cfg.device))\n",
    "        second_axis.append(second)\n",
    "        third_axis.append(third)\n",
    "\n",
    "    return torch.cat(first_axis), torch.cat(second_axis), torch.cat(third_axis)\n",
    "\n",
    "def get_index_into(indices):\n",
    "    '''\n",
    "    given data that is [N,V] and indicies that are [N,K] with each index being an index into the V space\n",
    "    this gives you indexes you can use to access your values\n",
    "    '''\n",
    "    num_data, num_per_data = indices.size()\n",
    "    # we want\n",
    "    # [0,0,0,...,] num per data of these\n",
    "    # [1,1,1,...,] num per data of these\n",
    "    # ...\n",
    "    # [num_data-1, num_data-1, ...]\n",
    "    first_axis_index = torch.arange(num_data, dtype=torch.long).view(num_data, 1)*torch.ones([num_data, num_per_data], dtype=torch.long)\n",
    "    # now we flatten it so it has an index for each term aligned with our indices\n",
    "    first_axis_index = first_axis_index.flatten()\n",
    "    second_axis_index = indices.flatten()\n",
    "    return first_axis_index, second_axis_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.set_grad_enabled(False)\n",
    "device = get_device()\n",
    "print(\"device\", device)\n",
    "model = HookedMamba.from_pretrained(\"state-spaces/mamba-370m\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['<|endoftext|>', ' supposed', 'column', ' Alien', ' looking', 'equal', ' Women', ' Cour', ' incomplete', ' cov', ' inventory', '840', '}$', ' force', ' Plans', 'ycin', ' airport', ' senators', ' Wolf', 'idav', ' nerv', ' Menu', ' gig', ' Statement', 'strom', ' cosmetic', ' receivers', ' keep', 'If', ' indent', ' Command', ' supposed', 'column', ' Alien', ' looking', 'equal', ' Women', ' Cour', ' incomplete', ' cov', ' inventory', '840', '}$', ' force', ' Plans', 'ycin', ' airport', ' senators', ' Wolf', 'idav', ' nerv', ' Menu', ' gig', ' Statement', 'strom', ' cosmetic', ' receivers', ' keep', 'If', ' indent']\n",
      "answers   are [' Command']\n",
      "incorrect are [' mile']\n",
      "['<|endoftext|>', ' supposed', 'column', ' Alien', ' looking', 'equal', ' Women', ' Cour', ' incomplete', ' cov', ' inventory', '840', '}$', ' force', ' Plans', 'ycin', ' airport', ' senators', ' Wolf', 'idav', ' nerv', ' Menu', ' gig', ' Statement', 'strom', ' cosmetic', ' receivers', ' keep', 'If', ' indent', ' mile', ' supposed', 'column', ' Alien', ' looking', 'equal', ' Women', ' Cour', ' incomplete', ' cov', ' inventory', '840', '}$', ' force', ' Plans', 'ycin', ' airport', ' senators', ' Wolf', 'idav', ' nerv', ' Menu', ' gig', ' Statement', 'strom', ' cosmetic', ' receivers', ' keep', 'If', ' indent']\n",
      "answers   are [' mile']\n",
      "incorrect are [' Command']\n",
      "1\n",
      "['<|endoftext|>', ' readonly', ' 366', 'educated', 'YPE', ' Challenge', ' oct', 'SV', ' person', ' Mov', ' XIII', ' todd', 'emat', 'porary', 'jquery', ' threads', ' move', ' standing', ',[@', 'pos', 'annel', 'osocial', 'arth', ' plur', ' directions', ' putative', ' ect', 'Poss', 'Accessor', ' CO', ' Vector', ' readonly', ' 366', 'educated', 'YPE', ' Challenge', ' oct', 'SV', ' person', ' Mov', ' XIII', ' todd', 'emat', 'porary', 'jquery', ' threads', ' move', ' standing', ',[@', 'pos', 'annel', 'osocial', 'arth', ' plur', ' directions', ' putative', ' ect', 'Poss', 'Accessor', ' CO']\n",
      "answers   are [' Vector']\n",
      "incorrect are ['072']\n",
      "['<|endoftext|>', ' readonly', ' 366', 'educated', 'YPE', ' Challenge', ' oct', 'SV', ' person', ' Mov', ' XIII', ' todd', 'emat', 'porary', 'jquery', ' threads', ' move', ' standing', ',[@', 'pos', 'annel', 'osocial', 'arth', ' plur', ' directions', ' putative', ' ect', 'Poss', 'Accessor', ' CO', '072', ' readonly', ' 366', 'educated', 'YPE', ' Challenge', ' oct', 'SV', ' person', ' Mov', ' XIII', ' todd', 'emat', 'porary', 'jquery', ' threads', ' move', ' standing', ',[@', 'pos', 'annel', 'osocial', 'arth', ' plur', ' directions', ' putative', ' ect', 'Poss', 'Accessor', ' CO']\n",
      "answers   are ['072']\n",
      "incorrect are [' Vector']\n",
      "2\n",
      "['<|endoftext|>', ' Vacc', 'igion', ' fake', 'hered', ' despite', ' ST', 'ANY', ' fulfilled', 'ox', ' 312', ' factors', 'FB', ' exported', 'editor', ' bicy', 'utory', 'Events', ' GSH', 'Mart', ' android', ' hands', ' histogram', ' Webster', ' moves', ' Programs', ' Bott', ' GV', 'BE', ' appl', 'idth', ' Vacc', 'igion', ' fake', 'hered', ' despite', ' ST', 'ANY', ' fulfilled', 'ox', ' 312', ' factors', 'FB', ' exported', 'editor', ' bicy', 'utory', 'Events', ' GSH', 'Mart', ' android', ' hands', ' histogram', ' Webster', ' moves', ' Programs', ' Bott', ' GV', 'BE', ' appl']\n",
      "answers   are ['idth']\n",
      "incorrect are [' floor']\n",
      "['<|endoftext|>', ' Vacc', 'igion', ' fake', 'hered', ' despite', ' ST', 'ANY', ' fulfilled', 'ox', ' 312', ' factors', 'FB', ' exported', 'editor', ' bicy', 'utory', 'Events', ' GSH', 'Mart', ' android', ' hands', ' histogram', ' Webster', ' moves', ' Programs', ' Bott', ' GV', 'BE', ' appl', ' floor', ' Vacc', 'igion', ' fake', 'hered', ' despite', ' ST', 'ANY', ' fulfilled', 'ox', ' 312', ' factors', 'FB', ' exported', 'editor', ' bicy', 'utory', 'Events', ' GSH', 'Mart', ' android', ' hands', ' histogram', ' Webster', ' moves', ' Programs', ' Bott', ' GV', 'BE', ' appl']\n",
      "answers   are [' floor']\n",
      "incorrect are ['idth']\n",
      "3\n",
      "['<|endoftext|>', ' get', ' pp', ' QC', ' finish', ' players', ' acquaint', 'ACP', ' blank', ' pir', 'arine', ' efficacy', ' unc', ' ms', 'inear', ' 510', '574', ' 768', 'FROM', 'Tx', ' fixed', ' PIN', ' 317', ' copolymer', ' enlightened', ' Priority', ' survivor', ' establish', 'USD', ' unfamiliar', ' trust', ' get', ' pp', ' QC', ' finish', ' players', ' acquaint', 'ACP', ' blank', ' pir', 'arine', ' efficacy', ' unc', ' ms', 'inear', ' 510', '574', ' 768', 'FROM', 'Tx', ' fixed', ' PIN', ' 317', ' copolymer', ' enlightened', ' Priority', ' survivor', ' establish', 'USD', ' unfamiliar']\n",
      "answers   are [' trust']\n",
      "incorrect are [' habitats']\n",
      "['<|endoftext|>', ' get', ' pp', ' QC', ' finish', ' players', ' acquaint', 'ACP', ' blank', ' pir', 'arine', ' efficacy', ' unc', ' ms', 'inear', ' 510', '574', ' 768', 'FROM', 'Tx', ' fixed', ' PIN', ' 317', ' copolymer', ' enlightened', ' Priority', ' survivor', ' establish', 'USD', ' unfamiliar', ' habitats', ' get', ' pp', ' QC', ' finish', ' players', ' acquaint', 'ACP', ' blank', ' pir', 'arine', ' efficacy', ' unc', ' ms', 'inear', ' 510', '574', ' 768', 'FROM', 'Tx', ' fixed', ' PIN', ' 317', ' copolymer', ' enlightened', ' Priority', ' survivor', ' establish', 'USD', ' unfamiliar']\n",
      "answers   are [' habitats']\n",
      "incorrect are [' trust']\n",
      "4\n",
      "['<|endoftext|>', ' indications', ' juvenile', ' guitar', 'Rank', 'FPar', ' 208', ' salient', 'Ready', 'Entry', 'rike', ' pointer', ' Phen', 'Sources', ' pointing', ' relapse', ' meditation', ' -------------------------------', ' statewide', ' would', ' onc', ' Accordingly', 'eness', 'STATIC', 'Several', ' adding', 'HIV', 'riched', ' Confed', ' chees', 'hypert', ' indications', ' juvenile', ' guitar', 'Rank', 'FPar', ' 208', ' salient', 'Ready', 'Entry', 'rike', ' pointer', ' Phen', 'Sources', ' pointing', ' relapse', ' meditation', ' -------------------------------', ' statewide', ' would', ' onc', ' Accordingly', 'eness', 'STATIC', 'Several', ' adding', 'HIV', 'riched', ' Confed', ' chees']\n",
      "answers   are ['hypert']\n",
      "incorrect are ['007']\n",
      "['<|endoftext|>', ' indications', ' juvenile', ' guitar', 'Rank', 'FPar', ' 208', ' salient', 'Ready', 'Entry', 'rike', ' pointer', ' Phen', 'Sources', ' pointing', ' relapse', ' meditation', ' -------------------------------', ' statewide', ' would', ' onc', ' Accordingly', 'eness', 'STATIC', 'Several', ' adding', 'HIV', 'riched', ' Confed', ' chees', '007', ' indications', ' juvenile', ' guitar', 'Rank', 'FPar', ' 208', ' salient', 'Ready', 'Entry', 'rike', ' pointer', ' Phen', 'Sources', ' pointing', ' relapse', ' meditation', ' -------------------------------', ' statewide', ' would', ' onc', ' Accordingly', 'eness', 'STATIC', 'Several', ' adding', 'HIV', 'riched', ' Confed', ' chees']\n",
      "answers   are ['007']\n",
      "incorrect are ['hypert']\n",
      "5\n",
      "['<|endoftext|>', ' Kamp', ' results', ' Administ', 'Path', 'ustomed', '146', ' carriers', ' spelled', 'Commun', ' footing', ' Bone', '*;', ' Kar', 'combin', 'complex', 'days', 'bl', ' insufficient', 'aline', ' AX', '099', '209', 'ica', ' moll', 'idas', ' contemplated', 'fm', 'adelphia', ' En', ' preferably', ' Kamp', ' results', ' Administ', 'Path', 'ustomed', '146', ' carriers', ' spelled', 'Commun', ' footing', ' Bone', '*;', ' Kar', 'combin', 'complex', 'days', 'bl', ' insufficient', 'aline', ' AX', '099', '209', 'ica', ' moll', 'idas', ' contemplated', 'fm', 'adelphia', ' En']\n",
      "answers   are [' preferably']\n",
      "incorrect are ['pose']\n",
      "['<|endoftext|>', ' Kamp', ' results', ' Administ', 'Path', 'ustomed', '146', ' carriers', ' spelled', 'Commun', ' footing', ' Bone', '*;', ' Kar', 'combin', 'complex', 'days', 'bl', ' insufficient', 'aline', ' AX', '099', '209', 'ica', ' moll', 'idas', ' contemplated', 'fm', 'adelphia', ' En', 'pose', ' Kamp', ' results', ' Administ', 'Path', 'ustomed', '146', ' carriers', ' spelled', 'Commun', ' footing', ' Bone', '*;', ' Kar', 'combin', 'complex', 'days', 'bl', ' insufficient', 'aline', ' AX', '099', '209', 'ica', ' moll', 'idas', ' contemplated', 'fm', 'adelphia', ' En']\n",
      "answers   are ['pose']\n",
      "incorrect are [' preferably']\n",
      "6\n",
      "['<|endoftext|>', ' preoperative', 'Comput', ' trails', ' DG', ' adapting', 'ionale', ' terminology', ' equilib', ' treaties', ' polygon', ' Messenger', ' Bi', ' corre', ' vou', ' effectiveness', ' Years', 'healthy', ' extremes', ' Support', 'Ra', ' Due', ' blogging', ' suggestion', ' mistakenly', ' muy', ' anesthetic', 'ammers', ' crossings', ' pursuing', 'ropol', ' preoperative', 'Comput', ' trails', ' DG', ' adapting', 'ionale', ' terminology', ' equilib', ' treaties', ' polygon', ' Messenger', ' Bi', ' corre', ' vou', ' effectiveness', ' Years', 'healthy', ' extremes', ' Support', 'Ra', ' Due', ' blogging', ' suggestion', ' mistakenly', ' muy', ' anesthetic', 'ammers', ' crossings', ' pursuing']\n",
      "answers   are ['ropol']\n",
      "incorrect are [' scrolling']\n",
      "['<|endoftext|>', ' preoperative', 'Comput', ' trails', ' DG', ' adapting', 'ionale', ' terminology', ' equilib', ' treaties', ' polygon', ' Messenger', ' Bi', ' corre', ' vou', ' effectiveness', ' Years', 'healthy', ' extremes', ' Support', 'Ra', ' Due', ' blogging', ' suggestion', ' mistakenly', ' muy', ' anesthetic', 'ammers', ' crossings', ' pursuing', ' scrolling', ' preoperative', 'Comput', ' trails', ' DG', ' adapting', 'ionale', ' terminology', ' equilib', ' treaties', ' polygon', ' Messenger', ' Bi', ' corre', ' vou', ' effectiveness', ' Years', 'healthy', ' extremes', ' Support', 'Ra', ' Due', ' blogging', ' suggestion', ' mistakenly', ' muy', ' anesthetic', 'ammers', ' crossings', ' pursuing']\n",
      "answers   are [' scrolling']\n",
      "incorrect are ['ropol']\n",
      "7\n",
      "['<|endoftext|>', 'ener', ' ste', '277', 'icularly', ' confrontation', 'orescence', ' boosted', 'STM', ' cro', ' converge', ' pleural', 'Advanced', ' actu', ' ambitious', ' nanoparticles', ' Shim', ' semicon', ' Genes', 'Ur', 'reads', ' engra', 'Wang', ' beside', '208', 'esti', ' Hav', 'Timeout', 'cerned', 'Arthur', 'Pop', 'ener', ' ste', '277', 'icularly', ' confrontation', 'orescence', ' boosted', 'STM', ' cro', ' converge', ' pleural', 'Advanced', ' actu', ' ambitious', ' nanoparticles', ' Shim', ' semicon', ' Genes', 'Ur', 'reads', ' engra', 'Wang', ' beside', '208', 'esti', ' Hav', 'Timeout', 'cerned', 'Arthur']\n",
      "answers   are ['Pop']\n",
      "incorrect are [' brutal']\n",
      "['<|endoftext|>', 'ener', ' ste', '277', 'icularly', ' confrontation', 'orescence', ' boosted', 'STM', ' cro', ' converge', ' pleural', 'Advanced', ' actu', ' ambitious', ' nanoparticles', ' Shim', ' semicon', ' Genes', 'Ur', 'reads', ' engra', 'Wang', ' beside', '208', 'esti', ' Hav', 'Timeout', 'cerned', 'Arthur', ' brutal', 'ener', ' ste', '277', 'icularly', ' confrontation', 'orescence', ' boosted', 'STM', ' cro', ' converge', ' pleural', 'Advanced', ' actu', ' ambitious', ' nanoparticles', ' Shim', ' semicon', ' Genes', 'Ur', 'reads', ' engra', 'Wang', ' beside', '208', 'esti', ' Hav', 'Timeout', 'cerned', 'Arthur']\n",
      "answers   are [' brutal']\n",
      "incorrect are ['Pop']\n",
      "8\n",
      "['<|endoftext|>', ' ult', 'pher', ' breath', 'Arthur', ' Boeing', 'ubble', ' zo', ' context', ' hidden', ' indul', ')}.$$', ' pixels', ' ArrayList', ' steal', ' smarter', 'NotNull', ' born', 'pton', ' subsequent', ' navigation', ' pivot', ' ninet', ' partitions', ' graduates', 'plements', '^*(\\\\', ' imperial', 'rization', ' piles', 'awt', ' ult', 'pher', ' breath', 'Arthur', ' Boeing', 'ubble', ' zo', ' context', ' hidden', ' indul', ')}.$$', ' pixels', ' ArrayList', ' steal', ' smarter', 'NotNull', ' born', 'pton', ' subsequent', ' navigation', ' pivot', ' ninet', ' partitions', ' graduates', 'plements', '^*(\\\\', ' imperial', 'rization', ' piles']\n",
      "answers   are ['awt']\n",
      "incorrect are ['Sign']\n",
      "['<|endoftext|>', ' ult', 'pher', ' breath', 'Arthur', ' Boeing', 'ubble', ' zo', ' context', ' hidden', ' indul', ')}.$$', ' pixels', ' ArrayList', ' steal', ' smarter', 'NotNull', ' born', 'pton', ' subsequent', ' navigation', ' pivot', ' ninet', ' partitions', ' graduates', 'plements', '^*(\\\\', ' imperial', 'rization', ' piles', 'Sign', ' ult', 'pher', ' breath', 'Arthur', ' Boeing', 'ubble', ' zo', ' context', ' hidden', ' indul', ')}.$$', ' pixels', ' ArrayList', ' steal', ' smarter', 'NotNull', ' born', 'pton', ' subsequent', ' navigation', ' pivot', ' ninet', ' partitions', ' graduates', 'plements', '^*(\\\\', ' imperial', 'rization', ' piles']\n",
      "answers   are ['Sign']\n",
      "incorrect are ['awt']\n",
      "9\n",
      "['<|endoftext|>', ' garden', ' capricious', ' Fl', 'particle', ' Truck', ' obstruct', ' flesh', ' sat', ' enter', ' ecosystems', 'iginally', ' carry', 'VEN', ' banning', ' bounded', ' advocate', ' mL', ' mediation', ' gambling', 'eterm', ' custom', 'Players', 'entre', 'Od', ' WE', '`', 'tl', 'isode', '347', ' Grow', ' garden', ' capricious', ' Fl', 'particle', ' Truck', ' obstruct', ' flesh', ' sat', ' enter', ' ecosystems', 'iginally', ' carry', 'VEN', ' banning', ' bounded', ' advocate', ' mL', ' mediation', ' gambling', 'eterm', ' custom', 'Players', 'entre', 'Od', ' WE', '`', 'tl', 'isode', '347']\n",
      "answers   are [' Grow']\n",
      "incorrect are ['Controls']\n",
      "['<|endoftext|>', ' garden', ' capricious', ' Fl', 'particle', ' Truck', ' obstruct', ' flesh', ' sat', ' enter', ' ecosystems', 'iginally', ' carry', 'VEN', ' banning', ' bounded', ' advocate', ' mL', ' mediation', ' gambling', 'eterm', ' custom', 'Players', 'entre', 'Od', ' WE', '`', 'tl', 'isode', '347', 'Controls', ' garden', ' capricious', ' Fl', 'particle', ' Truck', ' obstruct', ' flesh', ' sat', ' enter', ' ecosystems', 'iginally', ' carry', 'VEN', ' banning', ' bounded', ' advocate', ' mL', ' mediation', ' gambling', 'eterm', ' custom', 'Players', 'entre', 'Od', ' WE', '`', 'tl', 'isode', '347']\n",
      "answers   are ['Controls']\n",
      "incorrect are [' Grow']\n"
     ]
    }
   ],
   "source": [
    "def decode_and_encode(tokenizer, tokens):\n",
    "    '''\n",
    "    Gets rid of weird encoding issues by encoding and decoding\n",
    "    The tokens will be different that's okay and intentional\n",
    "    '''\n",
    "    prompt = tokenizer.decode(tokens).encode(\"ascii\", \"ignore\").decode(\"ascii\", \"ignore\")\n",
    "    return tokenizer.encode(prompt)\n",
    "\n",
    "def copy_data_generator(tokenizer, num_patching_pairs, copy_seq_len, num_repeats):\n",
    "    '''\n",
    "    Generates copy_seq_len random tokens, repeated twice (with the last token cut off)\n",
    "    This is just a test to see if it can copy the repeated sequence from before\n",
    "\n",
    "    for example, \n",
    "\n",
    "    uncorrupted:\n",
    "    a b c d a b c (answer is d)\n",
    "    corrupted:\n",
    "    a b c e a b c (answer is e)\n",
    "    '''\n",
    "    first_len = None\n",
    "\n",
    "    # ignore special tokens like BOS or PAD\n",
    "    special_token_ids = set()\n",
    "    for special_token_name, token_str in model.tokenizer.special_tokens_map.items():\n",
    "        special_token_ids.add(model.tokenizer.convert_tokens_to_ids([token_str])[0])\n",
    "    valid_ids = []\n",
    "    for tok in range(tokenizer.vocab_size):\n",
    "        if not tok in special_token_ids:\n",
    "            valid_ids.append(tok)    \n",
    "    valid_ids = torch.tensor(valid_ids)\n",
    "    for i in list(range(num_patching_pairs)):\n",
    "        while True:\n",
    "            # sample without replacement\n",
    "            # one extra so corrupted isn't in sequence\n",
    "            data = valid_ids[torch.randperm(len(valid_ids))[:copy_seq_len+1]].flatten()\n",
    "            corrupted_id = data[-1]\n",
    "            data_repeating = data[:-1]\n",
    "            uncorrupted_data = torch.concatenate([data_repeating]*num_repeats + [data_repeating[:-1]])\n",
    "            corrupted_data = torch.concatenate([data_repeating[:-1], torch.tensor([corrupted_id])]*num_repeats + [data_repeating[:-1]])\n",
    "            # make sure it encodes and decodes properly\n",
    "            uncorrupted_prompt = tokenizer.decode(uncorrupted_data).encode(\"ascii\", \"ignore\").decode(\"ascii\", \"ignore\")\n",
    "            corrupted_prompt = tokenizer.decode(corrupted_data).encode(\"ascii\", \"ignore\").decode(\"ascii\", \"ignore\")\n",
    "            uncorrupted_answer = tokenizer.decode(data_repeating[-1]).encode(\"ascii\", \"ignore\").decode(\"ascii\", \"ignore\")\n",
    "            corrupted_answer = tokenizer.decode(torch.tensor([corrupted_id])).encode(\"ascii\", \"ignore\").decode(\"ascii\", \"ignore\")\n",
    "            reencoded_uncorrupted_data = torch.tensor(tokenizer.encode(uncorrupted_prompt))\n",
    "            reencoded_corrupted_data = torch.tensor(tokenizer.encode(corrupted_prompt))\n",
    "            if reencoded_uncorrupted_data.size() == uncorrupted_data.size() and reencoded_corrupted_data.size() == corrupted_data.size() and torch.all(reencoded_uncorrupted_data == uncorrupted_data) and torch.all(reencoded_corrupted_data == corrupted_data):\n",
    "                break\n",
    "        yield uncorrupted_prompt, [uncorrupted_answer], [corrupted_answer]\n",
    "        yield corrupted_prompt, [corrupted_answer], [uncorrupted_answer]\n",
    "\n",
    "num_patching_pairs = 100\n",
    "seed = 42\n",
    "valid_seed = 41\n",
    "constrain_to_answers = False\n",
    "has_symmetric_patching = True\n",
    "varying_data_lengths = False\n",
    "copy_seq_len = 30\n",
    "num_repeats = 1 # just immediately copying seems only somewhat doable by this model\n",
    "\n",
    "data = generate_dataset(model=model,\n",
    "              data_generator=copy_data_generator,\n",
    "              num_patching_pairs=num_patching_pairs,\n",
    "              seed=seed,\n",
    "              valid_seed=valid_seed,\n",
    "              constrain_to_answers=constrain_to_answers,\n",
    "              has_symmetric_patching=has_symmetric_patching, \n",
    "              varying_data_lengths=varying_data_lengths,\n",
    "              copy_seq_len=copy_seq_len,\n",
    "              num_repeats=num_repeats)\n",
    "\n",
    "for i in list(range(10)):\n",
    "    uncorrupted_i = i*2\n",
    "    corrupted_i = i*2+1\n",
    "    uncorrupted = data.data[uncorrupted_i][:data.last_token_position[uncorrupted_i]+1]\n",
    "    corrupted = data.data[corrupted_i][:data.last_token_position[corrupted_i]+1]\n",
    "    print(i)\n",
    "    print(model.to_str_tokens(uncorrupted))\n",
    "    print(f\"answers   are {model.to_str_tokens(data.correct[uncorrupted_i])}\")\n",
    "    print(f\"incorrect are {model.to_str_tokens(data.incorrect[uncorrupted_i])}\")\n",
    "    print(model.to_str_tokens(corrupted))\n",
    "    print(f\"answers   are {model.to_str_tokens(data.correct[corrupted_i])}\")\n",
    "    print(f\"incorrect are {model.to_str_tokens(data.incorrect[corrupted_i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing example data points:\n",
      "<|endoftext|> supposedcolumn Alien lookingequal Women Cour incomplete cov inventory840}$ force Plansycin airport senators Wolfidav nerv Menu gig Statementstrom cosmetic receivers keepIf indent Command supposedcolumn Alien lookingequal Women Cour incomplete cov inventory840}$ force Plansycin airport senators Wolfidav nerv Menu gig Statementstrom cosmetic receivers keepIf indent\n",
      "  correct answer: ' Command'\n",
      "  incorrect answer: ' mile'\n",
      "<|endoftext|> supposedcolumn Alien lookingequal Women Cour incomplete cov inventory840}$ force Plansycin airport senators Wolfidav nerv Menu gig Statementstrom cosmetic receivers keepIf indent mile supposedcolumn Alien lookingequal Women Cour incomplete cov inventory840}$ force Plansycin airport senators Wolfidav nerv Menu gig Statementstrom cosmetic receivers keepIf indent\n",
      "  correct answer: ' mile'\n",
      "  incorrect answer: ' Command'\n",
      "<|endoftext|> readonly 366educatedYPE Challenge octSV person Mov XIII toddematporaryjquery threads move standing,[@posannelosocialarth plur directions putative ectPossAccessor CO Vector readonly 366educatedYPE Challenge octSV person Mov XIII toddematporaryjquery threads move standing,[@posannelosocialarth plur directions putative ectPossAccessor CO\n",
      "  correct answer: ' Vector'\n",
      "  incorrect answer: '072'\n",
      "<|endoftext|> readonly 366educatedYPE Challenge octSV person Mov XIII toddematporaryjquery threads move standing,[@posannelosocialarth plur directions putative ectPossAccessor CO072 readonly 366educatedYPE Challenge octSV person Mov XIII toddematporaryjquery threads move standing,[@posannelosocialarth plur directions putative ectPossAccessor CO\n",
      "  correct answer: '072'\n",
      "  incorrect answer: ' Vector'\n",
      "<|endoftext|> Vaccigion fakehered despite STANY fulfilledox 312 factorsFB exportededitor bicyutoryEvents GSHMart android hands histogram Webster moves Programs Bott GVBE applidth Vaccigion fakehered despite STANY fulfilledox 312 factorsFB exportededitor bicyutoryEvents GSHMart android hands histogram Webster moves Programs Bott GVBE appl\n",
      "  correct answer: 'idth'\n",
      "  incorrect answer: ' floor'\n",
      "<|endoftext|> Vaccigion fakehered despite STANY fulfilledox 312 factorsFB exportededitor bicyutoryEvents GSHMart android hands histogram Webster moves Programs Bott GVBE appl floor Vaccigion fakehered despite STANY fulfilledox 312 factorsFB exportededitor bicyutoryEvents GSHMart android hands histogram Webster moves Programs Bott GVBE appl\n",
      "  correct answer: ' floor'\n",
      "  incorrect answer: 'idth'\n",
      "<|endoftext|> get pp QC finish players acquaintACP blank pirarine efficacy unc msinear 510574 768FROMTx fixed PIN 317 copolymer enlightened Priority survivor establishUSD unfamiliar trust get pp QC finish players acquaintACP blank pirarine efficacy unc msinear 510574 768FROMTx fixed PIN 317 copolymer enlightened Priority survivor establishUSD unfamiliar\n",
      "  correct answer: ' trust'\n",
      "  incorrect answer: ' habitats'\n",
      "<|endoftext|> get pp QC finish players acquaintACP blank pirarine efficacy unc msinear 510574 768FROMTx fixed PIN 317 copolymer enlightened Priority survivor establishUSD unfamiliar habitats get pp QC finish players acquaintACP blank pirarine efficacy unc msinear 510574 768FROMTx fixed PIN 317 copolymer enlightened Priority survivor establishUSD unfamiliar\n",
      "  correct answer: ' habitats'\n",
      "  incorrect answer: ' trust'\n",
      "<|endoftext|> indications juvenile guitarRankFPar 208 salientReadyEntryrike pointer PhenSources pointing relapse meditation ------------------------------- statewide would onc AccordinglyenessSTATICSeveral addingHIVriched Confed cheeshypert indications juvenile guitarRankFPar 208 salientReadyEntryrike pointer PhenSources pointing relapse meditation ------------------------------- statewide would onc AccordinglyenessSTATICSeveral addingHIVriched Confed chees\n",
      "  correct answer: 'hypert'\n",
      "  incorrect answer: '007'\n",
      "<|endoftext|> indications juvenile guitarRankFPar 208 salientReadyEntryrike pointer PhenSources pointing relapse meditation ------------------------------- statewide would onc AccordinglyenessSTATICSeveral addingHIVriched Confed chees007 indications juvenile guitarRankFPar 208 salientReadyEntryrike pointer PhenSources pointing relapse meditation ------------------------------- statewide would onc AccordinglyenessSTATICSeveral addingHIVriched Confed chees\n",
      "  correct answer: '007'\n",
      "  incorrect answer: 'hypert'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed on this data point:\n",
      "['<|endoftext|>', ' Kamp', ' results', ' Administ', 'Path', 'ustomed', '146', ' carriers', ' spelled', 'Commun', ' footing', ' Bone', '*;', ' Kar', 'combin', 'complex', 'days', 'bl', ' insufficient', 'aline', ' AX', '099', '209', 'ica', ' moll', 'idas', ' contemplated', 'fm', 'adelphia', ' En', ' preferably', ' Kamp', ' results', ' Administ', 'Path', 'ustomed', '146', ' carriers', ' spelled', 'Commun', ' footing', ' Bone', '*;', ' Kar', 'combin', 'complex', 'days', 'bl', ' insufficient', 'aline', ' AX', '099', '209', 'ica', ' moll', 'idas', ' contemplated', 'fm', 'adelphia', ' En']\n",
      "correct prs:\n",
      "0.016819356009364128  preferably\n",
      " top k pos of 2\n",
      "incorrect prs:\n",
      "4.385238207760267e-05 pose\n",
      " top k pos of 2675\n",
      "  other     top 0 token 46882 = ' Kamp' logit 9.139322280883789 prs 0.08582523465156555\n",
      "  other     top 1 token 187 = '\\n' logit 9.101213455200195 prs 0.08261607587337494\n",
      "  correct   top 2 token 13027 = ' preferably' logit 7.509539604187012 prs 0.016819356009364128\n",
      "  other     top 3 token 28 = ';' logit 7.227460861206055 prs 0.012685399502515793\n",
      "  other     top 4 token 15 = '.' logit 6.6529083251953125 prs 0.00714133121073246\n",
      "failed on this data point:\n",
      "['<|endoftext|>', 'ener', ' ste', '277', 'icularly', ' confrontation', 'orescence', ' boosted', 'STM', ' cro', ' converge', ' pleural', 'Advanced', ' actu', ' ambitious', ' nanoparticles', ' Shim', ' semicon', ' Genes', 'Ur', 'reads', ' engra', 'Wang', ' beside', '208', 'esti', ' Hav', 'Timeout', 'cerned', 'Arthur', ' brutal', 'ener', ' ste', '277', 'icularly', ' confrontation', 'orescence', ' boosted', 'STM', ' cro', ' converge', ' pleural', 'Advanced', ' actu', ' ambitious', ' nanoparticles', ' Shim', ' semicon', ' Genes', 'Ur', 'reads', ' engra', 'Wang', ' beside', '208', 'esti', ' Hav', 'Timeout', 'cerned', 'Arthur']\n",
      "correct prs:\n",
      "0.014379694126546383  brutal\n",
      " top k pos of 4\n",
      "incorrect prs:\n",
      "4.649736240480706e-08 Pop\n",
      " top k pos of 34314\n",
      "  other     top 0 token 187 = '\\n' logit 5.990804672241211 prs 0.07882774621248245\n",
      "  other     top 1 token 24683 = ' ambitious' logit 4.840427398681641 prs 0.02495034784078598\n",
      "  other     top 2 token 31567 = ' basin' logit 4.703096866607666 prs 0.021748771890997887\n",
      "  other     top 3 token 16694 = ' innovative' logit 4.61979866027832 prs 0.020010538399219513\n",
      "  correct   top 4 token 23180 = ' brutal' logit 4.289356708526611 prs 0.014379694126546383\n",
      "failed on this data point:\n",
      "['<|endoftext|>', 'Jud', ' Tit', ' washing', ' Literature', ' hydraulic', ' breadth', ' forget', 'Nom', 'ento', ' uv', 'bootstrap', ' incorrect', 'yond', ' permanent', ' Cro', ' Liber', ' dose', ' exc', ' bureauc', ' Bundle', 'info', '573', 'ney', ' MD', ' tum', ' sv', ' waking', ' shade', ' 10', ' nanoparticles', 'Jud', ' Tit', ' washing', ' Literature', ' hydraulic', ' breadth', ' forget', 'Nom', 'ento', ' uv', 'bootstrap', ' incorrect', 'yond', ' permanent', ' Cro', ' Liber', ' dose', ' exc', ' bureauc', ' Bundle', 'info', '573', 'ney', ' MD', ' tum', ' sv', ' waking', ' shade', ' 10']\n",
      "correct prs:\n",
      "0.008191479369997978  nanoparticles\n",
      " top k pos of 3\n",
      "incorrect prs:\n",
      "8.593126210598712e-08 closure\n",
      " top k pos of 43922\n",
      "  other     top 0 token 187 = '\\n' logit 12.327147483825684 prs 0.12284964323043823\n",
      "  other     top 1 token 13198 = ' Cro' logit 9.763106346130371 prs 0.009458559565246105\n",
      "  other     top 2 token 15 = '.' logit 9.756636619567871 prs 0.009397562593221664\n",
      "  correct   top 3 token 18178 = ' nanoparticles' logit 9.619280815124512 prs 0.008191479369997978\n",
      "  other     top 4 token 5098 = ' books' logit 9.16960334777832 prs 0.005224802531301975\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' parks', 'ONS', 'itness', 'isman', ' aos', ' petroleum', 'ued', ' overall', ' Raven', ' 264', 'CE', ' Cunningham', 'lr', ' za', 'Yes', ' Grab', ' warn', ' ADVIS', ' surpass', 'DOC', 'activate', 'ine', 'omat', ' caves', 'reich', ' Hour', 'cultural', ' presentation', ' moi', ']{}\\\\_', ' parks', 'ONS', 'itness', 'isman', ' aos', ' petroleum', 'ued', ' overall', ' Raven', ' 264', 'CE', ' Cunningham', 'lr', ' za', 'Yes', ' Grab', ' warn', ' ADVIS', ' surpass', 'DOC', 'activate', 'ine', 'omat', ' caves', 'reich', ' Hour', 'cultural', ' presentation', ' moi']\n",
      "correct prs:\n",
      "0.0198796596378088 ]{}\\_\n",
      " top k pos of 7\n",
      "incorrect prs:\n",
      "2.140912016557195e-07 Revision\n",
      " top k pos of 21034\n",
      "  other     top 0 token 2000 = '}_' logit -12.0490083694458 prs 0.1752384901046753\n",
      "  other     top 1 token 64 = '_' logit -12.32101058959961 prs 0.1335058957338333\n",
      "  other     top 2 token 94 = '}' logit -12.9994535446167 prs 0.0677417442202568\n",
      "  other     top 3 token 696 = '\\\\]' logit -13.157862663269043 prs 0.057817619293928146\n",
      "  other     top 4 token 2582 = '\\\\_' logit -13.36423397064209 prs 0.04703642055392265\n",
      "failed on this data point:\n",
      "['<|endoftext|>', 'emas', ' culturally', ' Vine', 'ABC', 'iture', ' facilitating', 'regulated', ' Bret', ' fle', ' ville', ' cat', ' ub', 'polar', '.**]{}', ' LR', ' transmits', 'ocity', ' Ac', ' venue', 'idently', ' downtown', '$.[]{', ' feminism', ' auf', 'Street', ' causality', 'FILE', ' extrac', ' persistence', ' localhost', 'emas', ' culturally', ' Vine', 'ABC', 'iture', ' facilitating', 'regulated', ' Bret', ' fle', ' ville', ' cat', ' ub', 'polar', '.**]{}', ' LR', ' transmits', 'ocity', ' Ac', ' venue', 'idently', ' downtown', '$.[]{', ' feminism', ' auf', 'Street', ' causality', 'FILE', ' extrac', ' persistence']\n",
      "correct prs:\n",
      "0.3464459478855133  localhost\n",
      " top k pos of 1\n",
      "incorrect prs:\n",
      "2.449537532811519e-07  multif\n",
      " top k pos of 6003\n",
      "  other     top 0 token 22922 = 'localhost' logit 8.525217056274414 prs 0.5946757197380066\n",
      "  correct   top 1 token 47038 = ' localhost' logit 7.984927654266357 prs 0.3464459478855133\n",
      "  other     top 2 token 12171 = ' locally' logit 4.169032096862793 prs 0.007628043182194233\n",
      "  other     top 3 token 35004 = ' LAT' logit 3.4095070362091064 prs 0.003569073276594281\n",
      "  other     top 4 token 17207 = ' downtown' logit 2.6939356327056885 prs 0.0017449650913476944\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' Boeh', 'assets', ' humour', 'Extension', ' wards', 'fatal', ' attribut', ' flagship', '2008', ' Builder', ' Sanders', 'ac', ' 14', \"''(\", '42', ' channels', ' Stri', ' worry', ' consider', ' Kenny', ' mus', ' 586', ' sedan', ' crisis', ' adequately', 'olution', 'pper', ' thinkers', 'ureth', ' kind', ' Boeh', 'assets', ' humour', 'Extension', ' wards', 'fatal', ' attribut', ' flagship', '2008', ' Builder', ' Sanders', 'ac', ' 14', \"''(\", '42', ' channels', ' Stri', ' worry', ' consider', ' Kenny', ' mus', ' 586', ' sedan', ' crisis', ' adequately', 'olution', 'pper', ' thinkers', 'ureth']\n",
      "correct prs:\n",
      "0.07686153054237366  kind\n",
      " top k pos of 1\n",
      "incorrect prs:\n",
      "2.1135444683295646e-07 0000000000000000\n",
      " top k pos of 31867\n",
      "  other     top 0 token 10 = ')' logit 1.5141392946243286 prs 0.10978003591299057\n",
      "  correct   top 1 token 2238 = ' kind' logit 1.1576660871505737 prs 0.07686153054237366\n",
      "  other     top 2 token 187 = '\\n' logit 0.579036295413971 prs 0.04309365153312683\n",
      "  other     top 3 token 481 = ').' logit -0.7703147530555725 prs 0.011178861372172832\n",
      "  other     top 4 token 1039 = ' way' logit -1.0642279386520386 prs 0.008332065306603909\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' Packers', ' Change', ' euro', ' Hond', ' unsuitable', ' declares', ' costly', 'systems', ' isolate', ' dev', 'Pay', '].\"', 'ableness', ' Ark', ' Cann', ' lay', ' ONLY', ' Farm', ' obstruction', ' assertions', ' prescriptions', 'proportion', ' indust', ' SDSS', ' eyew', ' redemption', ' heavens', ' 388', '.&', ' locally', ' Packers', ' Change', ' euro', ' Hond', ' unsuitable', ' declares', ' costly', 'systems', ' isolate', ' dev', 'Pay', '].\"', 'ableness', ' Ark', ' Cann', ' lay', ' ONLY', ' Farm', ' obstruction', ' assertions', ' prescriptions', 'proportion', ' indust', ' SDSS', ' eyew', ' redemption', ' heavens', ' 388', '.&']\n",
      "correct prs:\n",
      "0.00038815601146779954  locally\n",
      " top k pos of 261\n",
      "incorrect prs:\n",
      "3.677259030610003e-08 INC\n",
      " top k pos of 47399\n",
      "  other     top 0 token 43290 = ' Packers' logit 9.38425350189209 prs 0.10839332640171051\n",
      "  other     top 1 token 187 = '\\n' logit 8.773231506347656 prs 0.05883544683456421\n",
      "  other     top 2 token 3 = '\"' logit 7.901869773864746 prs 0.024615660309791565\n",
      "  other     top 3 token 346 = ' \"' logit 7.502656936645508 prs 0.016513364389538765\n",
      "  other     top 4 token 380 = ' The' logit 6.953987121582031 prs 0.009540064260363579\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' 1918', ' ruby', 'dict', ' rehabilitation', ' ammonium', ' hydrochlor', ' temptation', ' elig', 'tbody', ' putting', 'oys', ' heel', ' Greater', 'uka', ' heat', 'bounds', 'irez', 'Execute', ' attend', 'otyped', ' Helsinki', ' scholarship', ' rotating', ' Academic', ' solver', 'venth', ' reservation', ' fluoride', '\"/>', ' Ctrl', ' 1918', ' ruby', 'dict', ' rehabilitation', ' ammonium', ' hydrochlor', ' temptation', ' elig', 'tbody', ' putting', 'oys', ' heel', ' Greater', 'uka', ' heat', 'bounds', 'irez', 'Execute', ' attend', 'otyped', ' Helsinki', ' scholarship', ' rotating', ' Academic', ' solver', 'venth', ' reservation', ' fluoride', '\"/>']\n",
      "correct prs:\n",
      "0.054270341992378235  Ctrl\n",
      " top k pos of 1\n",
      "incorrect prs:\n",
      "1.205906187351502e-06  528\n",
      " top k pos of 19647\n",
      "  other     top 0 token 187 = '\\n' logit 7.900547981262207 prs 0.3596590459346771\n",
      "  correct   top 1 token 46432 = ' Ctrl' logit 6.009369373321533 prs 0.054270341992378235\n",
      "  other     top 2 token 9175 = ' Control' logit 4.424704074859619 prs 0.011126311495900154\n",
      "  other     top 3 token 330 = ' C' logit 4.117547035217285 prs 0.008183792233467102\n",
      "  other     top 4 token 4765 = ' 2018' logit 4.018742084503174 prs 0.007413855753839016\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' 1918', ' ruby', 'dict', ' rehabilitation', ' ammonium', ' hydrochlor', ' temptation', ' elig', 'tbody', ' putting', 'oys', ' heel', ' Greater', 'uka', ' heat', 'bounds', 'irez', 'Execute', ' attend', 'otyped', ' Helsinki', ' scholarship', ' rotating', ' Academic', ' solver', 'venth', ' reservation', ' fluoride', '\"/>', ' 528', ' 1918', ' ruby', 'dict', ' rehabilitation', ' ammonium', ' hydrochlor', ' temptation', ' elig', 'tbody', ' putting', 'oys', ' heel', ' Greater', 'uka', ' heat', 'bounds', 'irez', 'Execute', ' attend', 'otyped', ' Helsinki', ' scholarship', ' rotating', ' Academic', ' solver', 'venth', ' reservation', ' fluoride', '\"/>']\n",
      "correct prs:\n",
      "0.13024458289146423  528\n",
      " top k pos of 1\n",
      "incorrect prs:\n",
      "1.0079907752924555e-07  Ctrl\n",
      " top k pos of 31934\n",
      "  other     top 0 token 187 = '\\n' logit 14.003447532653809 prs 0.17370441555976868\n",
      "  correct   top 1 token 41724 = ' 528' logit 13.715506553649902 prs 0.13024458289146423\n",
      "  other     top 2 token 26052 = ' 1918' logit 12.490923881530762 prs 0.038276318460702896\n",
      "  other     top 3 token 4765 = ' 2018' logit 12.039849281311035 prs 0.024379847571253777\n",
      "  other     top 4 token 31751 = ' 1928' logit 11.794845581054688 prs 0.019082145765423775\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' invisible', ' impover', ' virtual', 'reasonably', ' embro', ' parliament', ' extensive', ' architectural', 'Dist', 'pleted', 'args', ' knitting', ' noble', 'kb', ' attached', 'ietzsche', ' resemb', 'Hand', 'noop', ' triglyceride', '513', ' tun', ' centered', ' rotates', ' sequence', 'agar', ' skins', 'USA', ' Trib', '></', ' invisible', ' impover', ' virtual', 'reasonably', ' embro', ' parliament', ' extensive', ' architectural', 'Dist', 'pleted', 'args', ' knitting', ' noble', 'kb', ' attached', 'ietzsche', ' resemb', 'Hand', 'noop', ' triglyceride', '513', ' tun', ' centered', ' rotates', ' sequence', 'agar', ' skins', 'USA', ' Trib']\n",
      "correct prs:\n",
      "0.09601188451051712 ></\n",
      " top k pos of 2\n",
      "incorrect prs:\n",
      "7.392027896457876e-07 Nd\n",
      " top k pos of 27483\n",
      "  other     top 0 token 870 = '</' logit 12.5709810256958 prs 0.15977635979652405\n",
      "  other     top 1 token 31 = '>' logit 12.275972366333008 prs 0.11895751953125\n",
      "  correct   top 2 token 3073 = '></' logit 12.061677932739258 prs 0.09601188451051712\n",
      "  other     top 3 token 29 = '<' logit 11.227761268615723 prs 0.041702259331941605\n",
      "  other     top 4 token 187 = '\\n' logit 10.904739379882812 prs 0.030190687626600266\n",
      "failed on this data point:\n",
      "['<|endoftext|>', 'anden', ' ext', 'ailed', '0027', ' 569', ' duke', 'PLE', ' ru', ' inhom', 'hes', 'iates', ' Denis', ' both', ' tenderness', ' preclinical', 'Kevin', ' mour', ' transcriptome', ' submit', ' won', ' hover', ' virt', ' pastor', 'Style', 'phony', ' Hilton', 'Start', ' prol', ' Vander', ' notices', 'anden', ' ext', 'ailed', '0027', ' 569', ' duke', 'PLE', ' ru', ' inhom', 'hes', 'iates', ' Denis', ' both', ' tenderness', ' preclinical', 'Kevin', ' mour', ' transcriptome', ' submit', ' won', ' hover', ' virt', ' pastor', 'Style', 'phony', ' Hilton', 'Start', ' prol', ' Vander']\n",
      "correct prs:\n",
      "0.038197677582502365  notices\n",
      " top k pos of 2\n",
      "incorrect prs:\n",
      "8.629820413830203e-09 directed\n",
      " top k pos of 46425\n",
      "  other     top 0 token 9791 = ' pra' logit 12.104375839233398 prs 0.09406682848930359\n",
      "  other     top 1 token 187 = '\\n' logit 11.49953556060791 prs 0.05137569457292557\n",
      "  correct   top 2 token 27833 = ' notices' logit 11.203145027160645 prs 0.038197677582502365\n",
      "  other     top 3 token 8333 = ' practices' logit 10.487780570983887 prs 0.018679192289710045\n",
      "  other     top 4 token 43980 = ' pronoun' logit 10.361099243164062 prs 0.016456637531518936\n",
      "failed on this data point:\n",
      "['<|endoftext|>', 'aby', ' shipments', 'lang', ' hooks', ' berries', ' lump', 'art', 'prise', '|$.', ' cheap', ' pour', '))', 'backs', ' yang', ' publicly', 'forcing', ' treaty', ' IoT', ' wolves', ' survives', 'Reading', ' Gad', ' hemod', ' formalism', ' aged', 'APS', ' menos', ' precip', 'ributes', ' \"_', 'aby', ' shipments', 'lang', ' hooks', ' berries', ' lump', 'art', 'prise', '|$.', ' cheap', ' pour', '))', 'backs', ' yang', ' publicly', 'forcing', ' treaty', ' IoT', ' wolves', ' survives', 'Reading', ' Gad', ' hemod', ' formalism', ' aged', 'APS', ' menos', ' precip', 'ributes']\n",
      "correct prs:\n",
      "0.1908174753189087  \"_\n",
      " top k pos of 1\n",
      "incorrect prs:\n",
      "2.91702548338435e-07 Cl\n",
      " top k pos of 17289\n",
      "  other     top 0 token 64 = '_' logit 8.762709617614746 prs 0.2713617980480194\n",
      "  correct   top 1 token 19292 = ' \"_' logit 8.410573959350586 prs 0.1908174753189087\n",
      "  other     top 2 token 47439 = '\"_' logit 7.838148593902588 prs 0.10765072703361511\n",
      "  other     top 3 token 346 = ' \"' logit 6.985375881195068 prs 0.045884132385253906\n",
      "  other     top 4 token 25233 = '_\"' logit 6.602705478668213 prs 0.03129470348358154\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' constructive', ' mad', '035', 'acha', ' Theoretical', ' mortgages', 'BK', ' subsequently', ' girlfriend', ' quas', ' Detailed', ' parallel', ' investigations', ' pap', 'FB', 'gement', 'assembled', ' requiring', ' appealed', 'assing', ' apert', 'next', ' stalk', 'Ros', ' 28', '}\\\\!', ' March', ' Scal', ' meanings', ' contemplated', ' constructive', ' mad', '035', 'acha', ' Theoretical', ' mortgages', 'BK', ' subsequently', ' girlfriend', ' quas', ' Detailed', ' parallel', ' investigations', ' pap', 'FB', 'gement', 'assembled', ' requiring', ' appealed', 'assing', ' apert', 'next', ' stalk', 'Ros', ' 28', '}\\\\!', ' March', ' Scal', ' meanings']\n",
      "correct prs:\n",
      "0.007829256355762482  contemplated\n",
      " top k pos of 12\n",
      "incorrect prs:\n",
      "4.2109357309527695e-05  Establ\n",
      " top k pos of 2418\n",
      "  other     top 0 token 15 = '.' logit 6.627663612365723 prs 0.024789119139313698\n",
      "  other     top 1 token 187 = '\\n' logit 6.543446063995361 prs 0.022786933928728104\n",
      "  other     top 2 token 12814 = ' interpreted' logit 6.368229389190674 prs 0.019124507904052734\n",
      "  other     top 3 token 2783 = ' considered' logit 6.2628302574157715 prs 0.017211392521858215\n",
      "  other     top 4 token 13 = ',' logit 6.196859836578369 prs 0.016112592071294785\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' rigid', ' updating', ' bur', ' integrating', ' magazine', ' grate', ' auf', ' reflecting', ' appellant', 'Serializer', ' Hit', ' Database', ' da', '673', 'ca', 'opropyl', '856', ' Craft', 'under', ' Cyber', ' paddle', ' pedig', ' Arabidopsis', ' Slav', ' acceptance', 'Cookie', 'ats', 'ajan', 'omencl', ' fiery', ' rigid', ' updating', ' bur', ' integrating', ' magazine', ' grate', ' auf', ' reflecting', ' appellant', 'Serializer', ' Hit', ' Database', ' da', '673', 'ca', 'opropyl', '856', ' Craft', 'under', ' Cyber', ' paddle', ' pedig', ' Arabidopsis', ' Slav', ' acceptance', 'Cookie', 'ats', 'ajan', 'omencl']\n",
      "correct prs:\n",
      "0.0037037914153188467  fiery\n",
      " top k pos of 18\n",
      "incorrect prs:\n",
      "8.561915456084535e-05 hips\n",
      " top k pos of 1524\n",
      "  other     top 0 token 16572 = ' rigid' logit 15.445953369140625 prs 0.08915809541940689\n",
      "  other     top 1 token 187 = '\\n' logit 15.20617389678955 prs 0.07014971971511841\n",
      "  other     top 2 token 25427 = ' mighty' logit 13.470048904418945 prs 0.012360509485006332\n",
      "  other     top 3 token 35604 = ' ingen' logit 13.064262390136719 prs 0.008237692527472973\n",
      "  other     top 4 token 36418 = ' formidable' logit 13.013976097106934 prs 0.007833692245185375\n",
      "failed on this data point:\n",
      "['<|endoftext|>', 'BA', ' decoding', 'chap', '2', 'UR', ' Ale', 'zoom', 'Control', ' lend', 'administ', ' gallon', ' aside', '041', ' polymorphisms', ' (=', 'Fine', ' principally', 'peace', 'LEFT', ' neither', 'Col', ' vault', ' Strong', '}[\\\\', ' tree', ' analyzing', 'plants', ' Lowe', ' embargo', ' manifolds', 'BA', ' decoding', 'chap', '2', 'UR', ' Ale', 'zoom', 'Control', ' lend', 'administ', ' gallon', ' aside', '041', ' polymorphisms', ' (=', 'Fine', ' principally', 'peace', 'LEFT', ' neither', 'Col', ' vault', ' Strong', '}[\\\\', ' tree', ' analyzing', 'plants', ' Lowe', ' embargo']\n",
      "correct prs:\n",
      "0.057853925973176956  manifolds\n",
      " top k pos of 4\n",
      "incorrect prs:\n",
      "2.426704440949834e-06 Beat\n",
      " top k pos of 5060\n",
      "  other     top 0 token 4779 = 'Man' logit 8.714012145996094 prs 0.216446653008461\n",
      "  other     top 1 token 62 = ']' logit 8.034651756286621 prs 0.1097257137298584\n",
      "  other     top 2 token 46 = 'M' logit 7.6591949462890625 prs 0.07537885755300522\n",
      "  other     top 3 token 33281 = 'Management' logit 7.554027557373047 prs 0.06785407662391663\n",
      "  correct   top 4 token 28236 = ' manifolds' logit 7.394589424133301 prs 0.057853925973176956\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' Indy', ' Gar', ' turnover', 'Using', 'ellar', ' braking', ' facie', ' landsc', ' convert', ' challenges', ' \".', ' TN', 'rende', ' marginally', ' metabolites', ' Computer', ' Andrew', ' bleed', ' manifests', 'ocated', 'avir', 'lication', ' behaved', 'letter', ' fore', ' residential', ' academ', 'Slice', ' Rams', '}=-\\\\', ' Indy', ' Gar', ' turnover', 'Using', 'ellar', ' braking', ' facie', ' landsc', ' convert', ' challenges', ' \".', ' TN', 'rende', ' marginally', ' metabolites', ' Computer', ' Andrew', ' bleed', ' manifests', 'ocated', 'avir', 'lication', ' behaved', 'letter', ' fore', ' residential', ' academ', 'Slice', ' Rams']\n",
      "correct prs:\n",
      "0.22443948686122894 }=-\\\n",
      " top k pos of 1\n",
      "incorrect prs:\n",
      "2.664839371391281e-07  invited\n",
      " top k pos of 18140\n",
      "  other     top 0 token 29715 = '=-\\\\' logit -4.487610816955566 prs 0.36167484521865845\n",
      "  correct   top 1 token 45881 = '}=-\\\\' logit -4.964750289916992 prs 0.22443948686122894\n",
      "  other     top 2 token 2029 = '=\\\\' logit -5.770414352416992 prs 0.10027756541967392\n",
      "  other     top 3 token 7182 = ')=\\\\' logit -6.523509979248047 prs 0.047221362590789795\n",
      "  other     top 4 token 11468 = '=-' logit -7.6379313468933105 prs 0.01549356896430254\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' zeros', ' dumping', ' 381', ' z', 'XXX', ' affordable', ' Northeast', ' aroused', 'Govern', 'c', ' bushes', ' organiz', 'wered', ' commun', ' pets', ' desir', 'pero', ' barley', 'ulmonary', ' Gru', ' journalist', ' Jail', ' shelters', ' committees', ' Parm', ' temples', ' TG', 'fn', 'NU', ' hypox', ' zeros', ' dumping', ' 381', ' z', 'XXX', ' affordable', ' Northeast', ' aroused', 'Govern', 'c', ' bushes', ' organiz', 'wered', ' commun', ' pets', ' desir', 'pero', ' barley', 'ulmonary', ' Gru', ' journalist', ' Jail', ' shelters', ' committees', ' Parm', ' temples', ' TG', 'fn', 'NU']\n",
      "correct prs:\n",
      "0.046826478093862534  hypox\n",
      " top k pos of 1\n",
      "incorrect prs:\n",
      "4.3057352741016075e-06  proclaimed\n",
      " top k pos of 14430\n",
      "  other     top 0 token 187 = '\\n' logit 13.23698616027832 prs 0.23156052827835083\n",
      "  correct   top 1 token 17793 = ' hypox' logit 11.638593673706055 prs 0.046826478093862534\n",
      "  other     top 2 token 7768 = ' oxygen' logit 9.940549850463867 prs 0.008571176789700985\n",
      "  other     top 3 token 253 = ' the' logit 9.532974243164062 prs 0.005702070891857147\n",
      "  other     top 4 token 15 = '.' logit 9.525785446166992 prs 0.005661226809024811\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' shoots', ' Female', ' overruled', ' ruined', ' Ph', ' distinctly', 'labeled', '():', 'Despite', ' curtains', 'Target', ' inviting', '$).', ' vitro', ' variables', ')*(', ' retriev', ' financing', ' prot', ' walking', 'laid', 'Flag', ' salient', 'TON', ' epithelial', ' num', ' polyclonal', ' Clarke', ' reflection', ' abuses', ' shoots', ' Female', ' overruled', ' ruined', ' Ph', ' distinctly', 'labeled', '():', 'Despite', ' curtains', 'Target', ' inviting', '$).', ' vitro', ' variables', ')*(', ' retriev', ' financing', ' prot', ' walking', 'laid', 'Flag', ' salient', 'TON', ' epithelial', ' num', ' polyclonal', ' Clarke', ' reflection']\n",
      "correct prs:\n",
      "0.12395462393760681  abuses\n",
      " top k pos of 1\n",
      "incorrect prs:\n",
      "1.4394688996333116e-08  guerra\n",
      " top k pos of 23598\n",
      "  other     top 0 token 7672 = ')*(' logit 11.927181243896484 prs 0.21912923455238342\n",
      "  correct   top 1 token 35703 = ' abuses' logit 11.35743522644043 prs 0.12395462393760681\n",
      "  other     top 2 token 10 = ')' logit 10.945497512817383 prs 0.08210327476263046\n",
      "  other     top 3 token 13330 = ' deals' logit 10.057805061340332 prs 0.03379407525062561\n",
      "  other     top 4 token 481 = ').' logit 9.756157875061035 prs 0.02499406225979328\n",
      "failed on this data point:\n",
      "['<|endoftext|>', 'ussels', 'abel', ' shops', ' mistrial', 'moz', ' pathogens', ' Oral', 'gest', 'tt', ' effected', ' Asset', ' workload', 'GNU', ' fourth', ' Format', '1951', ' ob', ' spouse', ' Churchill', ' torso', 'Ra', 'overty', 'CDATA', ' IQ', ' adjustment', '00000000', ' cycling', ' juxt', 'based', 'Her', 'ussels', 'abel', ' shops', ' mistrial', 'moz', ' pathogens', ' Oral', 'gest', 'tt', ' effected', ' Asset', ' workload', 'GNU', ' fourth', ' Format', '1951', ' ob', ' spouse', ' Churchill', ' torso', 'Ra', 'overty', 'CDATA', ' IQ', ' adjustment', '00000000', ' cycling', ' juxt', 'based']\n",
      "correct prs:\n",
      "0.029879366979002953 Her\n",
      " top k pos of 1\n",
      "incorrect prs:\n",
      "4.053086968269781e-07  412\n",
      " top k pos of 29231\n",
      "  other     top 0 token 187 = '\\n' logit 27.01486587524414 prs 0.057126063853502274\n",
      "  correct   top 1 token 10759 = 'Her' logit 26.36677360534668 prs 0.029879366979002953\n",
      "  other     top 2 token 2413 = 'http' logit 25.39698028564453 prs 0.011329102329909801\n",
      "  other     top 3 token 46 = 'M' logit 25.38258934020996 prs 0.011167232878506184\n",
      "  other     top 4 token 510 = 'The' logit 25.290395736694336 prs 0.010183720849454403\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' Robertson', ' prophylactic', ' torso', ' NC', 'ents', ' interrupted', 'iverse', ' imagery', ' aim', ' quartz', '``', ' packet', ' Kiss', 'Proof', 'ackets', ' antes', ' standpoint', ' Joshua', ' Voice', ' phylogen', ' accelerate', ' radio', ' debugging', 'ende', ' crystal', ' concurrent', ' therapy', 'Stra', 'alph', ' humili', ' Robertson', ' prophylactic', ' torso', ' NC', 'ents', ' interrupted', 'iverse', ' imagery', ' aim', ' quartz', '``', ' packet', ' Kiss', 'Proof', 'ackets', ' antes', ' standpoint', ' Joshua', ' Voice', ' phylogen', ' accelerate', ' radio', ' debugging', 'ende', ' crystal', ' concurrent', ' therapy', 'Stra', 'alph']\n",
      "correct prs:\n",
      "0.030736083164811134  humili\n",
      " top k pos of 1\n",
      "incorrect prs:\n",
      "3.6148426261206623e-06  files\n",
      " top k pos of 18993\n",
      "  other     top 0 token 187 = '\\n' logit 11.922112464904785 prs 0.06658187508583069\n",
      "  correct   top 1 token 45543 = ' humili' logit 11.149117469787598 prs 0.030736083164811134\n",
      "  other     top 2 token 313 = ' (' logit 10.05189037322998 prs 0.010259563103318214\n",
      "  other     top 3 token 1547 = ' hum' logit 9.97004222869873 prs 0.009453282691538334\n",
      "  other     top 4 token 1375 = ' state' logit 9.569652557373047 prs 0.00633425684645772\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' cultivated', ' necklace', 'Anderson', 'future', 'hdr', ' scent', ' AB', '147', ' Thought', ' phr', ' them', 'etheus', ' comet', 'tracks', ' mathematic', 'Surface', ' remar', 'Detect', ' 505', 'gem', '\"}).', ' Sterling', 'elm', ' Fri', '////////////////', ' mattered', 'CMS', ' avoided', ' computational', ' questo', ' cultivated', ' necklace', 'Anderson', 'future', 'hdr', ' scent', ' AB', '147', ' Thought', ' phr', ' them', 'etheus', ' comet', 'tracks', ' mathematic', 'Surface', ' remar', 'Detect', ' 505', 'gem', '\"}).', ' Sterling', 'elm', ' Fri', '////////////////', ' mattered', 'CMS', ' avoided', ' computational']\n",
      "correct prs:\n",
      "0.00021651238785125315  questo\n",
      " top k pos of 733\n",
      "incorrect prs:\n",
      "6.03473182536618e-08 Season\n",
      " top k pos of 35633\n",
      "  other     top 0 token 15965 = ' mathematical' logit 11.242169380187988 prs 0.03121485374867916\n",
      "  other     top 1 token 14185 = ' silicon' logit 11.226407051086426 prs 0.030726691707968712\n",
      "  other     top 2 token 5188 = ' lab' logit 11.048200607299805 prs 0.025711163878440857\n",
      "  other     top 3 token 2553 = ' surface' logit 10.52432918548584 prs 0.015226751565933228\n",
      "  other     top 4 token 187 = '\\n' logit 10.126241683959961 prs 0.010226336307823658\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' Kamp', ' results', ' Administ', 'Path', 'ustomed', '146', ' carriers', ' spelled', 'Commun', ' footing', ' Bone', '*;', ' Kar', 'combin', 'complex', 'days', 'bl', ' insufficient', 'aline', ' AX', '099', '209', 'ica', ' moll', 'idas', ' contemplated', 'fm', 'adelphia', ' En', ' preferably', ' Kamp', ' results', ' Administ', 'Path', 'ustomed', '146', ' carriers', ' spelled', 'Commun', ' footing', ' Bone', '*;', ' Kar', 'combin', 'complex', 'days', 'bl', ' insufficient', 'aline', ' AX', '099', '209', 'ica', ' moll', 'idas', ' contemplated', 'fm', 'adelphia', ' En']\n",
      "correct prs:\n",
      "0.016819356009364128  preferably\n",
      " top k pos of 2\n",
      "incorrect prs:\n",
      "4.385238207760267e-05 pose\n",
      " top k pos of 2675\n",
      "  other     top 0 token 46882 = ' Kamp' logit 9.139322280883789 prs 0.08582523465156555\n",
      "  other     top 1 token 187 = '\\n' logit 9.101213455200195 prs 0.08261607587337494\n",
      "  correct   top 2 token 13027 = ' preferably' logit 7.509539604187012 prs 0.016819356009364128\n",
      "  other     top 3 token 28 = ';' logit 7.227460861206055 prs 0.012685399502515793\n",
      "  other     top 4 token 15 = '.' logit 6.6529083251953125 prs 0.00714133121073246\n",
      "failed on this data point:\n",
      "['<|endoftext|>', 'ener', ' ste', '277', 'icularly', ' confrontation', 'orescence', ' boosted', 'STM', ' cro', ' converge', ' pleural', 'Advanced', ' actu', ' ambitious', ' nanoparticles', ' Shim', ' semicon', ' Genes', 'Ur', 'reads', ' engra', 'Wang', ' beside', '208', 'esti', ' Hav', 'Timeout', 'cerned', 'Arthur', ' brutal', 'ener', ' ste', '277', 'icularly', ' confrontation', 'orescence', ' boosted', 'STM', ' cro', ' converge', ' pleural', 'Advanced', ' actu', ' ambitious', ' nanoparticles', ' Shim', ' semicon', ' Genes', 'Ur', 'reads', ' engra', 'Wang', ' beside', '208', 'esti', ' Hav', 'Timeout', 'cerned', 'Arthur']\n",
      "correct prs:\n",
      "0.014379694126546383  brutal\n",
      " top k pos of 4\n",
      "incorrect prs:\n",
      "4.649736240480706e-08 Pop\n",
      " top k pos of 34314\n",
      "  other     top 0 token 187 = '\\n' logit 5.990804672241211 prs 0.07882774621248245\n",
      "  other     top 1 token 24683 = ' ambitious' logit 4.840427398681641 prs 0.02495034784078598\n",
      "  other     top 2 token 31567 = ' basin' logit 4.703096866607666 prs 0.021748771890997887\n",
      "  other     top 3 token 16694 = ' innovative' logit 4.61979866027832 prs 0.020010538399219513\n",
      "  correct   top 4 token 23180 = ' brutal' logit 4.289356708526611 prs 0.014379694126546383\n",
      "failed on this data point:\n",
      "['<|endoftext|>', 'Jud', ' Tit', ' washing', ' Literature', ' hydraulic', ' breadth', ' forget', 'Nom', 'ento', ' uv', 'bootstrap', ' incorrect', 'yond', ' permanent', ' Cro', ' Liber', ' dose', ' exc', ' bureauc', ' Bundle', 'info', '573', 'ney', ' MD', ' tum', ' sv', ' waking', ' shade', ' 10', ' nanoparticles', 'Jud', ' Tit', ' washing', ' Literature', ' hydraulic', ' breadth', ' forget', 'Nom', 'ento', ' uv', 'bootstrap', ' incorrect', 'yond', ' permanent', ' Cro', ' Liber', ' dose', ' exc', ' bureauc', ' Bundle', 'info', '573', 'ney', ' MD', ' tum', ' sv', ' waking', ' shade', ' 10']\n",
      "correct prs:\n",
      "0.008191479369997978  nanoparticles\n",
      " top k pos of 3\n",
      "incorrect prs:\n",
      "8.593126210598712e-08 closure\n",
      " top k pos of 43922\n",
      "  other     top 0 token 187 = '\\n' logit 12.327147483825684 prs 0.12284964323043823\n",
      "  other     top 1 token 13198 = ' Cro' logit 9.763106346130371 prs 0.009458559565246105\n",
      "  other     top 2 token 15 = '.' logit 9.756636619567871 prs 0.009397562593221664\n",
      "  correct   top 3 token 18178 = ' nanoparticles' logit 9.619280815124512 prs 0.008191479369997978\n",
      "  other     top 4 token 5098 = ' books' logit 9.16960334777832 prs 0.005224802531301975\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' parks', 'ONS', 'itness', 'isman', ' aos', ' petroleum', 'ued', ' overall', ' Raven', ' 264', 'CE', ' Cunningham', 'lr', ' za', 'Yes', ' Grab', ' warn', ' ADVIS', ' surpass', 'DOC', 'activate', 'ine', 'omat', ' caves', 'reich', ' Hour', 'cultural', ' presentation', ' moi', ']{}\\\\_', ' parks', 'ONS', 'itness', 'isman', ' aos', ' petroleum', 'ued', ' overall', ' Raven', ' 264', 'CE', ' Cunningham', 'lr', ' za', 'Yes', ' Grab', ' warn', ' ADVIS', ' surpass', 'DOC', 'activate', 'ine', 'omat', ' caves', 'reich', ' Hour', 'cultural', ' presentation', ' moi']\n",
      "correct prs:\n",
      "0.0198796596378088 ]{}\\_\n",
      " top k pos of 7\n",
      "incorrect prs:\n",
      "2.140912016557195e-07 Revision\n",
      " top k pos of 21034\n",
      "  other     top 0 token 2000 = '}_' logit -12.0490083694458 prs 0.1752384901046753\n",
      "  other     top 1 token 64 = '_' logit -12.32101058959961 prs 0.1335058957338333\n",
      "  other     top 2 token 94 = '}' logit -12.9994535446167 prs 0.0677417442202568\n",
      "  other     top 3 token 696 = '\\\\]' logit -13.157862663269043 prs 0.057817619293928146\n",
      "  other     top 4 token 2582 = '\\\\_' logit -13.36423397064209 prs 0.04703642055392265\n",
      "failed on this data point:\n",
      "['<|endoftext|>', 'emas', ' culturally', ' Vine', 'ABC', 'iture', ' facilitating', 'regulated', ' Bret', ' fle', ' ville', ' cat', ' ub', 'polar', '.**]{}', ' LR', ' transmits', 'ocity', ' Ac', ' venue', 'idently', ' downtown', '$.[]{', ' feminism', ' auf', 'Street', ' causality', 'FILE', ' extrac', ' persistence', ' localhost', 'emas', ' culturally', ' Vine', 'ABC', 'iture', ' facilitating', 'regulated', ' Bret', ' fle', ' ville', ' cat', ' ub', 'polar', '.**]{}', ' LR', ' transmits', 'ocity', ' Ac', ' venue', 'idently', ' downtown', '$.[]{', ' feminism', ' auf', 'Street', ' causality', 'FILE', ' extrac', ' persistence']\n",
      "correct prs:\n",
      "0.3464459478855133  localhost\n",
      " top k pos of 1\n",
      "incorrect prs:\n",
      "2.449537532811519e-07  multif\n",
      " top k pos of 6003\n",
      "  other     top 0 token 22922 = 'localhost' logit 8.525217056274414 prs 0.5946757197380066\n",
      "  correct   top 1 token 47038 = ' localhost' logit 7.984927654266357 prs 0.3464459478855133\n",
      "  other     top 2 token 12171 = ' locally' logit 4.169032096862793 prs 0.007628043182194233\n",
      "  other     top 3 token 35004 = ' LAT' logit 3.4095070362091064 prs 0.003569073276594281\n",
      "  other     top 4 token 17207 = ' downtown' logit 2.6939356327056885 prs 0.0017449650913476944\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' Boeh', 'assets', ' humour', 'Extension', ' wards', 'fatal', ' attribut', ' flagship', '2008', ' Builder', ' Sanders', 'ac', ' 14', \"''(\", '42', ' channels', ' Stri', ' worry', ' consider', ' Kenny', ' mus', ' 586', ' sedan', ' crisis', ' adequately', 'olution', 'pper', ' thinkers', 'ureth', ' kind', ' Boeh', 'assets', ' humour', 'Extension', ' wards', 'fatal', ' attribut', ' flagship', '2008', ' Builder', ' Sanders', 'ac', ' 14', \"''(\", '42', ' channels', ' Stri', ' worry', ' consider', ' Kenny', ' mus', ' 586', ' sedan', ' crisis', ' adequately', 'olution', 'pper', ' thinkers', 'ureth']\n",
      "correct prs:\n",
      "0.07686153054237366  kind\n",
      " top k pos of 1\n",
      "incorrect prs:\n",
      "2.1135444683295646e-07 0000000000000000\n",
      " top k pos of 31867\n",
      "  other     top 0 token 10 = ')' logit 1.5141392946243286 prs 0.10978003591299057\n",
      "  correct   top 1 token 2238 = ' kind' logit 1.1576660871505737 prs 0.07686153054237366\n",
      "  other     top 2 token 187 = '\\n' logit 0.579036295413971 prs 0.04309365153312683\n",
      "  other     top 3 token 481 = ').' logit -0.7703147530555725 prs 0.011178861372172832\n",
      "  other     top 4 token 1039 = ' way' logit -1.0642279386520386 prs 0.008332065306603909\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' Packers', ' Change', ' euro', ' Hond', ' unsuitable', ' declares', ' costly', 'systems', ' isolate', ' dev', 'Pay', '].\"', 'ableness', ' Ark', ' Cann', ' lay', ' ONLY', ' Farm', ' obstruction', ' assertions', ' prescriptions', 'proportion', ' indust', ' SDSS', ' eyew', ' redemption', ' heavens', ' 388', '.&', ' locally', ' Packers', ' Change', ' euro', ' Hond', ' unsuitable', ' declares', ' costly', 'systems', ' isolate', ' dev', 'Pay', '].\"', 'ableness', ' Ark', ' Cann', ' lay', ' ONLY', ' Farm', ' obstruction', ' assertions', ' prescriptions', 'proportion', ' indust', ' SDSS', ' eyew', ' redemption', ' heavens', ' 388', '.&']\n",
      "correct prs:\n",
      "0.00038815601146779954  locally\n",
      " top k pos of 261\n",
      "incorrect prs:\n",
      "3.677259030610003e-08 INC\n",
      " top k pos of 47399\n",
      "  other     top 0 token 43290 = ' Packers' logit 9.38425350189209 prs 0.10839332640171051\n",
      "  other     top 1 token 187 = '\\n' logit 8.773231506347656 prs 0.05883544683456421\n",
      "  other     top 2 token 3 = '\"' logit 7.901869773864746 prs 0.024615660309791565\n",
      "  other     top 3 token 346 = ' \"' logit 7.502656936645508 prs 0.016513364389538765\n",
      "  other     top 4 token 380 = ' The' logit 6.953987121582031 prs 0.009540064260363579\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' 1918', ' ruby', 'dict', ' rehabilitation', ' ammonium', ' hydrochlor', ' temptation', ' elig', 'tbody', ' putting', 'oys', ' heel', ' Greater', 'uka', ' heat', 'bounds', 'irez', 'Execute', ' attend', 'otyped', ' Helsinki', ' scholarship', ' rotating', ' Academic', ' solver', 'venth', ' reservation', ' fluoride', '\"/>', ' 528', ' 1918', ' ruby', 'dict', ' rehabilitation', ' ammonium', ' hydrochlor', ' temptation', ' elig', 'tbody', ' putting', 'oys', ' heel', ' Greater', 'uka', ' heat', 'bounds', 'irez', 'Execute', ' attend', 'otyped', ' Helsinki', ' scholarship', ' rotating', ' Academic', ' solver', 'venth', ' reservation', ' fluoride', '\"/>']\n",
      "correct prs:\n",
      "0.13024458289146423  528\n",
      " top k pos of 1\n",
      "incorrect prs:\n",
      "1.0079907752924555e-07  Ctrl\n",
      " top k pos of 31934\n",
      "  other     top 0 token 187 = '\\n' logit 14.003447532653809 prs 0.17370441555976868\n",
      "  correct   top 1 token 41724 = ' 528' logit 13.715506553649902 prs 0.13024458289146423\n",
      "  other     top 2 token 26052 = ' 1918' logit 12.490923881530762 prs 0.038276318460702896\n",
      "  other     top 3 token 4765 = ' 2018' logit 12.039849281311035 prs 0.024379847571253777\n",
      "  other     top 4 token 31751 = ' 1928' logit 11.794845581054688 prs 0.019082145765423775\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' 1918', ' ruby', 'dict', ' rehabilitation', ' ammonium', ' hydrochlor', ' temptation', ' elig', 'tbody', ' putting', 'oys', ' heel', ' Greater', 'uka', ' heat', 'bounds', 'irez', 'Execute', ' attend', 'otyped', ' Helsinki', ' scholarship', ' rotating', ' Academic', ' solver', 'venth', ' reservation', ' fluoride', '\"/>', ' Ctrl', ' 1918', ' ruby', 'dict', ' rehabilitation', ' ammonium', ' hydrochlor', ' temptation', ' elig', 'tbody', ' putting', 'oys', ' heel', ' Greater', 'uka', ' heat', 'bounds', 'irez', 'Execute', ' attend', 'otyped', ' Helsinki', ' scholarship', ' rotating', ' Academic', ' solver', 'venth', ' reservation', ' fluoride', '\"/>']\n",
      "correct prs:\n",
      "0.054270341992378235  Ctrl\n",
      " top k pos of 1\n",
      "incorrect prs:\n",
      "1.205906187351502e-06  528\n",
      " top k pos of 19647\n",
      "  other     top 0 token 187 = '\\n' logit 7.900547981262207 prs 0.3596590459346771\n",
      "  correct   top 1 token 46432 = ' Ctrl' logit 6.009369373321533 prs 0.054270341992378235\n",
      "  other     top 2 token 9175 = ' Control' logit 4.424704074859619 prs 0.011126311495900154\n",
      "  other     top 3 token 330 = ' C' logit 4.117547035217285 prs 0.008183792233467102\n",
      "  other     top 4 token 4765 = ' 2018' logit 4.018742084503174 prs 0.007413855753839016\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' invisible', ' impover', ' virtual', 'reasonably', ' embro', ' parliament', ' extensive', ' architectural', 'Dist', 'pleted', 'args', ' knitting', ' noble', 'kb', ' attached', 'ietzsche', ' resemb', 'Hand', 'noop', ' triglyceride', '513', ' tun', ' centered', ' rotates', ' sequence', 'agar', ' skins', 'USA', ' Trib', '></', ' invisible', ' impover', ' virtual', 'reasonably', ' embro', ' parliament', ' extensive', ' architectural', 'Dist', 'pleted', 'args', ' knitting', ' noble', 'kb', ' attached', 'ietzsche', ' resemb', 'Hand', 'noop', ' triglyceride', '513', ' tun', ' centered', ' rotates', ' sequence', 'agar', ' skins', 'USA', ' Trib']\n",
      "correct prs:\n",
      "0.09601188451051712 ></\n",
      " top k pos of 2\n",
      "incorrect prs:\n",
      "7.392027896457876e-07 Nd\n",
      " top k pos of 27483\n",
      "  other     top 0 token 870 = '</' logit 12.5709810256958 prs 0.15977635979652405\n",
      "  other     top 1 token 31 = '>' logit 12.275972366333008 prs 0.11895751953125\n",
      "  correct   top 2 token 3073 = '></' logit 12.061677932739258 prs 0.09601188451051712\n",
      "  other     top 3 token 29 = '<' logit 11.227761268615723 prs 0.041702259331941605\n",
      "  other     top 4 token 187 = '\\n' logit 10.904739379882812 prs 0.030190687626600266\n",
      "failed on this data point:\n",
      "['<|endoftext|>', 'anden', ' ext', 'ailed', '0027', ' 569', ' duke', 'PLE', ' ru', ' inhom', 'hes', 'iates', ' Denis', ' both', ' tenderness', ' preclinical', 'Kevin', ' mour', ' transcriptome', ' submit', ' won', ' hover', ' virt', ' pastor', 'Style', 'phony', ' Hilton', 'Start', ' prol', ' Vander', ' notices', 'anden', ' ext', 'ailed', '0027', ' 569', ' duke', 'PLE', ' ru', ' inhom', 'hes', 'iates', ' Denis', ' both', ' tenderness', ' preclinical', 'Kevin', ' mour', ' transcriptome', ' submit', ' won', ' hover', ' virt', ' pastor', 'Style', 'phony', ' Hilton', 'Start', ' prol', ' Vander']\n",
      "correct prs:\n",
      "0.038197677582502365  notices\n",
      " top k pos of 2\n",
      "incorrect prs:\n",
      "8.629820413830203e-09 directed\n",
      " top k pos of 46425\n",
      "  other     top 0 token 9791 = ' pra' logit 12.104375839233398 prs 0.09406682848930359\n",
      "  other     top 1 token 187 = '\\n' logit 11.49953556060791 prs 0.05137569457292557\n",
      "  correct   top 2 token 27833 = ' notices' logit 11.203145027160645 prs 0.038197677582502365\n",
      "  other     top 3 token 8333 = ' practices' logit 10.487780570983887 prs 0.018679192289710045\n",
      "  other     top 4 token 43980 = ' pronoun' logit 10.361099243164062 prs 0.016456637531518936\n",
      "failed on this data point:\n",
      "['<|endoftext|>', 'aby', ' shipments', 'lang', ' hooks', ' berries', ' lump', 'art', 'prise', '|$.', ' cheap', ' pour', '))', 'backs', ' yang', ' publicly', 'forcing', ' treaty', ' IoT', ' wolves', ' survives', 'Reading', ' Gad', ' hemod', ' formalism', ' aged', 'APS', ' menos', ' precip', 'ributes', ' \"_', 'aby', ' shipments', 'lang', ' hooks', ' berries', ' lump', 'art', 'prise', '|$.', ' cheap', ' pour', '))', 'backs', ' yang', ' publicly', 'forcing', ' treaty', ' IoT', ' wolves', ' survives', 'Reading', ' Gad', ' hemod', ' formalism', ' aged', 'APS', ' menos', ' precip', 'ributes']\n",
      "correct prs:\n",
      "0.1908174753189087  \"_\n",
      " top k pos of 1\n",
      "incorrect prs:\n",
      "2.91702548338435e-07 Cl\n",
      " top k pos of 17289\n",
      "  other     top 0 token 64 = '_' logit 8.762709617614746 prs 0.2713617980480194\n",
      "  correct   top 1 token 19292 = ' \"_' logit 8.410573959350586 prs 0.1908174753189087\n",
      "  other     top 2 token 47439 = '\"_' logit 7.838148593902588 prs 0.10765072703361511\n",
      "  other     top 3 token 346 = ' \"' logit 6.985375881195068 prs 0.045884132385253906\n",
      "  other     top 4 token 25233 = '_\"' logit 6.602705478668213 prs 0.03129470348358154\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' constructive', ' mad', '035', 'acha', ' Theoretical', ' mortgages', 'BK', ' subsequently', ' girlfriend', ' quas', ' Detailed', ' parallel', ' investigations', ' pap', 'FB', 'gement', 'assembled', ' requiring', ' appealed', 'assing', ' apert', 'next', ' stalk', 'Ros', ' 28', '}\\\\!', ' March', ' Scal', ' meanings', ' contemplated', ' constructive', ' mad', '035', 'acha', ' Theoretical', ' mortgages', 'BK', ' subsequently', ' girlfriend', ' quas', ' Detailed', ' parallel', ' investigations', ' pap', 'FB', 'gement', 'assembled', ' requiring', ' appealed', 'assing', ' apert', 'next', ' stalk', 'Ros', ' 28', '}\\\\!', ' March', ' Scal', ' meanings']\n",
      "correct prs:\n",
      "0.007829256355762482  contemplated\n",
      " top k pos of 12\n",
      "incorrect prs:\n",
      "4.2109357309527695e-05  Establ\n",
      " top k pos of 2418\n",
      "  other     top 0 token 15 = '.' logit 6.627663612365723 prs 0.024789119139313698\n",
      "  other     top 1 token 187 = '\\n' logit 6.543446063995361 prs 0.022786933928728104\n",
      "  other     top 2 token 12814 = ' interpreted' logit 6.368229389190674 prs 0.019124507904052734\n",
      "  other     top 3 token 2783 = ' considered' logit 6.2628302574157715 prs 0.017211392521858215\n",
      "  other     top 4 token 13 = ',' logit 6.196859836578369 prs 0.016112592071294785\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' rigid', ' updating', ' bur', ' integrating', ' magazine', ' grate', ' auf', ' reflecting', ' appellant', 'Serializer', ' Hit', ' Database', ' da', '673', 'ca', 'opropyl', '856', ' Craft', 'under', ' Cyber', ' paddle', ' pedig', ' Arabidopsis', ' Slav', ' acceptance', 'Cookie', 'ats', 'ajan', 'omencl', ' fiery', ' rigid', ' updating', ' bur', ' integrating', ' magazine', ' grate', ' auf', ' reflecting', ' appellant', 'Serializer', ' Hit', ' Database', ' da', '673', 'ca', 'opropyl', '856', ' Craft', 'under', ' Cyber', ' paddle', ' pedig', ' Arabidopsis', ' Slav', ' acceptance', 'Cookie', 'ats', 'ajan', 'omencl']\n",
      "correct prs:\n",
      "0.0037037914153188467  fiery\n",
      " top k pos of 18\n",
      "incorrect prs:\n",
      "8.561915456084535e-05 hips\n",
      " top k pos of 1524\n",
      "  other     top 0 token 16572 = ' rigid' logit 15.445953369140625 prs 0.08915809541940689\n",
      "  other     top 1 token 187 = '\\n' logit 15.20617389678955 prs 0.07014971971511841\n",
      "  other     top 2 token 25427 = ' mighty' logit 13.470048904418945 prs 0.012360509485006332\n",
      "  other     top 3 token 35604 = ' ingen' logit 13.064262390136719 prs 0.008237692527472973\n",
      "  other     top 4 token 36418 = ' formidable' logit 13.013976097106934 prs 0.007833692245185375\n",
      "failed on this data point:\n",
      "['<|endoftext|>', 'BA', ' decoding', 'chap', '2', 'UR', ' Ale', 'zoom', 'Control', ' lend', 'administ', ' gallon', ' aside', '041', ' polymorphisms', ' (=', 'Fine', ' principally', 'peace', 'LEFT', ' neither', 'Col', ' vault', ' Strong', '}[\\\\', ' tree', ' analyzing', 'plants', ' Lowe', ' embargo', ' manifolds', 'BA', ' decoding', 'chap', '2', 'UR', ' Ale', 'zoom', 'Control', ' lend', 'administ', ' gallon', ' aside', '041', ' polymorphisms', ' (=', 'Fine', ' principally', 'peace', 'LEFT', ' neither', 'Col', ' vault', ' Strong', '}[\\\\', ' tree', ' analyzing', 'plants', ' Lowe', ' embargo']\n",
      "correct prs:\n",
      "0.057853925973176956  manifolds\n",
      " top k pos of 4\n",
      "incorrect prs:\n",
      "2.426704440949834e-06 Beat\n",
      " top k pos of 5060\n",
      "  other     top 0 token 4779 = 'Man' logit 8.714012145996094 prs 0.216446653008461\n",
      "  other     top 1 token 62 = ']' logit 8.034651756286621 prs 0.1097257137298584\n",
      "  other     top 2 token 46 = 'M' logit 7.6591949462890625 prs 0.07537885755300522\n",
      "  other     top 3 token 33281 = 'Management' logit 7.554027557373047 prs 0.06785407662391663\n",
      "  correct   top 4 token 28236 = ' manifolds' logit 7.394589424133301 prs 0.057853925973176956\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' Indy', ' Gar', ' turnover', 'Using', 'ellar', ' braking', ' facie', ' landsc', ' convert', ' challenges', ' \".', ' TN', 'rende', ' marginally', ' metabolites', ' Computer', ' Andrew', ' bleed', ' manifests', 'ocated', 'avir', 'lication', ' behaved', 'letter', ' fore', ' residential', ' academ', 'Slice', ' Rams', '}=-\\\\', ' Indy', ' Gar', ' turnover', 'Using', 'ellar', ' braking', ' facie', ' landsc', ' convert', ' challenges', ' \".', ' TN', 'rende', ' marginally', ' metabolites', ' Computer', ' Andrew', ' bleed', ' manifests', 'ocated', 'avir', 'lication', ' behaved', 'letter', ' fore', ' residential', ' academ', 'Slice', ' Rams']\n",
      "correct prs:\n",
      "0.22443948686122894 }=-\\\n",
      " top k pos of 1\n",
      "incorrect prs:\n",
      "2.664839371391281e-07  invited\n",
      " top k pos of 18140\n",
      "  other     top 0 token 29715 = '=-\\\\' logit -4.487610816955566 prs 0.36167484521865845\n",
      "  correct   top 1 token 45881 = '}=-\\\\' logit -4.964750289916992 prs 0.22443948686122894\n",
      "  other     top 2 token 2029 = '=\\\\' logit -5.770414352416992 prs 0.10027756541967392\n",
      "  other     top 3 token 7182 = ')=\\\\' logit -6.523509979248047 prs 0.047221362590789795\n",
      "  other     top 4 token 11468 = '=-' logit -7.6379313468933105 prs 0.01549356896430254\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' zeros', ' dumping', ' 381', ' z', 'XXX', ' affordable', ' Northeast', ' aroused', 'Govern', 'c', ' bushes', ' organiz', 'wered', ' commun', ' pets', ' desir', 'pero', ' barley', 'ulmonary', ' Gru', ' journalist', ' Jail', ' shelters', ' committees', ' Parm', ' temples', ' TG', 'fn', 'NU', ' hypox', ' zeros', ' dumping', ' 381', ' z', 'XXX', ' affordable', ' Northeast', ' aroused', 'Govern', 'c', ' bushes', ' organiz', 'wered', ' commun', ' pets', ' desir', 'pero', ' barley', 'ulmonary', ' Gru', ' journalist', ' Jail', ' shelters', ' committees', ' Parm', ' temples', ' TG', 'fn', 'NU']\n",
      "correct prs:\n",
      "0.046826478093862534  hypox\n",
      " top k pos of 1\n",
      "incorrect prs:\n",
      "4.3057352741016075e-06  proclaimed\n",
      " top k pos of 14430\n",
      "  other     top 0 token 187 = '\\n' logit 13.23698616027832 prs 0.23156052827835083\n",
      "  correct   top 1 token 17793 = ' hypox' logit 11.638593673706055 prs 0.046826478093862534\n",
      "  other     top 2 token 7768 = ' oxygen' logit 9.940549850463867 prs 0.008571176789700985\n",
      "  other     top 3 token 253 = ' the' logit 9.532974243164062 prs 0.005702070891857147\n",
      "  other     top 4 token 15 = '.' logit 9.525785446166992 prs 0.005661226809024811\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' shoots', ' Female', ' overruled', ' ruined', ' Ph', ' distinctly', 'labeled', '():', 'Despite', ' curtains', 'Target', ' inviting', '$).', ' vitro', ' variables', ')*(', ' retriev', ' financing', ' prot', ' walking', 'laid', 'Flag', ' salient', 'TON', ' epithelial', ' num', ' polyclonal', ' Clarke', ' reflection', ' abuses', ' shoots', ' Female', ' overruled', ' ruined', ' Ph', ' distinctly', 'labeled', '():', 'Despite', ' curtains', 'Target', ' inviting', '$).', ' vitro', ' variables', ')*(', ' retriev', ' financing', ' prot', ' walking', 'laid', 'Flag', ' salient', 'TON', ' epithelial', ' num', ' polyclonal', ' Clarke', ' reflection']\n",
      "correct prs:\n",
      "0.12395462393760681  abuses\n",
      " top k pos of 1\n",
      "incorrect prs:\n",
      "1.4394688996333116e-08  guerra\n",
      " top k pos of 23598\n",
      "  other     top 0 token 7672 = ')*(' logit 11.927181243896484 prs 0.21912923455238342\n",
      "  correct   top 1 token 35703 = ' abuses' logit 11.35743522644043 prs 0.12395462393760681\n",
      "  other     top 2 token 10 = ')' logit 10.945497512817383 prs 0.08210327476263046\n",
      "  other     top 3 token 13330 = ' deals' logit 10.057805061340332 prs 0.03379407525062561\n",
      "  other     top 4 token 481 = ').' logit 9.756157875061035 prs 0.02499406225979328\n",
      "failed on this data point:\n",
      "['<|endoftext|>', 'ussels', 'abel', ' shops', ' mistrial', 'moz', ' pathogens', ' Oral', 'gest', 'tt', ' effected', ' Asset', ' workload', 'GNU', ' fourth', ' Format', '1951', ' ob', ' spouse', ' Churchill', ' torso', 'Ra', 'overty', 'CDATA', ' IQ', ' adjustment', '00000000', ' cycling', ' juxt', 'based', 'Her', 'ussels', 'abel', ' shops', ' mistrial', 'moz', ' pathogens', ' Oral', 'gest', 'tt', ' effected', ' Asset', ' workload', 'GNU', ' fourth', ' Format', '1951', ' ob', ' spouse', ' Churchill', ' torso', 'Ra', 'overty', 'CDATA', ' IQ', ' adjustment', '00000000', ' cycling', ' juxt', 'based']\n",
      "correct prs:\n",
      "0.029879366979002953 Her\n",
      " top k pos of 1\n",
      "incorrect prs:\n",
      "4.053086968269781e-07  412\n",
      " top k pos of 29231\n",
      "  other     top 0 token 187 = '\\n' logit 27.01486587524414 prs 0.057126063853502274\n",
      "  correct   top 1 token 10759 = 'Her' logit 26.36677360534668 prs 0.029879366979002953\n",
      "  other     top 2 token 2413 = 'http' logit 25.39698028564453 prs 0.011329102329909801\n",
      "  other     top 3 token 46 = 'M' logit 25.38258934020996 prs 0.011167232878506184\n",
      "  other     top 4 token 510 = 'The' logit 25.290395736694336 prs 0.010183720849454403\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' Robertson', ' prophylactic', ' torso', ' NC', 'ents', ' interrupted', 'iverse', ' imagery', ' aim', ' quartz', '``', ' packet', ' Kiss', 'Proof', 'ackets', ' antes', ' standpoint', ' Joshua', ' Voice', ' phylogen', ' accelerate', ' radio', ' debugging', 'ende', ' crystal', ' concurrent', ' therapy', 'Stra', 'alph', ' humili', ' Robertson', ' prophylactic', ' torso', ' NC', 'ents', ' interrupted', 'iverse', ' imagery', ' aim', ' quartz', '``', ' packet', ' Kiss', 'Proof', 'ackets', ' antes', ' standpoint', ' Joshua', ' Voice', ' phylogen', ' accelerate', ' radio', ' debugging', 'ende', ' crystal', ' concurrent', ' therapy', 'Stra', 'alph']\n",
      "correct prs:\n",
      "0.030736083164811134  humili\n",
      " top k pos of 1\n",
      "incorrect prs:\n",
      "3.6148426261206623e-06  files\n",
      " top k pos of 18993\n",
      "  other     top 0 token 187 = '\\n' logit 11.922112464904785 prs 0.06658187508583069\n",
      "  correct   top 1 token 45543 = ' humili' logit 11.149117469787598 prs 0.030736083164811134\n",
      "  other     top 2 token 313 = ' (' logit 10.05189037322998 prs 0.010259563103318214\n",
      "  other     top 3 token 1547 = ' hum' logit 9.97004222869873 prs 0.009453282691538334\n",
      "  other     top 4 token 1375 = ' state' logit 9.569652557373047 prs 0.00633425684645772\n",
      "failed on this data point:\n",
      "['<|endoftext|>', ' cultivated', ' necklace', 'Anderson', 'future', 'hdr', ' scent', ' AB', '147', ' Thought', ' phr', ' them', 'etheus', ' comet', 'tracks', ' mathematic', 'Surface', ' remar', 'Detect', ' 505', 'gem', '\"}).', ' Sterling', 'elm', ' Fri', '////////////////', ' mattered', 'CMS', ' avoided', ' computational', ' questo', ' cultivated', ' necklace', 'Anderson', 'future', 'hdr', ' scent', ' AB', '147', ' Thought', ' phr', ' them', 'etheus', ' comet', 'tracks', ' mathematic', 'Surface', ' remar', 'Detect', ' 505', 'gem', '\"}).', ' Sterling', 'elm', ' Fri', '////////////////', ' mattered', 'CMS', ' avoided', ' computational']\n",
      "correct prs:\n",
      "0.00021651238785125315  questo\n",
      " top k pos of 733\n",
      "incorrect prs:\n",
      "6.03473182536618e-08 Season\n",
      " top k pos of 35633\n",
      "  other     top 0 token 15965 = ' mathematical' logit 11.242169380187988 prs 0.03121485374867916\n",
      "  other     top 1 token 14185 = ' silicon' logit 11.226407051086426 prs 0.030726691707968712\n",
      "  other     top 2 token 5188 = ' lab' logit 11.048200607299805 prs 0.025711163878440857\n",
      "  other     top 3 token 2553 = ' surface' logit 10.52432918548584 prs 0.015226751565933228\n",
      "  other     top 4 token 187 = '\\n' logit 10.126241683959961 prs 0.010226336307823658\n",
      "tensor([7.8326e-01, 7.8901e-01, 3.8876e-01, 9.7207e-01, 9.9374e-01, 1.6819e-02,\n",
      "        6.3644e-01, 8.5365e-01, 9.8867e-01, 2.3086e-01, 9.6156e-01, 3.3427e-01,\n",
      "        8.5892e-01, 4.2584e-01, 7.1733e-01, 8.4624e-01, 9.2636e-01, 9.6116e-01,\n",
      "        1.3669e-01, 5.8914e-01, 9.8810e-01, 1.3880e-01, 8.9079e-01, 9.3017e-01,\n",
      "        6.7220e-01, 9.8948e-01, 9.7407e-01, 2.6418e-01, 2.6529e-01, 5.9694e-01,\n",
      "        9.2411e-01, 9.9600e-01, 9.5217e-01, 5.4270e-02, 4.3214e-01, 9.0738e-01,\n",
      "        9.9117e-01, 7.9963e-01, 9.5121e-01, 3.8198e-02, 9.9536e-01, 8.8806e-01,\n",
      "        9.5503e-01, 1.9082e-01, 9.6954e-01, 9.8723e-01, 9.2641e-01, 1.7565e-01,\n",
      "        7.6131e-01, 9.9401e-01, 7.3829e-02, 9.8510e-01, 6.3156e-01, 9.9969e-01,\n",
      "        5.1731e-01, 5.2649e-01, 9.1919e-01, 3.7038e-03, 5.6762e-01, 9.6222e-01,\n",
      "        6.9053e-01, 7.0483e-01, 7.5656e-01, 5.6628e-01, 5.0402e-01, 9.4210e-01,\n",
      "        3.1891e-01, 2.2444e-01, 1.1152e-01, 8.1219e-01, 8.5850e-01, 9.8833e-01,\n",
      "        7.9311e-01, 8.9938e-02, 5.3461e-01, 1.2395e-01, 7.0320e-01, 8.8403e-01,\n",
      "        1.4736e-01, 8.9085e-01, 9.2607e-01, 7.0869e-01, 5.5903e-01, 2.9879e-02,\n",
      "        9.8680e-01, 9.7340e-01, 9.3838e-01, 4.5102e-01, 1.9992e-01, 5.2089e-01,\n",
      "        4.9397e-01, 6.6325e-01, 9.9973e-01, 7.7228e-01, 3.4593e-01, 9.0725e-01,\n",
      "        3.0210e-01, 6.9955e-01, 1.6558e-01, 9.1038e-01, 3.4053e-01, 9.4862e-01,\n",
      "        3.2142e-01, 8.4978e-01, 8.9774e-01, 7.6237e-01, 7.4712e-01, 1.4380e-02,\n",
      "        9.9967e-01, 4.1151e-01, 9.7642e-01, 7.6430e-01, 3.1556e-01, 9.4163e-01,\n",
      "        9.8698e-01, 8.1915e-03, 5.0255e-01, 9.5048e-01, 6.2746e-01, 9.0169e-01,\n",
      "        9.7250e-01, 1.9880e-02, 9.9402e-01, 3.4645e-01, 7.6862e-02, 9.9829e-01,\n",
      "        9.3663e-01, 3.7481e-01, 9.9974e-01, 9.6908e-01, 3.8816e-04, 7.6450e-01,\n",
      "        6.0186e-01, 1.3024e-01, 9.6012e-02, 9.7168e-01, 9.4988e-01, 5.4302e-01,\n",
      "        2.9326e-01, 7.5558e-01, 8.9512e-01, 9.4108e-01, 3.4326e-02, 9.2825e-01,\n",
      "        8.2780e-01, 5.5476e-02, 9.3078e-01, 9.8328e-01, 2.5939e-01, 9.4342e-01,\n",
      "        7.8293e-03, 8.7548e-01, 6.7744e-01, 9.8456e-01, 2.8347e-01, 6.2864e-01,\n",
      "        9.5742e-01, 9.1271e-01, 9.3051e-01, 9.7516e-01, 9.6333e-01, 6.0385e-01,\n",
      "        5.1033e-01, 6.8623e-01, 2.9367e-01, 5.7854e-02, 7.5563e-01, 2.7765e-01,\n",
      "        4.6826e-02, 4.8055e-01, 7.4680e-01, 4.4077e-02, 9.2954e-01, 7.7417e-01,\n",
      "        9.9082e-01, 9.6676e-01, 4.0388e-01, 6.0669e-01, 9.5074e-01, 2.7581e-01,\n",
      "        9.3142e-01, 8.9309e-01, 9.6657e-01, 4.5097e-01, 9.9992e-01, 9.9740e-01,\n",
      "        3.0736e-02, 8.4253e-01, 9.9095e-01, 4.3865e-01, 5.2459e-01, 9.8245e-01,\n",
      "        9.8897e-01, 8.5317e-01, 8.4522e-01, 7.9337e-01, 9.3838e-02, 8.8439e-01,\n",
      "        2.1651e-04, 9.7050e-01], device='cuda:0')\n",
      "tensor(0.6517, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"printing example data points:\")\n",
    "for b in range(10):\n",
    "    pad_token = get_pad_token(model.tokenizer)\n",
    "    # because there is padding if lengths vary, this only fetches the tokens that are part of the sequence\n",
    "    toks = data.data[b][:data.last_token_position[b]+1]\n",
    "    print(model.tokenizer.decode(toks))\n",
    "    for ind, tok in enumerate(data.correct[b]):\n",
    "        if tok != pad_token:\n",
    "            print(f\"  correct answer: {repr(model.tokenizer.decode([tok.item()]))}\")\n",
    "    for ind, tok in enumerate(data.incorrect[b]):\n",
    "        if tok != pad_token:\n",
    "            print(f\"  incorrect answer: {repr(model.tokenizer.decode([tok.item()]))}\")\n",
    "\n",
    "TOP_K = 5\n",
    "from acdc import ACDCEvalData\n",
    "from acdc import get_pad_token\n",
    "def logging_incorrect_metric(data: ACDCEvalData):\n",
    "    pad_token = get_pad_token(model.tokenizer)\n",
    "    for data_subset in [data.patched, data.corrupted]:\n",
    "        batch, _ = data_subset.data.size()\n",
    "        for b in range(batch):\n",
    "            if not data_subset.top_is_correct[b].item():\n",
    "                if not data.constrain_to_answers:\n",
    "                    logits = data_subset.logits[b]\n",
    "                    prs = torch.nn.functional.softmax(logits, dim=0)\n",
    "                    top = torch.argsort(-logits)\n",
    "                toks = data_subset.data[b][:data_subset.last_token_position[b]+1]\n",
    "                print(\"failed on this data point:\")\n",
    "                print(model.to_str_tokens(toks))\n",
    "                print(\"correct prs:\")\n",
    "                for i, tok in enumerate(data_subset.correct[b]):\n",
    "                    if tok.item() != pad_token:\n",
    "                        print(data_subset.correct_prs[b,i].item(), model.tokenizer.decode([tok.item()]))\n",
    "                        if not data.constrain_to_answers:\n",
    "                            top_k_pos = (top==tok.item()).nonzero().item()\n",
    "                            print(f\" top k pos of {top_k_pos}\")\n",
    "                print(\"incorrect prs:\")\n",
    "                for i, tok in enumerate(data_subset.incorrect[b]):\n",
    "                    if tok.item() != pad_token:\n",
    "                        print(data_subset.incorrect_prs[b,i].item(), model.tokenizer.decode([tok.item()]))\n",
    "                        if not data.constrain_to_answers:\n",
    "                            top_k_pos = (top==tok.item()).nonzero().item()\n",
    "                            print(f\" top k pos of {top_k_pos}\")\n",
    "                if not data.constrain_to_answers:\n",
    "                    for i, tok in enumerate(top[:TOP_K]):\n",
    "                        if tok.item() in [x.item() for x in data_subset.correct[b]]:\n",
    "                            print(f\"  correct   top {i} token {tok} = {repr(model.tokenizer.decode([tok]))} logit {logits[tok]} prs {prs[tok]}\")\n",
    "                        elif tok.item() in [x.item() for x in data_subset.incorrect[b]]:\n",
    "                            print(f\"  incorrect top {i} token {tok} = {repr(model.tokenizer.decode([tok]))} logit {logits[tok]} prs {prs[tok]}\")\n",
    "                        else:\n",
    "                            print(f\"  other     top {i} token {tok} = {repr(model.tokenizer.decode([tok]))} logit {logits[tok]} prs {prs[tok]}\")\n",
    "    return data.patched.correct_prs[:,0]\n",
    "\n",
    "pr_correct = data.eval(model=model, batch_size=10, metric=logging_incorrect_metric)\n",
    "print(pr_correct)\n",
    "print(torch.mean(pr_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:27<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:35<00:00,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:41<00:00,  4.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:46<00:00,  5.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:58<00:00,  6.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:05<00:00,  7.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:17<00:00,  8.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:20<00:00,  8.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:32<00:00, 10.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:39<00:00, 11.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:46<00:00, 11.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:03<00:00, 13.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:18<00:00, 15.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:22<00:00, 15.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:40<00:00, 17.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:50<00:00, 18.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:14<00:00, 15.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:55<00:00, 12.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:03<00:00, 13.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:12<00:00, 14.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:19<00:00, 15.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:27<00:00, 16.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:35<00:00, 17.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:47<00:00, 18.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:57<00:00, 19.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [03:06<00:00, 20.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [03:16<00:00, 21.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [03:20<00:00, 22.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [03:36<00:00, 24.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [03:44<00:00, 24.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [03:59<00:00, 26.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [04:15<00:00, 28.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [04:29<00:00, 29.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [04:50<00:00, 32.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [05:13<00:00, 34.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [05:35<00:00, 37.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [05:55<00:00, 39.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [06:04<00:00, 40.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [06:30<00:00, 43.34s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from acdc import accuracy_metric\n",
    "num_patching_pairs = 100\n",
    "seed = 42\n",
    "valid_seed = 41\n",
    "constrain_to_answers = False\n",
    "has_symmetric_patching = True\n",
    "varying_data_lengths = False\n",
    "\n",
    "max_seq_len = 40\n",
    "max_num_repeats = 10\n",
    "\n",
    "output_data = torch.zeros([max_seq_len-1, max_num_repeats-1])\n",
    "for i, copy_seq_len in enumerate(range(1,max_seq_len)):\n",
    "    print(copy_seq_len)\n",
    "    for j, num_repeats in enumerate(tqdm(range(1,max_num_repeats))):\n",
    "        seed = i+j*num_repeats\n",
    "        data = generate_dataset(model=model,\n",
    "                    data_generator=copy_data_generator,\n",
    "                    num_patching_pairs=num_patching_pairs,\n",
    "                    seed=seed,\n",
    "                    valid_seed=valid_seed,\n",
    "                    constrain_to_answers=constrain_to_answers,\n",
    "                    has_symmetric_patching=has_symmetric_patching, \n",
    "                    varying_data_lengths=varying_data_lengths,\n",
    "                    copy_seq_len=copy_seq_len,\n",
    "                    num_repeats=num_repeats)\n",
    "        output_data[i,j] = torch.mean(data.eval(model=model, batch_size=20, metric=accuracy_metric)).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "num tokens in sequence: %{x}<br>num times repeated: %{y}<br>color: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "x": [
          "1",
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8",
          "9",
          "10",
          "11",
          "12",
          "13",
          "14",
          "15",
          "16",
          "17",
          "18",
          "19",
          "20",
          "21",
          "22",
          "23",
          "24",
          "25",
          "26",
          "27",
          "28",
          "29",
          "30",
          "31",
          "32",
          "33",
          "34",
          "35",
          "36",
          "37",
          "38",
          "39"
         ],
         "xaxis": "x",
         "y": [
          "1",
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8",
          "9"
         ],
         "yaxis": "y",
         "z": [
          [
           0.02499999850988388,
           0.3999999761581421,
           0.6449999809265137,
           0.7249999642372131,
           0.824999988079071,
           0.9049999713897705,
           0.8849999904632568,
           0.8799999952316284,
           0.8999999761581421,
           0.8949999809265137,
           0.8650000095367432,
           0.8799999952316284,
           0.9049999713897705,
           0.8949999809265137,
           0.9449999928474426,
           0.875,
           0.8399999737739563,
           0.9300000071525574,
           0.9300000071525574,
           0.8499999642372131,
           0.8999999761581421,
           0.9149999618530273,
           0.8899999856948853,
           0.8549999594688416,
           0.875,
           0.9199999570846558,
           0.8650000095367432,
           0.875,
           0.8650000095367432,
           0.8799999952316284,
           0.9099999666213989,
           0.9149999618530273,
           0.9199999570846558,
           0.8799999952316284,
           0.8650000095367432,
           0.8999999761581421,
           0.8999999761581421,
           0.9649999737739563,
           0.8899999856948853
          ],
          [
           0.20499999821186066,
           0.7999999523162842,
           0.9599999785423279,
           0.9649999737739563,
           0.9649999737739563,
           0.9899999499320984,
           0.9899999499320984,
           0.9599999785423279,
           0.9899999499320984,
           1,
           0.9899999499320984,
           0.98499995470047,
           0.9950000047683716,
           0.98499995470047,
           0.9899999499320984,
           0.98499995470047,
           0.9799999594688416,
           0.98499995470047,
           0.98499995470047,
           0.9749999642372131,
           0.9799999594688416,
           0.9799999594688416,
           0.98499995470047,
           0.9799999594688416,
           0.98499995470047,
           0.9950000047683716,
           0.9749999642372131,
           0.9899999499320984,
           0.9699999690055847,
           0.9950000047683716,
           0.98499995470047,
           0.98499995470047,
           0.9799999594688416,
           0.9749999642372131,
           0.98499995470047,
           0.98499995470047,
           0.9950000047683716,
           0.9950000047683716,
           0.9799999594688416
          ],
          [
           0.375,
           0.9699999690055847,
           0.98499995470047,
           0.9799999594688416,
           0.98499995470047,
           0.9899999499320984,
           0.9899999499320984,
           0.9899999499320984,
           1,
           0.9950000047683716,
           0.9950000047683716,
           0.9950000047683716,
           0.9950000047683716,
           0.9899999499320984,
           1,
           0.9950000047683716,
           0.9950000047683716,
           0.9899999499320984,
           0.9899999499320984,
           0.9799999594688416,
           1,
           0.98499995470047,
           1,
           0.9950000047683716,
           0.9899999499320984,
           0.98499995470047,
           0.9899999499320984,
           0.9699999690055847,
           0.9899999499320984,
           0.9950000047683716,
           0.9899999499320984,
           0.9899999499320984,
           0.9749999642372131,
           0.9950000047683716,
           0.98499995470047,
           0.9950000047683716,
           0.9799999594688416,
           0.98499995470047,
           1
          ],
          [
           0.6800000071525574,
           0.9799999594688416,
           0.9899999499320984,
           0.98499995470047,
           1,
           0.9950000047683716,
           0.9950000047683716,
           0.9950000047683716,
           1,
           0.9899999499320984,
           1,
           1,
           0.9950000047683716,
           1,
           0.9950000047683716,
           0.98499995470047,
           0.9899999499320984,
           0.9799999594688416,
           0.9950000047683716,
           1,
           0.9899999499320984,
           0.9950000047683716,
           0.9950000047683716,
           0.9899999499320984,
           0.9899999499320984,
           0.9950000047683716,
           1,
           0.9950000047683716,
           1,
           0.98499995470047,
           0.9899999499320984,
           0.9899999499320984,
           0.9799999594688416,
           0.9899999499320984,
           1,
           0.9950000047683716,
           0.98499995470047,
           0.9950000047683716,
           1
          ],
          [
           0.7599999904632568,
           0.9950000047683716,
           0.9899999499320984,
           1,
           0.9899999499320984,
           0.9899999499320984,
           0.9899999499320984,
           0.9950000047683716,
           0.9950000047683716,
           0.98499995470047,
           0.9950000047683716,
           0.9950000047683716,
           0.9950000047683716,
           0.9950000047683716,
           0.9799999594688416,
           0.9699999690055847,
           1,
           1,
           1,
           1,
           0.9950000047683716,
           1,
           0.9950000047683716,
           0.9950000047683716,
           0.9950000047683716,
           0.9950000047683716,
           1,
           0.98499995470047,
           0.9950000047683716,
           0.9899999499320984,
           1,
           0.9950000047683716,
           0.9649999737739563,
           0.98499995470047,
           1,
           0.9899999499320984,
           0.9950000047683716,
           0.9799999594688416,
           1
          ],
          [
           0.8499999642372131,
           0.9899999499320984,
           1,
           0.9899999499320984,
           0.9950000047683716,
           0.9950000047683716,
           0.9899999499320984,
           0.9950000047683716,
           0.9950000047683716,
           0.9899999499320984,
           0.9950000047683716,
           0.9899999499320984,
           0.9899999499320984,
           0.98499995470047,
           0.9899999499320984,
           0.9950000047683716,
           0.9899999499320984,
           0.9950000047683716,
           0.9899999499320984,
           0.9950000047683716,
           1,
           0.9950000047683716,
           1,
           0.9899999499320984,
           0.9950000047683716,
           1,
           0.9950000047683716,
           0.9950000047683716,
           0.9950000047683716,
           1,
           0.9950000047683716,
           0.9950000047683716,
           0.9950000047683716,
           0.9799999594688416,
           1,
           0.98499995470047,
           1,
           0.9950000047683716,
           0.9950000047683716
          ],
          [
           0.8499999642372131,
           0.98499995470047,
           0.9950000047683716,
           0.9950000047683716,
           0.9899999499320984,
           1,
           1,
           0.9950000047683716,
           0.9950000047683716,
           0.9950000047683716,
           1,
           0.9950000047683716,
           1,
           1,
           1,
           0.9899999499320984,
           0.9950000047683716,
           0.9799999594688416,
           0.9899999499320984,
           1,
           0.9899999499320984,
           1,
           1,
           1,
           0.98499995470047,
           1,
           0.9899999499320984,
           1,
           0.9950000047683716,
           0.9950000047683716,
           0.9950000047683716,
           0.9899999499320984,
           1,
           0.9950000047683716,
           1,
           0.9950000047683716,
           0.9899999499320984,
           1,
           0.9899999499320984
          ],
          [
           0.875,
           1,
           0.9899999499320984,
           0.9950000047683716,
           0.9950000047683716,
           0.9899999499320984,
           1,
           0.9899999499320984,
           1,
           1,
           0.9950000047683716,
           1,
           0.9950000047683716,
           0.98499995470047,
           1,
           0.9950000047683716,
           1,
           1,
           1,
           1,
           0.9950000047683716,
           1,
           0.9950000047683716,
           1,
           0.9799999594688416,
           1,
           0.98499995470047,
           0.9950000047683716,
           1,
           0.9899999499320984,
           0.9899999499320984,
           0.98499995470047,
           0.9950000047683716,
           0.9950000047683716,
           0.9899999499320984,
           0.98499995470047,
           1,
           0.9899999499320984,
           0.98499995470047
          ],
          [
           0.8949999809265137,
           0.9899999499320984,
           0.9950000047683716,
           1,
           0.9950000047683716,
           0.9950000047683716,
           0.98499995470047,
           0.9950000047683716,
           1,
           0.9950000047683716,
           1,
           1,
           1,
           0.9950000047683716,
           1,
           0.9950000047683716,
           0.9950000047683716,
           1,
           0.9950000047683716,
           1,
           0.98499995470047,
           1,
           0.9950000047683716,
           0.9950000047683716,
           0.9950000047683716,
           0.9899999499320984,
           1,
           0.9950000047683716,
           0.9950000047683716,
           0.9950000047683716,
           1,
           0.98499995470047,
           0.9950000047683716,
           0.9899999499320984,
           0.9899999499320984,
           1,
           0.9950000047683716,
           1,
           1
          ]
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "coloraxis": {
         "cmid": 0,
         "colorscale": [
          [
           0,
           "rgb(103,0,31)"
          ],
          [
           0.1,
           "rgb(178,24,43)"
          ],
          [
           0.2,
           "rgb(214,96,77)"
          ],
          [
           0.3,
           "rgb(244,165,130)"
          ],
          [
           0.4,
           "rgb(253,219,199)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(209,229,240)"
          ],
          [
           0.7,
           "rgb(146,197,222)"
          ],
          [
           0.8,
           "rgb(67,147,195)"
          ],
          [
           0.9,
           "rgb(33,102,172)"
          ],
          [
           1,
           "rgb(5,48,97)"
          ]
         ]
        },
        "font": {
         "color": "black",
         "size": 9
        },
        "height": 600,
        "legend": {
         "x": 0.01,
         "xanchor": "left",
         "y": 0.99,
         "yanchor": "top"
        },
        "margin": {
         "b": 0,
         "l": 0,
         "r": 0,
         "t": 100
        },
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "accuracy of mamba-370m on copy task"
        },
        "width": 800,
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "scaleanchor": "y",
         "tickmode": "array",
         "ticktext": [
          "1",
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8",
          "9",
          "10",
          "11",
          "12",
          "13",
          "14",
          "15",
          "16",
          "17",
          "18",
          "19",
          "20",
          "21",
          "22",
          "23",
          "24",
          "25",
          "26",
          "27",
          "28",
          "29",
          "30",
          "31",
          "32",
          "33",
          "34",
          "35",
          "36",
          "37",
          "38",
          "39"
         ],
         "tickvals": [
          "1",
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8",
          "9",
          "10",
          "11",
          "12",
          "13",
          "14",
          "15",
          "16",
          "17",
          "18",
          "19",
          "20",
          "21",
          "22",
          "23",
          "24",
          "25",
          "26",
          "27",
          "28",
          "29",
          "30",
          "31",
          "32",
          "33",
          "34",
          "35",
          "36",
          "37",
          "38",
          "39"
         ],
         "title": {
          "text": "num tokens in sequence"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "tickmode": "array",
         "ticktext": [
          "1",
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8",
          "9"
         ],
         "tickvals": [
          "1",
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8",
          "9"
         ],
         "title": {
          "text": "num times repeated"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"697756e7-260b-4c8f-8750-de641d887274\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"697756e7-260b-4c8f-8750-de641d887274\")) {                    Plotly.newPlot(                        \"697756e7-260b-4c8f-8750-de641d887274\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"x\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\"],\"y\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"],\"z\":[[0.02499999850988388,0.3999999761581421,0.6449999809265137,0.7249999642372131,0.824999988079071,0.9049999713897705,0.8849999904632568,0.8799999952316284,0.8999999761581421,0.8949999809265137,0.8650000095367432,0.8799999952316284,0.9049999713897705,0.8949999809265137,0.9449999928474426,0.875,0.8399999737739563,0.9300000071525574,0.9300000071525574,0.8499999642372131,0.8999999761581421,0.9149999618530273,0.8899999856948853,0.8549999594688416,0.875,0.9199999570846558,0.8650000095367432,0.875,0.8650000095367432,0.8799999952316284,0.9099999666213989,0.9149999618530273,0.9199999570846558,0.8799999952316284,0.8650000095367432,0.8999999761581421,0.8999999761581421,0.9649999737739563,0.8899999856948853],[0.20499999821186066,0.7999999523162842,0.9599999785423279,0.9649999737739563,0.9649999737739563,0.9899999499320984,0.9899999499320984,0.9599999785423279,0.9899999499320984,1.0,0.9899999499320984,0.98499995470047,0.9950000047683716,0.98499995470047,0.9899999499320984,0.98499995470047,0.9799999594688416,0.98499995470047,0.98499995470047,0.9749999642372131,0.9799999594688416,0.9799999594688416,0.98499995470047,0.9799999594688416,0.98499995470047,0.9950000047683716,0.9749999642372131,0.9899999499320984,0.9699999690055847,0.9950000047683716,0.98499995470047,0.98499995470047,0.9799999594688416,0.9749999642372131,0.98499995470047,0.98499995470047,0.9950000047683716,0.9950000047683716,0.9799999594688416],[0.375,0.9699999690055847,0.98499995470047,0.9799999594688416,0.98499995470047,0.9899999499320984,0.9899999499320984,0.9899999499320984,1.0,0.9950000047683716,0.9950000047683716,0.9950000047683716,0.9950000047683716,0.9899999499320984,1.0,0.9950000047683716,0.9950000047683716,0.9899999499320984,0.9899999499320984,0.9799999594688416,1.0,0.98499995470047,1.0,0.9950000047683716,0.9899999499320984,0.98499995470047,0.9899999499320984,0.9699999690055847,0.9899999499320984,0.9950000047683716,0.9899999499320984,0.9899999499320984,0.9749999642372131,0.9950000047683716,0.98499995470047,0.9950000047683716,0.9799999594688416,0.98499995470047,1.0],[0.6800000071525574,0.9799999594688416,0.9899999499320984,0.98499995470047,1.0,0.9950000047683716,0.9950000047683716,0.9950000047683716,1.0,0.9899999499320984,1.0,1.0,0.9950000047683716,1.0,0.9950000047683716,0.98499995470047,0.9899999499320984,0.9799999594688416,0.9950000047683716,1.0,0.9899999499320984,0.9950000047683716,0.9950000047683716,0.9899999499320984,0.9899999499320984,0.9950000047683716,1.0,0.9950000047683716,1.0,0.98499995470047,0.9899999499320984,0.9899999499320984,0.9799999594688416,0.9899999499320984,1.0,0.9950000047683716,0.98499995470047,0.9950000047683716,1.0],[0.7599999904632568,0.9950000047683716,0.9899999499320984,1.0,0.9899999499320984,0.9899999499320984,0.9899999499320984,0.9950000047683716,0.9950000047683716,0.98499995470047,0.9950000047683716,0.9950000047683716,0.9950000047683716,0.9950000047683716,0.9799999594688416,0.9699999690055847,1.0,1.0,1.0,1.0,0.9950000047683716,1.0,0.9950000047683716,0.9950000047683716,0.9950000047683716,0.9950000047683716,1.0,0.98499995470047,0.9950000047683716,0.9899999499320984,1.0,0.9950000047683716,0.9649999737739563,0.98499995470047,1.0,0.9899999499320984,0.9950000047683716,0.9799999594688416,1.0],[0.8499999642372131,0.9899999499320984,1.0,0.9899999499320984,0.9950000047683716,0.9950000047683716,0.9899999499320984,0.9950000047683716,0.9950000047683716,0.9899999499320984,0.9950000047683716,0.9899999499320984,0.9899999499320984,0.98499995470047,0.9899999499320984,0.9950000047683716,0.9899999499320984,0.9950000047683716,0.9899999499320984,0.9950000047683716,1.0,0.9950000047683716,1.0,0.9899999499320984,0.9950000047683716,1.0,0.9950000047683716,0.9950000047683716,0.9950000047683716,1.0,0.9950000047683716,0.9950000047683716,0.9950000047683716,0.9799999594688416,1.0,0.98499995470047,1.0,0.9950000047683716,0.9950000047683716],[0.8499999642372131,0.98499995470047,0.9950000047683716,0.9950000047683716,0.9899999499320984,1.0,1.0,0.9950000047683716,0.9950000047683716,0.9950000047683716,1.0,0.9950000047683716,1.0,1.0,1.0,0.9899999499320984,0.9950000047683716,0.9799999594688416,0.9899999499320984,1.0,0.9899999499320984,1.0,1.0,1.0,0.98499995470047,1.0,0.9899999499320984,1.0,0.9950000047683716,0.9950000047683716,0.9950000047683716,0.9899999499320984,1.0,0.9950000047683716,1.0,0.9950000047683716,0.9899999499320984,1.0,0.9899999499320984],[0.875,1.0,0.9899999499320984,0.9950000047683716,0.9950000047683716,0.9899999499320984,1.0,0.9899999499320984,1.0,1.0,0.9950000047683716,1.0,0.9950000047683716,0.98499995470047,1.0,0.9950000047683716,1.0,1.0,1.0,1.0,0.9950000047683716,1.0,0.9950000047683716,1.0,0.9799999594688416,1.0,0.98499995470047,0.9950000047683716,1.0,0.9899999499320984,0.9899999499320984,0.98499995470047,0.9950000047683716,0.9950000047683716,0.9899999499320984,0.98499995470047,1.0,0.9899999499320984,0.98499995470047],[0.8949999809265137,0.9899999499320984,0.9950000047683716,1.0,0.9950000047683716,0.9950000047683716,0.98499995470047,0.9950000047683716,1.0,0.9950000047683716,1.0,1.0,1.0,0.9950000047683716,1.0,0.9950000047683716,0.9950000047683716,1.0,0.9950000047683716,1.0,0.98499995470047,1.0,0.9950000047683716,0.9950000047683716,0.9950000047683716,0.9899999499320984,1.0,0.9950000047683716,0.9950000047683716,0.9950000047683716,1.0,0.98499995470047,0.9950000047683716,0.9899999499320984,0.9899999499320984,1.0,0.9950000047683716,1.0,1.0]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"num tokens in sequence: %{x}\\u003cbr\\u003enum times repeated: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"num tokens in sequence\"},\"tickmode\":\"array\",\"tickvals\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\"],\"ticktext\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\"]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"num times repeated\"},\"tickmode\":\"array\",\"tickvals\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"],\"ticktext\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"accuracy of mamba-370m on copy task\"},\"font\":{\"size\":9,\"color\":\"black\"},\"margin\":{\"l\":0,\"r\":0,\"t\":100,\"b\":0},\"width\":800,\"height\":600,\"autosize\":false,\"showlegend\":true,\"legend\":{\"yanchor\":\"top\",\"y\":0.99,\"xanchor\":\"left\",\"x\":0.01}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('697756e7-260b-4c8f-8750-de641d887274');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(output_data.T, x=[str(x) for x in range(1,max_seq_len)], y=[str(y) for y in range(1,max_num_repeats)], fix_size=True, font_size=9, title='accuracy of mamba-370m on copy task', xaxis='num tokens in sequence', yaxis='num times repeated')\n",
    "\n",
    "# here we see that it is very good as long as sequence longer than 3 and it's repeated at least twice. It's alright for repeated once but I'd prefer starting with a higher accuracy.\n",
    "# I'll do 6 tokens in sequence, and repeat it 2 times (so 3 times total)\n",
    "# 1 hr 5 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_uncorrupted = model.tokenizer.encode(data.data[0])\n",
    "prompt_corrupted = model.tokenizer.encode(data.data[1])\n",
    "uncorrupted_answer = model.tokenizer.encode(data.correct[0])\n",
    "corrupted_answer = model.tokenizer.encode(data.correct[1])\n",
    "\n",
    "print(prompt_uncorrupted)\n",
    "print(prompt_corrupted)\n",
    "print(uncorrupted_answer)\n",
    "print(corrupted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from neel nanda's examples\n",
    "\n",
    "H_N_PATCHING_LAYER = 39\n",
    "\n",
    "# default settings aren't very good, these are better\n",
    "plot_args = {\n",
    "    'width': 800,\n",
    "    'height': 600,\n",
    "    \"autosize\": False,\n",
    "    'showlegend': True,\n",
    "    'margin': {\"l\":0,\"r\":0,\"t\":100,\"b\":0}\n",
    "}\n",
    "\n",
    "# you can modify this to only run things on a subset of layers\n",
    "limited_layers = list(range(model.cfg.n_layers))\n",
    "\n",
    "\n",
    "answer_tokens = sorted(list(set([model.tokenizer.encode(uncorrupted_answer)[0], model.tokenizer.encode(corrupted_answer)[0]])))\n",
    "\n",
    "prompt_uncorrupted_tokens = model.to_tokens(prompt_uncorrupted)\n",
    "prompt_corrupted_tokens = model.to_tokens(prompt_corrupted)\n",
    "\n",
    "# logits should be [B,L,V] \n",
    "def uncorrupted_logit_minus_corrupted_logit(logits, uncorrupted_answer, corrupted_answer):\n",
    "    uncorrupted_index = model.to_single_token(uncorrupted_answer)\n",
    "    corrupted_index = model.to_single_token(corrupted_answer)\n",
    "    return logits[0, -1, uncorrupted_index] - logits[0, -1, corrupted_index]\n",
    "\n",
    "# prs should be [B,L,V] \n",
    "def uncorrupted_pr_minus_corrupted_pr(prs, uncorrupted_answer, corrupted_answer):\n",
    "    uncorrupted_index = model.to_single_token(uncorrupted_answer)\n",
    "    corrupted_index = model.to_single_token(corrupted_answer)\n",
    "    return prs[0, -1, uncorrupted_index] - prs[0, -1, corrupted_index]\n",
    "\n",
    "\n",
    "\n",
    "# [B,L,V]\n",
    "corrupted_logits, corrupted_activations = model.run_with_cache(prompt_corrupted_tokens, only_use_these_layers=limited_layers)\n",
    "corrupted_logit_diff = uncorrupted_logit_minus_corrupted_logit(logits=corrupted_logits, uncorrupted_answer=uncorrupted_answer, corrupted_answer=corrupted_answer)\n",
    "corrupted_prs = torch.softmax(corrupted_logits, dim=2)\n",
    "corrupted_pr_diff = uncorrupted_pr_minus_corrupted_pr(prs=corrupted_prs, uncorrupted_answer=uncorrupted_answer, corrupted_answer=corrupted_answer)\n",
    "\n",
    "\n",
    "# [B,L,V]\n",
    "uncorrupted_logits = model.run_with_hooks(prompt_uncorrupted_tokens, only_use_these_layers=limited_layers)\n",
    "uncorrupted_logit_diff = uncorrupted_logit_minus_corrupted_logit(logits=uncorrupted_logits, uncorrupted_answer=uncorrupted_answer, corrupted_answer=corrupted_answer)\n",
    "uncorrupted_prs = torch.softmax(uncorrupted_logits, dim=2)\n",
    "uncorrupted_pr_diff = uncorrupted_pr_minus_corrupted_pr(prs=uncorrupted_prs, uncorrupted_answer=uncorrupted_answer, corrupted_answer=corrupted_answer)\n",
    "\n",
    "uncorrupted_index = model.to_single_token(uncorrupted_answer)\n",
    "corrupted_index = model.to_single_token(corrupted_answer)\n",
    "print(f'uncorrupted prompt\\n{prompt_uncorrupted}')\n",
    "print(f\"{repr(uncorrupted_answer)} logit {uncorrupted_logits[0,-1,uncorrupted_index]}\")\n",
    "print(f\"{repr(uncorrupted_answer)} pr {uncorrupted_prs[0,-1,uncorrupted_index]}\")\n",
    "print(f\"{repr(corrupted_answer)} logit {uncorrupted_logits[0,-1,corrupted_index]}\")\n",
    "print(f\"{repr(corrupted_answer)} pr {uncorrupted_prs[0,-1,corrupted_index]}\")\n",
    "print(f'\\ncorrupted prompt\\n{prompt_corrupted}')\n",
    "print(f\"{repr(uncorrupted_answer)} logit {corrupted_logits[0,-1,uncorrupted_index]}\")\n",
    "print(f\"{repr(uncorrupted_answer)} pr {corrupted_prs[0,-1,uncorrupted_index]}\")\n",
    "print(f\"{repr(corrupted_answer)} logit {corrupted_logits[0,-1,corrupted_index]}\")\n",
    "print(f\"{repr(corrupted_answer)} pr {corrupted_prs[0,-1,corrupted_index]}\")\n",
    "\n",
    "# We make a tensor to store the results for each patching run. We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.\n",
    "L = len(prompt_uncorrupted_tokens[0])\n",
    "if len(prompt_corrupted_tokens[0]) != len(prompt_uncorrupted_tokens[0]):\n",
    "    raise Exception(\"Prompts are not the same length\") # feel free to comment this out, you can patch for different sized prompts its just a lil sus\n",
    "\n",
    "# diff is logit of uncorrupted_answer - logit of corrupted_answer\n",
    "# we expect corrupted_diff to have a negative value (as corrupted should put high pr on corrupted_answer)\n",
    "# we expect uncorrupted to have a positive value (as uncorrupted should put high pr on uncorrupted_answer)\n",
    "# thus we can treat these as (rough) min and max possible values\n",
    "min_logit_diff = corrupted_logit_diff\n",
    "max_logit_diff = uncorrupted_logit_diff\n",
    "\n",
    "min_pr_diff = corrupted_pr_diff\n",
    "max_pr_diff = uncorrupted_pr_diff\n",
    "\n",
    "\n",
    "def generate_always_hooks():\n",
    "    hooks = []\n",
    "\n",
    "    LAYER = 39\n",
    "    _, L = prompt_uncorrupted_tokens.size()\n",
    "    for pos in range(L):\n",
    "        # slice(None, None) is same as : (which means all)\n",
    "        #hooks.append((f'blocks.{LAYER}.hook_h.{pos}', partial(h_patching_hook, layer=LAYER, position=pos, batch=slice(None, None))))\n",
    "        hooks.append((f'blocks.{LAYER}.hook_layer_input', partial(position_patching_hook, layer=LAYER, position=pos, batch=slice(None, None))))\n",
    "\n",
    "    #ABLATE_POS = 3\n",
    "    #hooks.append((f'blocks.{35}.hook_layer_input', partial(position_patching_hook, layer=LAYER, position=3, batch=slice(None, None))))\n",
    "    #hooks.append((f'blocks.{40}.hook_layer_input', partial(position_patching_hook, layer=LAYER, position=3, batch=slice(None, None))))\n",
    "    #FINAL_POS = 19\n",
    "    #hooks.append((f'blocks.{47}.hook_layer_input', partial(position_patching_hook, layer=LAYER, position=FINAL_POS, batch=slice(None, None))))\n",
    "    return hooks\n",
    "always_hooks = generate_always_hooks()\n",
    "\n",
    "always_logits = model.run_with_hooks(prompt_uncorrupted_tokens, fwd_hooks=always_hooks, only_use_these_layers=limited_layers, fast_ssm=False, fast_conv=False)\n",
    "\n",
    "\n",
    "always_prs = torch.softmax(always_logits, dim=2)\n",
    "always_logit_diff = uncorrupted_logit_minus_corrupted_logit(logits=always_logits,\n",
    "                                                                uncorrupted_answer=uncorrupted_answer,\n",
    "                                                                corrupted_answer=corrupted_answer)\n",
    "# normalize it so\n",
    "# 0 means min_logit_diff (so 0 means that it is acting like the corrupted model)\n",
    "# 1 means max_logit_diff (so 1 means that it is acting like the uncorrupted model)\n",
    "normalized_always_logit_diff = (always_logit_diff-min_logit_diff)/(max_logit_diff - min_logit_diff)\n",
    "# now flip them, since most interventions will do nothing and thus act like uncorrupted model, visually its better to have that at 0\n",
    "# so now\n",
    "# 0 means that it is acting like the uncorrupted model\n",
    "# 1 means that it is acting like the corrupted model\n",
    "normalized_always_logit_diff = 1.0 - normalized_always_logit_diff\n",
    "\n",
    "# same for pr\n",
    "always_pr_diff = uncorrupted_pr_minus_corrupted_pr(prs=always_prs,\n",
    "                                                    uncorrupted_answer=uncorrupted_answer,\n",
    "                                                    corrupted_answer=corrupted_answer)\n",
    "normalized_always_pr_diff = 1.0-(always_pr_diff-min_pr_diff)/(max_pr_diff - min_pr_diff)\n",
    "\n",
    "# make token labels that describe the patch\n",
    "corrupted_str_tokens = model.to_str_tokens(prompt_corrupted_tokens)\n",
    "uncorrupted_str_tokens = model.to_str_tokens(prompt_uncorrupted_tokens)\n",
    "token_labels = []\n",
    "for index, (corrupted_token, uncorrupted_token) in enumerate(zip(corrupted_str_tokens, uncorrupted_str_tokens)):\n",
    "    if corrupted_token == uncorrupted_token:\n",
    "        token_labels.append(f\"{corrupted_token}_{index}\")\n",
    "    else:\n",
    "        token_labels.append(f\"{uncorrupted_token}->{corrupted_token}_{index}\")\n",
    "\n",
    "def run_patching(patching_type,\n",
    "                 patching_hook_name_func,\n",
    "                 patching_hook_func,\n",
    "                 batch_size,\n",
    "                 show_options, \n",
    "                 min_logit_diff,\n",
    "                 max_logit_diff,\n",
    "                 min_pr_diff,\n",
    "                 max_pr_diff,\n",
    "                 token_labels,\n",
    "                 prompt_uncorrupted_tokens,\n",
    "                 uncorrupted_answer,\n",
    "                 corrupted_answer,\n",
    "                 always_hooks=None,\n",
    "                 show_plot=True,\n",
    "                 **kwargs):\n",
    "    _, L = prompt_uncorrupted_tokens.size()\n",
    "    torch.cuda.empty_cache()\n",
    "    hook_title = patching_hook_name_func(layer='{layer}', position='{position}')\n",
    "    print(f\"running patching {patching_type}, using hook {hook_title}\")\n",
    "    global patching_result_logits, patching_result_prs # if you want to access it once this is done running\n",
    "    n_layers = len(limited_layers)\n",
    "\n",
    "    num_results = n_layers\n",
    "    if patching_type == H_N_PATCHING:\n",
    "        print(f\"on layer H_N_PATCHING_LAYER={H_N_PATCHING_LAYER}\")\n",
    "        N = model.cfg.N\n",
    "        num_results = N\n",
    "    elif patching_type == CONV_FILTERS_PATCHING:\n",
    "        D_conv = model.cfg.D_conv\n",
    "        num_results = (D_conv-1)*n_layers # -1 because the zero one is always zero so we ignore it\n",
    "    \n",
    "    patching_result_normalized_logits = torch.zeros((num_results, L), device=model.cfg.device)\n",
    "    patching_result_normalized_prs = torch.zeros((num_results, L), device=model.cfg.device)\n",
    "\n",
    "    num_answers = len(answer_tokens)\n",
    "    patching_result_logits = torch.zeros((num_results, L, num_answers), device=model.cfg.device)\n",
    "    patching_result_prs = torch.zeros((num_results, L, num_answers), device=model.cfg.device)\n",
    "    \n",
    "    hooks = []\n",
    "    # skipping h needs A_bar stored, so also add that hook\n",
    "    if patching_type == SKIPPING_H_PATCHING:\n",
    "        for i, layer in list(enumerate(limited_layers)):\n",
    "            hooks.append((f'blocks.{layer}.hook_A_bar', partial(A_bar_storage_hook_for_skipping_h, layer=layer)))\n",
    "\n",
    "    # skipping layer needs layer_input (resid_pre) stored, so also add that hook\n",
    "    if patching_type == LAYER_SKIPPING:\n",
    "        for i, layer in list(enumerate(limited_layers)):\n",
    "            hooks.append((f'blocks.{layer}.hook_resid_pre', partial(layer_input_storage_hook, layer=layer)))\n",
    "    \n",
    "    # conv filters works via initializing things, then storing all the stuff we want to hook, then doing all that in place at the same time\n",
    "    if patching_type == CONV_FILTERS_PATCHING:\n",
    "        for i, layer in list(enumerate(limited_layers)):\n",
    "            # reset the storage to empty/initialize stuff\n",
    "            hooks.append((f'blocks.{layer}.hook_layer_input', better_conv_patching_init_hook))\n",
    "            # doing all the patches we have stored (below) at the same time\n",
    "            hooks.append((f'blocks.{layer}.hook_conv', partial(better_conv_patching_hook, input_hook_name=f'blocks.{layer}.hook_in_proj', layer=layer)))\n",
    "\n",
    "    if not always_hooks is None:\n",
    "        hooks += always_hooks\n",
    "\n",
    "    initial_num_hooks = len(hooks)\n",
    "\n",
    "    \n",
    "    if patching_type == H_N_PATCHING:\n",
    "        batch = 0\n",
    "        indices = []\n",
    "        for n in range(N):\n",
    "            for position in range(L):\n",
    "                patching_hook_name = patching_hook_name_func(layer=H_N_PATCHING_LAYER, position=position)\n",
    "                if batch_size != BATCH_SIZE_ALL: batch = batch % int(batch_size)\n",
    "                patching_hook = partial(patching_hook_func, layer=H_N_PATCHING_LAYER, position=position, n=n, batch=batch)\n",
    "                batch += 1\n",
    "                indices.append((n,position))\n",
    "                hooks.append((patching_hook_name, patching_hook))\n",
    "    elif patching_type == CONV_FILTERS_PATCHING:\n",
    "        batch = 0\n",
    "        indices = []\n",
    "        D_conv = model.cfg.D_conv\n",
    "        ind = 0\n",
    "        for i, layer in list(enumerate(limited_layers)):\n",
    "            for conv_filter_i in range(D_conv):\n",
    "                if conv_filter_i == 0: continue # this -d_conv-1 filter is always zero for some reason\n",
    "                for position in range(L):\n",
    "                    patching_hook_name = patching_hook_name_func(layer=layer, position=position)\n",
    "                    if batch_size != BATCH_SIZE_ALL: batch = batch % int(batch_size)\n",
    "                    hooks.append((f'blocks.{layer}.hook_in_proj', partial(better_conv_patching_storage_hook, position=position, layer=layer, conv_filter_i=conv_filter_i, batch=batch)))\n",
    "                    #patching_hook = partial(patching_hook_func, layer=layer, position=position, batch=batch, conv_filter_i=conv_filter_i)\n",
    "                    batch += 1\n",
    "                    indices.append((ind,position))\n",
    "                    #hooks.append((patching_hook_name, patching_hook))\n",
    "                ind += 1\n",
    "    else:\n",
    "        batch = 0\n",
    "        indices = []\n",
    "        for i, layer in list(enumerate(limited_layers)):\n",
    "            for position in range(L):\n",
    "                patching_hook_name = patching_hook_name_func(layer=layer, position=position)\n",
    "                if batch_size != BATCH_SIZE_ALL: batch = batch % int(batch_size)\n",
    "                patching_hook = partial(patching_hook_func, layer=layer, position=position, batch=batch)\n",
    "                batch += 1\n",
    "                indices.append((i,position))\n",
    "                hooks.append((patching_hook_name, patching_hook))\n",
    "\n",
    "    \n",
    "    if batch_size != BATCH_SIZE_ALL:\n",
    "        V = model.cfg.V\n",
    "        patched_logits = torch.zeros([len(indices), L, V])\n",
    "        for batch_start in tqdm(list(range(0, len(indices), int(batch_size)))):\n",
    "            batch_end = min(len(indices), batch_start+int(batch_size))\n",
    "            # always do the initial they are for storage\n",
    "            batch_hooks = hooks[:initial_num_hooks] + hooks[initial_num_hooks+batch_start:initial_num_hooks+batch_end]\n",
    "            cur_batch_size = batch_end-batch_start\n",
    "            patched_logits[batch_start:batch_end] = model.run_with_hooks(prompt_uncorrupted_tokens.expand(cur_batch_size,L), fwd_hooks=batch_hooks, only_use_these_layers=limited_layers, **kwargs)\n",
    "    else:\n",
    "        # [B,L,V]\n",
    "        patched_logits = model.run_with_hooks(prompt_uncorrupted_tokens.expand(batch,L), fwd_hooks=hooks, only_use_these_layers=limited_layers, **kwargs)\n",
    "   \n",
    "    # [B,L,V]\n",
    "    patched_prs = torch.softmax(patched_logits, dim=2)\n",
    "    print(\"finished patching, plotting...\")\n",
    "    for b, (i,position) in enumerate(indices):\n",
    "        if corrupted_answer != uncorrupted_answer:\n",
    "            patched_logit_diff = uncorrupted_logit_minus_corrupted_logit(logits=patched_logits[b:b+1],\n",
    "                                                                         uncorrupted_answer=uncorrupted_answer,\n",
    "                                                                         corrupted_answer=corrupted_answer)\n",
    "            # normalize it so\n",
    "            # 0 means min_logit_diff (so 0 means that it is acting like the corrupted model)\n",
    "            # 1 means max_logit_diff (so 1 means that it is acting like the uncorrupted model)\n",
    "            normalized_patched_logit_diff = (patched_logit_diff-min_logit_diff)/(max_logit_diff - min_logit_diff)\n",
    "            # now flip them, since most interventions will do nothing and thus act like uncorrupted model, visually its better to have that at 0\n",
    "            # so now\n",
    "            # 0 means that it is acting like the uncorrupted model\n",
    "            # 1 means that it is acting like the corrupted model\n",
    "            normalized_patched_logit_diff = 1.0 - normalized_patched_logit_diff\n",
    "            normalized_patched_logit_diff = normalized_patched_logit_diff #normalized_always_logit_diff - normalized_patched_logit_diff\n",
    "            patching_result_normalized_logits[i, position] = normalized_patched_logit_diff\n",
    "            \n",
    "            # same for pr\n",
    "            patched_pr_diff = uncorrupted_pr_minus_corrupted_pr(prs=patched_prs[b:b+1],\n",
    "                                                                uncorrupted_answer=uncorrupted_answer,\n",
    "                                                                corrupted_answer=corrupted_answer)\n",
    "            normalized_patched_pr_diff = 1.0-(patched_pr_diff-min_pr_diff)/(max_pr_diff - min_pr_diff)\n",
    "            normalized_patched_pr_diff = normalized_always_pr_diff - normalized_patched_pr_diff\n",
    "            patching_result_normalized_prs[i, position] = normalized_patched_pr_diff\n",
    "\n",
    "        for k, answer_token in enumerate(answer_tokens):\n",
    "            patching_result_logits[i, position, k] = patched_logits[b,-1,answer_token]\n",
    "            patching_result_prs[i, position, k] = patched_prs[b,-1,answer_token]\n",
    "    \n",
    "        \n",
    "    if patching_type == H_N_PATCHING:\n",
    "        layer_labels = [str(n) for n in range(N)]\n",
    "    elif patching_type == CONV_FILTERS_PATCHING:\n",
    "        layer_labels = []\n",
    "        for layer in limited_layers:\n",
    "            for conv_i in range(1, D_conv):\n",
    "                layer_labels.append(f\"layer {layer} conv {conv_i-D_conv+1}\")\n",
    "    else:\n",
    "        layer_labels = [str(layer) for layer in limited_layers]\n",
    "    figs = []\n",
    "    y_axis = 'Layer'\n",
    "    if patching_type == H_N_PATCHING:\n",
    "        y_axis = 'N'\n",
    "    if show_plot:\n",
    "        if corrupted_answer != uncorrupted_answer:\n",
    "            if show_options in [SHOW_LOGITS, SHOW_BOTH]:\n",
    "                figs.append(imfig(patching_result_normalized_logits, x=token_labels, y=layer_labels, xaxis=\"Position\", yaxis=y_axis, title=f\"Normalized logit difference after patching {patching_type} using hook {hook_title}\", font_size=8))\n",
    "            if show_options in [SHOW_PR, SHOW_BOTH]:\n",
    "                figs.append(imfig(patching_result_normalized_prs, x=token_labels, y=layer_labels, xaxis=\"Position\", yaxis=y_axis, title=f\"Normalized pr difference after patching {patching_type} using hook {hook_title}\", font_size=8))\n",
    "        \n",
    "        for k, answer_token in enumerate(answer_tokens):\n",
    "            if show_options in [SHOW_LOGITS, SHOW_BOTH]:\n",
    "                figs.append(imfig(patching_result_logits[:,:,k], color_continuous_midpoint=None, x=token_labels, y=layer_labels, xaxis=\"Position\", yaxis=\"Layer\", title=f\"Logit of uncorrupted answer {repr(model.tokenizer.decode([answer_token]))} after patching {patching_type} using hook {hook_title}\", font_size=8))\n",
    "            if show_options in [SHOW_PR, SHOW_BOTH]:\n",
    "                figs.append(imfig(patching_result_prs[:,:,k], x=token_labels, y=layer_labels, xaxis=\"Position\", yaxis=y_axis, title=f\"Pr of uncorrupted answer {repr(model.tokenizer.decode([answer_token]))} after patching {patching_type} using hook {hook_title}\", font_size=8)) \n",
    "        \n",
    "        for fig in figs:\n",
    "            plot_args_copy = dict(list(plot_args.items()))\n",
    "            if patching_type == CONV_FILTERS_PATCHING:\n",
    "                plot_args_copy['height'] *= D_conv\n",
    "            fig.update_layout(**plot_args_copy)\n",
    "            fig.update_layout(legend=dict(\n",
    "                yanchor=\"top\",\n",
    "                y=0.99,\n",
    "                xanchor=\"left\",\n",
    "                x=0.01\n",
    "            ))\n",
    "            fig.show()\n",
    "    else:\n",
    "        return layer_labels, y_axis, patching_result_normalized_logits, patching_result_normalized_prs, patching_result_logits, patching_result_prs\n",
    "\n",
    "## hooks for conv filter patching\n",
    "def conv_input_storage_hook(\n",
    "    conv_input: Float[torch.Tensor, \"B L E\"],\n",
    "    hook: HookPoint,\n",
    "    layer: int,\n",
    ") -> Float[torch.Tensor, \"B L E\"]:\n",
    "    global progress # it's slow enough that progress bar is useful\n",
    "    if layer == 0:\n",
    "        progress = tqdm(total=len(limited_layers))\n",
    "    else:\n",
    "        progress.update(1)\n",
    "    global storage\n",
    "    storage = {}\n",
    "    storage['conv_input'] = conv_input\n",
    "    return conv_input\n",
    "\n",
    "def conv_patching_hook(\n",
    "    conv_output: Float[torch.Tensor, \"B L E\"],\n",
    "    hook: HookPoint,\n",
    "    layer: int,\n",
    "    position: int,\n",
    "    batch: int,\n",
    "    conv_filter_i: int,\n",
    ") -> Float[torch.Tensor, \"B L E\"]:\n",
    "    global storage\n",
    "    conv_input = storage['conv_input']\n",
    "    B, L, E = conv_input.size()\n",
    "    conv_input = rearrange(conv_input, 'B L E -> B E L')\n",
    "    conv_input_corrupted = rearrange(corrupted_activations[f'blocks.{layer}.hook_in_proj'], 'B L E -> B E L')\n",
    "    \n",
    "    ### This is identical to what the conv is doing\n",
    "    # pad zeros in front\n",
    "    # [B,E,D_CONV-1+L]\n",
    "    D_CONV = model.cfg.d_conv\n",
    "    padded_input = torch.nn.functional.pad(conv_input, (D_CONV-1,0), mode='constant', value=0)\n",
    "    padded_input_corrupted = torch.nn.functional.pad(conv_input_corrupted, (D_CONV-1,0), mode='constant', value=0)\n",
    "    output = torch.zeros([B,E,L], device=model.cfg.device)\n",
    "    # [E,1,D_CONV]\n",
    "    conv_weight = model.blocks[layer].conv1d.weight\n",
    "    # [E]\n",
    "    conv_bias = model.blocks[layer].conv1d.bias\n",
    "    # this is inefficient because its recomputing things every time\n",
    "    # but I don't want to have to rely on the ordering of hooks because that's sus\n",
    "    # so this is good enough\n",
    "    for i in range(D_CONV):\n",
    "        filter_str = f'filter_{i}'\n",
    "        if not filter_str in storage:\n",
    "            # [B,E,L]                      [E,1]                      [B,E,L]\n",
    "            filter_contribution = conv_weight[:,0,i].view(E,1)*padded_input[:,:,i:i+L]\n",
    "            storage[filter_str] = filter_contribution\n",
    "        filter_contribution = storage[filter_str]\n",
    "        if i == conv_filter_i:\n",
    "            # [1,E,L]                                   [E,1]                          # [1,E,L]\n",
    "            corrupted_filter_contribution = conv_weight[:,0,i].view(E,1)*padded_input_corrupted[:,:,i:i+L]\n",
    "            # [E]                                                    [E]\n",
    "            filter_contribution[batch,:,position] = corrupted_filter_contribution[0,:,position]\n",
    "        storage[filter_str] = filter_contribution\n",
    "        output += filter_contribution\n",
    "        #output += conv_weight[:,0,i].view(E,1)*conv_input\n",
    "        #if i == D_CONV-1:\n",
    "        #    output += conv_weight[:,0,i].view(E,1)*conv_input\n",
    "\n",
    "    # bias is not dependent on input so no reason to patch on it, just apply it as normal\n",
    "    output += conv_bias.view(E, 1)\n",
    "    \n",
    "    output = rearrange(output, 'B E L -> B L E')\n",
    "    return output\n",
    "\n",
    "\n",
    "# we do a hacky thing where this first hook clears the global storage\n",
    "# second hook stores all the hooks\n",
    "# then third hook computes the output (over all the hooks)\n",
    "# this avoids recomputing and so is much faster\n",
    "global storage\n",
    "global conv_storage\n",
    "storage = {}\n",
    "conv_storage = {}\n",
    "CONV_HOOKS = \"conv hooks\"\n",
    "CONV_BATCHES = \"conv batches\"\n",
    "def better_conv_patching_init_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    **kwargs\n",
    "):\n",
    "    #print(\"init hook with layer\", hook.name)\n",
    "    # we need to clear this here\n",
    "    # i tried having a \"current layer\" variable in the conv_storage that only clears when it doesn't match\n",
    "    # but that doesn't work if you only patch the same layer over and over,\n",
    "    # as stuff gets carried over\n",
    "    # this way of doing things is much safer and lets us assume it'll be empty\n",
    "    # well not quite, note that conv_patching_hook will be called with different batch_start and batch_end inputs during one forward pass\n",
    "    # so we need to account for that in the keys we use\n",
    "    global conv_storage\n",
    "    global storage\n",
    "    storage = {}\n",
    "    conv_storage = {CONV_BATCHES: set()}\n",
    "    return x\n",
    "\n",
    "def better_conv_patching_storage_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    conv_filter_i: int,\n",
    "    position: int,\n",
    "    layer: int,\n",
    "    batch: int,\n",
    "    **kwargs,\n",
    "):\n",
    "    #print(\"append hook with layer\", hook.name, \"conv filter\", conv_filter_i, \"position\", position, \"layer\", layer, \"batch\", batch)\n",
    "    batch_start = batch\n",
    "    batch_end = batch+1\n",
    "    global storage\n",
    "    storage[hook.name] = x\n",
    "    global conv_storage\n",
    "    hooks_key = (CONV_HOOKS, batch_start, batch_end)\n",
    "    if not hooks_key in conv_storage:\n",
    "        conv_storage[hooks_key] = [] # we can't do this above because it'll be emptied again on the next batch before this is called\n",
    "    conv_storage[hooks_key].append({\"position\": position, \"conv_filter_i\": conv_filter_i})\n",
    "    conv_storage[CONV_BATCHES].add((batch_start, batch_end))\n",
    "    return x\n",
    "\n",
    "from jaxtyping import Float\n",
    "from einops import rearrange\n",
    "global corrupted_activations\n",
    "\n",
    "global conv_storage\n",
    "def better_conv_patching_hook(\n",
    "    conv_output: Float[torch.Tensor, \"B L E\"],\n",
    "    hook: HookPoint,\n",
    "    input_hook_name: str,\n",
    "    layer: int,\n",
    "    **kwargs,\n",
    ") -> Float[torch.Tensor, \"B L E\"]:\n",
    "    global conv_storage\n",
    "    global storage\n",
    "    ### This is identical to what the conv is doing\n",
    "    # but we break it apart so we can patch on individual filters\n",
    "    \n",
    "    D_CONV = model.cfg.d_conv\n",
    "\n",
    "    global corrupted_activations\n",
    "    # [E,1,D_CONV]\n",
    "    conv_weight = model.blocks[layer].conv1d.weight\n",
    "    # [E]\n",
    "    conv_bias = model.blocks[layer].conv1d.bias\n",
    "    \n",
    "    # don't recompute these if we don't need to\n",
    "    # because we stored all the hooks and batches in conv_storage, we can just do them all at once\n",
    "    output_key = f'output' # they need to share an output because they write to the same output tensor\n",
    "    if not output_key in conv_storage:\n",
    "        #print(\"layer\", layer, \"keys\", conv_storage)\n",
    "        apply_to_all_hooks = [] # this is important because otherwise the [0:None] would overwrite the previous results (or vice versa)\n",
    "        apply_to_all_key = (CONV_HOOKS, 0, None)\n",
    "        if apply_to_all_key in conv_storage:\n",
    "            apply_to_all_hooks = conv_storage[apply_to_all_key]\n",
    "        for batch_start, batch_end in conv_storage[CONV_BATCHES]:\n",
    "            if batch_start == 0 and batch_end == None: continue # we cover this in the apply to all hooks above\n",
    "            def get_filter_key(i):\n",
    "                return f'filter_{i}'\n",
    "            conv_input_uncorrupted = storage[input_hook_name][batch_start:batch_end]\n",
    "            conv_input_corrupted = corrupted_activations[input_hook_name]\n",
    "            B, L, E = conv_input_uncorrupted.size()\n",
    "            \n",
    "            conv_input_uncorrupted = rearrange(conv_input_uncorrupted, 'B L E -> B E L')\n",
    "            conv_input_corrupted = rearrange(conv_input_corrupted, 'B L E -> B E L')\n",
    "            \n",
    "            # pad zeros in front\n",
    "            # [B,E,D_CONV-1+L]\n",
    "            padded_input_uncorrupted = torch.nn.functional.pad(conv_input_uncorrupted, (D_CONV-1,0), mode='constant', value=0)\n",
    "            padded_input_corrupted = torch.nn.functional.pad(conv_input_corrupted, (D_CONV-1,0), mode='constant', value=0)\n",
    "    \n",
    "            # compute the initial filter values\n",
    "            for i in range(D_CONV):\n",
    "                filter_key = get_filter_key(i)\n",
    "                # [B,E,L]                      [E,1]                      [B,E,L]\n",
    "                filter_contribution = conv_weight[:,0,i].view(E,1)*padded_input_uncorrupted[:,:,i:i+L]\n",
    "                conv_storage[filter_key] = filter_contribution\n",
    "            \n",
    "            # apply all the hooks\n",
    "            for hook in conv_storage[(CONV_HOOKS, batch_start, batch_end)] + apply_to_all_hooks:\n",
    "                position = hook['position']\n",
    "                conv_filter_i = hook['conv_filter_i']\n",
    "                #print(f\"position {position} conv_filter_i {conv_filter_i} batch_start {batch_start} batch_end {batch_end}\")\n",
    "                filter_key = get_filter_key(conv_filter_i)\n",
    "                # [1,E,L]                                   [E,1]                          # [B,E,L]\n",
    "                corrupted_filter_contribution = conv_weight[:,0,conv_filter_i].view(E,1)*padded_input_corrupted[:,:,conv_filter_i:conv_filter_i+L]\n",
    "                filter_contribution = conv_storage[filter_key]\n",
    "                if position is None:\n",
    "                    # [B,E,L]                    [B,E,L]\n",
    "                    filter_contribution = corrupted_filter_contribution\n",
    "                else:\n",
    "                    # [B,E]                                                  [B,E]\n",
    "                    filter_contribution[:,:,position] = corrupted_filter_contribution[:,:,position]\n",
    "                conv_storage[filter_key] = filter_contribution\n",
    "            \n",
    "            # compute the output\n",
    "            output = torch.zeros([B,E,L], device=model.cfg.device)\n",
    "            #print(f'B {B} B2 {B2} E {E} L {L} conv_storage keys {conv_storage.keys()} filter sizes {[(k,v.size()) for (k,v) in conv_storage.items() if not type(v) is int]}')\n",
    "            for i in range(D_CONV):\n",
    "                filter_key = get_filter_key(i)\n",
    "                output += conv_storage[filter_key]\n",
    "                del conv_storage[filter_key] # clean up now we are done with it, just to be safe\n",
    "            # bias is not dependent on input so no reason to patch on it, just apply it as normal\n",
    "            output += conv_bias.view(E, 1)\n",
    "            output = rearrange(output, 'B E L -> B L E')\n",
    "            # interleave it back with the corrupted as every other\n",
    "            conv_output[batch_start:batch_end] = output\n",
    "        conv_storage[output_key] = conv_output\n",
    "    return conv_storage[output_key]\n",
    "           \n",
    "\n",
    "\n",
    "## hooks for layer skipping\n",
    "def layer_input_storage_hook(\n",
    "    layer_input: Float[torch.Tensor, \"B L D\"],\n",
    "    hook: HookPoint,\n",
    "    layer: int,\n",
    ") -> Float[torch.Tensor, \"B L D\"]:\n",
    "    global storage\n",
    "    storage = {}\n",
    "    storage['layer_input'] = layer_input\n",
    "    return layer_input\n",
    "\n",
    "def layer_output_skipping_hook(\n",
    "    layer_output: Float[torch.Tensor, \"B L D\"],\n",
    "    hook: HookPoint,\n",
    "    position: int,\n",
    "    layer: int,\n",
    "    batch: int,\n",
    ") -> Float[torch.Tensor, \"B L D\"]:\n",
    "    global storage\n",
    "    layer_input = storage['layer_input']\n",
    "    # intervene on the batch at the position\n",
    "    layer_output[batch,position,:] = layer_input[batch,position,:]\n",
    "    return layer_output\n",
    "\n",
    "\n",
    "## hooks for h skipping\n",
    "def A_bar_storage_hook_for_skipping_h(\n",
    "    A_bar: Float[torch.Tensor, \"B L E N\"],\n",
    "    hook: HookPoint,\n",
    "    layer: int,\n",
    ") -> Float[torch.Tensor, \"B L E N\"]:\n",
    "    global storage\n",
    "    storage = {}\n",
    "    storage['A_bar'] = A_bar\n",
    "    return A_bar\n",
    "\n",
    "def skipping_h_hook(\n",
    "    h: Float[torch.Tensor, \"B E N\"],\n",
    "    hook: HookPoint,\n",
    "    position: int,\n",
    "    layer: int,\n",
    "    batch: int,\n",
    ") -> Float[torch.Tensor, \"B E N\"]:\n",
    "    #print(\"fetching\", storage[grab_pos][0,0,0:5], \"from position\", grab_pos)\n",
    "    #print(\"my value (being ignore) is\", h[0,0,0:5])\n",
    "    #print(f\"skipping ahead h at position {position}\")\n",
    "    global storage\n",
    "    B,E,N = h.size()\n",
    "    grab_pos = position-1\n",
    "    if grab_pos < 0:\n",
    "        h[batch,:,:] = torch.zeros((E,N), device=model.cfg.device)\n",
    "    else:\n",
    "        B,E,N = h.size()\n",
    "        A_contribution = torch.ones((E,N), device=model.cfg.device)\n",
    "        for missed_pos in range(grab_pos+1, position+1):\n",
    "            A_contribution *= storage['A_bar'][batch,missed_pos,:,:]\n",
    "        h_stored = storage[grab_pos][batch,:,:]\n",
    "        h[batch,:,:] = A_contribution*h_stored\n",
    "        #return A_contribution*storage[grab_pos]\n",
    "    storage[position] = h\n",
    "    return h\n",
    "\n",
    "\n",
    "## Regular patching hooks\n",
    "def position_patching_hook( # also works for B L E, B L E N, and B L N sized things\n",
    "    x: Float[torch.Tensor, \"B L D\"],\n",
    "    hook: HookPoint,\n",
    "    position: int,\n",
    "    layer: int, # we don't care about this\n",
    "    batch: int,\n",
    ") -> Float[torch.Tensor, \"B L D\"]:\n",
    "    # only intervene on the specific pos\n",
    "    corrupted_x = corrupted_activations[hook.name]\n",
    "    x[batch, position, :] = corrupted_x[0, position, :]\n",
    "    return x\n",
    "\n",
    "def h_patching_hook(\n",
    "    h: Float[torch.Tensor, \"B E N\"],\n",
    "    hook: HookPoint,\n",
    "    position: int,\n",
    "    layer: int,\n",
    "    batch: int,\n",
    ") -> Float[torch.Tensor, \"B E N\"]:\n",
    "    corrupted_h = corrupted_activations[hook.name]\n",
    "    h[batch] = corrupted_h[0]\n",
    "    return h\n",
    "\n",
    "def h_n_patching_hook(\n",
    "    h: Float[torch.Tensor, \"B E N\"],\n",
    "    hook: HookPoint,\n",
    "    position: int,\n",
    "    layer: int,\n",
    "    n: int,\n",
    "    batch: int,\n",
    ") -> Float[torch.Tensor, \"B E N\"]:\n",
    "    corrupted_h = corrupted_activations[hook.name]\n",
    "    h[batch,:,n] = corrupted_h[0,:,n]\n",
    "    return h\n",
    "\n",
    "SKIPPING_H_PATCHING = 'skipping h'\n",
    "H_N_PATCHING = 'h_n'\n",
    "LAYER_SKIPPING = 'skipping layer'\n",
    "CONV_FILTERS_PATCHING = 'conv filters'\n",
    "\n",
    "patching_types = {\n",
    "    'resid pre': (lambda layer, position: f'blocks.{layer}.hook_resid_pre', position_patching_hook),\n",
    "    'layer input': (lambda layer, position: f'blocks.{layer}.hook_layer_input', position_patching_hook),\n",
    "    'normalized input': (lambda layer, position: f'blocks.{layer}.hook_normalized_input', position_patching_hook),\n",
    "    'skip': (lambda layer, position: f'blocks.{layer}.hook_skip', position_patching_hook), \n",
    "    'in proj': (lambda layer, position: f'blocks.{layer}.hook_in_proj', position_patching_hook), \n",
    "    CONV_FILTERS_PATCHING: (lambda layer, position: f'blocks.{layer}.hook_conv', conv_patching_hook),\n",
    "    'conv': (lambda layer, position: f'blocks.{layer}.hook_conv', position_patching_hook), \n",
    "    'delta 1': (lambda layer, position: f'blocks.{layer}.hook_delta_1', position_patching_hook), \n",
    "    'delta 2': (lambda layer, position: f'blocks.{layer}.hook_delta_2', position_patching_hook), \n",
    "    'delta': (lambda layer, position: f'blocks.{layer}.hook_delta', position_patching_hook), \n",
    "    'A_bar': (lambda layer, position: f'blocks.{layer}.hook_A_bar', position_patching_hook), \n",
    "    'B': (lambda layer, position: f'blocks.{layer}.hook_B', position_patching_hook), \n",
    "    'B_bar': (lambda layer, position: f'blocks.{layer}.hook_B_bar', position_patching_hook), \n",
    "    'C': (lambda layer, position: f'blocks.{layer}.hook_C', position_patching_hook), \n",
    "    'ssm input': (lambda layer, position: f'blocks.{layer}.hook_ssm_input', position_patching_hook),\n",
    "    SKIPPING_H_PATCHING: (lambda layer, position: f'blocks.{layer}.hook_h.{position}', skipping_h_hook),\n",
    "    'h': (lambda layer, position: f'blocks.{layer}.hook_h.{position}', h_patching_hook),\n",
    "    H_N_PATCHING: (lambda layer, position: f'blocks.{layer}.hook_h.{position}', h_n_patching_hook),\n",
    "    'y': (lambda layer, position: f'blocks.{layer}.hook_y', position_patching_hook),\n",
    "    'ssm output': (lambda layer, position: f'blocks.{layer}.hook_ssm_output', position_patching_hook),\n",
    "    'after skip': (lambda layer, position: f'blocks.{layer}.hook_after_skip', position_patching_hook),\n",
    "    'out proj': (lambda layer, position: f'blocks.{layer}.hook_out_proj', position_patching_hook),\n",
    "    'resid post': (lambda layer, position: f'blocks.{layer}.hook_resid_post', position_patching_hook),\n",
    "    LAYER_SKIPPING: (lambda layer, position: f'blocks.{layer}.hook_resid_post', layer_output_skipping_hook),\n",
    "}\n",
    "\n",
    "patching_types_keys = list(patching_types.keys())\n",
    "\n",
    "def choose_patching_type(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_patching_type.patching_type = change['new'] # hack, gives this function the patching_type attribute\n",
    "\n",
    "choose_patching_type.patching_type = patching_types_keys[0]\n",
    "\n",
    "patching_type_dropdown = ipywidgets.Dropdown(\n",
    "    options=patching_types_keys,\n",
    "    value=patching_types_keys[0],\n",
    "    description='patching type',\n",
    ")\n",
    "patching_type_dropdown.observe(choose_patching_type)\n",
    "display(patching_type_dropdown)\n",
    "\n",
    "BATCH_SIZE_ALL = 'all'\n",
    "batch_size_keys = [BATCH_SIZE_ALL] + [str(b) for b in range(model.cfg.n_layers*model.cfg.D_conv*L)]\n",
    "\n",
    "def choose_batch_size(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_batch_size.batch_size = change['new']\n",
    "\n",
    "choose_batch_size.batch_size = batch_size_keys[0]\n",
    "\n",
    "choose_batch_size_dropdown = ipywidgets.Dropdown(\n",
    "    options=batch_size_keys,\n",
    "    value=batch_size_keys[0],\n",
    "    description='batch size',\n",
    ")\n",
    "choose_batch_size_dropdown.observe(choose_batch_size)\n",
    "display(choose_batch_size_dropdown)\n",
    "\n",
    "fast_conv_keys = ['True', 'False']\n",
    "\n",
    "def choose_fast_conv(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_fast_conv.fast_conv = change['new'] == 'True'\n",
    "\n",
    "choose_fast_conv.fast_conv = fast_conv_keys[0] == 'True'\n",
    "\n",
    "choose_fast_conv_dropdown = ipywidgets.Dropdown(\n",
    "    options=fast_conv_keys,\n",
    "    value=fast_conv_keys[0],\n",
    "    description='fast conv',\n",
    ")\n",
    "choose_fast_conv_dropdown.observe(choose_fast_conv)\n",
    "display(choose_fast_conv_dropdown)\n",
    "\n",
    "\n",
    "fast_ssm_keys = ['False', 'True']\n",
    "\n",
    "def choose_fast_ssm(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_fast_ssm.fast_ssm = change['new'] == 'True'\n",
    "\n",
    "choose_fast_ssm.fast_ssm = fast_ssm_keys[0] == 'True'\n",
    "\n",
    "choose_fast_ssm_dropdown = ipywidgets.Dropdown(\n",
    "    options=fast_ssm_keys,\n",
    "    value=fast_ssm_keys[0],\n",
    "    description='fast ssm',\n",
    ")\n",
    "choose_fast_ssm_dropdown.observe(choose_fast_ssm)\n",
    "display(choose_fast_ssm_dropdown)\n",
    "\n",
    "SHOW_PR = 'Pr'\n",
    "SHOW_LOGITS = 'Logits'\n",
    "SHOW_BOTH = 'Both'\n",
    "show_options = [SHOW_LOGITS, SHOW_PR, SHOW_BOTH]\n",
    "\n",
    "def choose_show_options(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_show_options.show_options = change['new']\n",
    "\n",
    "choose_show_options.show_options = show_options[0]\n",
    "\n",
    "show_options_dropdown = ipywidgets.Dropdown(\n",
    "    options=show_options,\n",
    "    value=show_options[0],\n",
    "    description='logits or pr',\n",
    ")\n",
    "show_options_dropdown.observe(choose_show_options)\n",
    "display(show_options_dropdown)\n",
    "\n",
    "\n",
    "\n",
    "def do_patching(arg, show_plot=True):\n",
    "    with output: # this lets the stuff we output here be visible\n",
    "        clear_output()\n",
    "        patching_type = choose_patching_type.patching_type\n",
    "        hook_name_func, hook_func = patching_types[patching_type]\n",
    "        return run_patching(\n",
    "                     patching_type=patching_type,\n",
    "                     patching_hook_name_func=hook_name_func,\n",
    "                     patching_hook_func=hook_func,\n",
    "                     batch_size=choose_batch_size.batch_size,\n",
    "                     fast_ssm=choose_fast_ssm.fast_ssm,\n",
    "                     fast_conv=choose_fast_conv.fast_conv,\n",
    "                     show_options=choose_show_options.show_options,\n",
    "                     show_plot=show_plot,\n",
    "                     min_logit_diff=min_logit_diff,\n",
    "                     max_logit_diff=max_logit_diff,\n",
    "                     min_pr_diff=min_pr_diff,\n",
    "                     max_pr_diff=max_pr_diff,\n",
    "                     token_labels=token_labels,\n",
    "                     prompt_uncorrupted_tokens=prompt_uncorrupted_tokens,\n",
    "                     uncorrupted_answer=uncorrupted_answer,\n",
    "                     corrupted_answer=corrupted_answer,\n",
    "                     always_hooks=generate_always_hooks())\n",
    "\n",
    "patching_button = ipywidgets.Button(description = 'Run Patching')\n",
    "patching_button.on_click(do_patching)\n",
    "display(patching_button)\n",
    "\n",
    "# you can't just display stuff inside a widget callback, you need a wrap any display code in this\n",
    "output = ipywidgets.Output()\n",
    "display(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
