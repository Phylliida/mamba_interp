{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Notebook\n",
    "\n",
    "This notebook contains the most relevant mech interp tools I've developed, centered around doing circuit analysis for any task\n",
    "\n",
    "It took me longer than 15 hours to develop all this! However I picked a fresh task (copy) I'd never studied on mamba before, and the 15 hours were spent copying code from my other notebooks into here/cleaning it up/analyzing the results/training some extra SAEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports/General Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires install of mamba lens\n",
    "# https://github.com/Phylliida/MambaLens\n",
    "# pip install git+https://github.com/Phylliida/MambaLens.git\n",
    "# and also my implementation of ACDC (used for dataset managment)\n",
    "# pip install git+https://github.com/Phylliida/ACDC.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import transformer_lens.utils as utils\n",
    "from mamba_lens import HookedMamba\n",
    "from acdc.data.utils import generate_dataset\n",
    "from tqdm import tqdm\n",
    "from acdc import get_pad_token\n",
    "from acdc import accuracy_metric\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from functools import partial\n",
    "from jaxtyping import Float\n",
    "from IPython.display import clear_output, display\n",
    "import ipywidgets\n",
    "from mamba_lens.input_dependent_hooks import clean_hooks\n",
    "from einops import rearrange\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "import traceback\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, HTML\n",
    "import pickle\n",
    "\n",
    "from acdc import (\n",
    "    Edge,\n",
    "    ACDCConfig,\n",
    "    LOG_LEVEL_INFO,\n",
    "    ACDCEvalData,\n",
    ")\n",
    "\n",
    "from acdc import ACDCEvalData\n",
    "from acdc import get_pad_token\n",
    "from acdc import accuracy_metric\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# from transformer lens\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        # Parse the PyTorch version to check if it's below version 2.0\n",
    "        major_version = int(torch.__version__.split(\".\")[0])\n",
    "        if major_version >= 2:\n",
    "            return torch.device(\"mps\")\n",
    "\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "# modified from neel nanda's examples\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", font_size=None, show=True, color_continuous_midpoint=0.0, fix_size=False, height=None, **kwargs):\n",
    "    fig = px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=color_continuous_midpoint, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs)\n",
    "    if not font_size is None:\n",
    "        if 'x' in kwargs:\n",
    "            fig.update_layout(\n",
    "              xaxis = dict(\n",
    "                tickmode='array',\n",
    "                tickvals = kwargs['x'],\n",
    "                ticktext = kwargs['x'], \n",
    "                ),\n",
    "               font=dict(size=font_size, color=\"black\"))\n",
    "        if 'y' in kwargs:\n",
    "            fig.update_layout(\n",
    "              yaxis = dict(\n",
    "                tickmode='array',\n",
    "                tickvals = kwargs['y'],\n",
    "                ticktext = kwargs['y'], \n",
    "                ),\n",
    "               font=dict(size=font_size, color=\"black\"))\n",
    "    if fix_size:\n",
    "        # default settings aren't very good, these are better\n",
    "        plot_args = {\n",
    "            'width': 800,\n",
    "            'height': 600,\n",
    "            \"autosize\": False,\n",
    "            'showlegend': True,\n",
    "            'margin': {\"l\":0,\"r\":0,\"t\":100,\"b\":0}\n",
    "        }\n",
    "        if model.cfg.n_layers < len(kwargs['y']):\n",
    "            plot_args['height'] *= model.cfg.D_conv\n",
    "        \n",
    "        if not height is None:\n",
    "            plot_args['height'] = height\n",
    "        fig.update_layout(**plot_args)\n",
    "        fig.update_layout(legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=0.01\n",
    "        ))\n",
    "    if show:\n",
    "        fig.show(renderer)\n",
    "    else:\n",
    "        return fig\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)\n",
    "\n",
    "def bar_chart(data, x_labels, y_label, title, font_size=None):\n",
    "    # it requires a pandas dict with the columns and rows named, annoying\n",
    "    # by default rows and columns are named with ints so we relabel them accordingly\n",
    "    renames = dict([(i, x_labels[i]) for i in range(len(x_labels))])\n",
    "    ps = pd.DataFrame(data.cpu().numpy()).rename(renames, axis='rows').rename({0: y_label}, axis='columns')\n",
    "    fig = px.bar(ps, y=y_label, x=x_labels, title=title)\n",
    "    if not font_size is None:\n",
    "        fig.update_layout(\n",
    "          xaxis = dict(\n",
    "            tickmode='array',\n",
    "            tickvals = x_labels,\n",
    "            ticktext = x_labels, \n",
    "            ),\n",
    "           font=dict(size=font_size, color=\"black\"))\n",
    "    fig.show()\n",
    "\n",
    "def get_batched_index_into(indices):\n",
    "    '''\n",
    "    given data that is [B,N,V] and indicies that are [B,N,K] with each index being an index into the V space\n",
    "    this gives you indexes you can use to access your values\n",
    "    '''\n",
    "    first_axis = []\n",
    "    second_axis = []\n",
    "    third_axis = []\n",
    "    B, _, _ = indices.size()\n",
    "    for b in range(B):\n",
    "        second, third = get_index_into(indices[b])\n",
    "        first_axis.append(torch.full(second.size(), fill_value=b, device=model.cfg.device))\n",
    "        second_axis.append(second)\n",
    "        third_axis.append(third)\n",
    "\n",
    "    return torch.cat(first_axis), torch.cat(second_axis), torch.cat(third_axis)\n",
    "\n",
    "def get_index_into(indices):\n",
    "    '''\n",
    "    given data that is [N,V] and indicies that are [N,K] with each index being an index into the V space\n",
    "    this gives you indexes you can use to access your values\n",
    "    '''\n",
    "    num_data, num_per_data = indices.size()\n",
    "    # we want\n",
    "    # [0,0,0,...,] num per data of these\n",
    "    # [1,1,1,...,] num per data of these\n",
    "    # ...\n",
    "    # [num_data-1, num_data-1, ...]\n",
    "    first_axis_index = torch.arange(num_data, dtype=torch.long).view(num_data, 1)*torch.ones([num_data, num_per_data], dtype=torch.long)\n",
    "    # now we flatten it so it has an index for each term aligned with our indices\n",
    "    first_axis_index = first_axis_index.flatten()\n",
    "    second_axis_index = indices.flatten()\n",
    "    return first_axis_index, second_axis_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.set_grad_enabled(False)\n",
    "device = get_device()\n",
    "print(\"device\", device)\n",
    "model = HookedMamba.from_pretrained(\"state-spaces/mamba-370m\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['<|endoftext|>', ' Electron', 'rising', ' inaccur', ' invisible', ',...,', ' proteins', ' Electron', 'rising', ' inaccur', ' invisible', ',...,', ' proteins', ' Electron', 'rising', ' inaccur', ' invisible', ',...,']\n",
      "answers   are [' proteins']\n",
      "incorrect are [' Love']\n",
      "['<|endoftext|>', ' Electron', 'rising', ' inaccur', ' invisible', ',...,', ' Love', ' Electron', 'rising', ' inaccur', ' invisible', ',...,', ' Love', ' Electron', 'rising', ' inaccur', ' invisible', ',...,']\n",
      "answers   are [' Love']\n",
      "incorrect are [' proteins']\n",
      "1\n",
      "['<|endoftext|>', ' appreciation', ' swarm', ' uncertainties', ' register', ' 1985', ' persists', ' appreciation', ' swarm', ' uncertainties', ' register', ' 1985', ' persists', ' appreciation', ' swarm', ' uncertainties', ' register', ' 1985']\n",
      "answers   are [' persists']\n",
      "incorrect are ['633']\n",
      "['<|endoftext|>', ' appreciation', ' swarm', ' uncertainties', ' register', ' 1985', '633', ' appreciation', ' swarm', ' uncertainties', ' register', ' 1985', '633', ' appreciation', ' swarm', ' uncertainties', ' register', ' 1985']\n",
      "answers   are ['633']\n",
      "incorrect are [' persists']\n",
      "2\n",
      "['<|endoftext|>', ' collected', 'Query', ' encoder', 'feedback', ' perspectives', ' Gallery', ' collected', 'Query', ' encoder', 'feedback', ' perspectives', ' Gallery', ' collected', 'Query', ' encoder', 'feedback', ' perspectives']\n",
      "answers   are [' Gallery']\n",
      "incorrect are [' preserving']\n",
      "['<|endoftext|>', ' collected', 'Query', ' encoder', 'feedback', ' perspectives', ' preserving', ' collected', 'Query', ' encoder', 'feedback', ' perspectives', ' preserving', ' collected', 'Query', ' encoder', 'feedback', ' perspectives']\n",
      "answers   are [' preserving']\n",
      "incorrect are [' Gallery']\n",
      "3\n",
      "['<|endoftext|>', ' ship', ' burns', ' vil', '\"});', ']\",', ' browser', ' ship', ' burns', ' vil', '\"});', ']\",', ' browser', ' ship', ' burns', ' vil', '\"});', ']\",']\n",
      "answers   are [' browser']\n",
      "incorrect are [' clustering']\n",
      "['<|endoftext|>', ' ship', ' burns', ' vil', '\"});', ']\",', ' clustering', ' ship', ' burns', ' vil', '\"});', ']\",', ' clustering', ' ship', ' burns', ' vil', '\"});', ']\",']\n",
      "answers   are [' clustering']\n",
      "incorrect are [' browser']\n",
      "4\n",
      "['<|endoftext|>', 'infty', ' %.', '498', ' Graduate', 'esty', 'atching', 'infty', ' %.', '498', ' Graduate', 'esty', 'atching', 'infty', ' %.', '498', ' Graduate', 'esty']\n",
      "answers   are ['atching']\n",
      "incorrect are [' NASA']\n",
      "['<|endoftext|>', 'infty', ' %.', '498', ' Graduate', 'esty', ' NASA', 'infty', ' %.', '498', ' Graduate', 'esty', ' NASA', 'infty', ' %.', '498', ' Graduate', 'esty']\n",
      "answers   are [' NASA']\n",
      "incorrect are ['atching']\n",
      "5\n",
      "['<|endoftext|>', ' Burk', ' nationalist', ' greens', 'greek', ' recorder', 'dll', ' Burk', ' nationalist', ' greens', 'greek', ' recorder', 'dll', ' Burk', ' nationalist', ' greens', 'greek', ' recorder']\n",
      "answers   are ['dll']\n",
      "incorrect are [' suit']\n",
      "['<|endoftext|>', ' Burk', ' nationalist', ' greens', 'greek', ' recorder', ' suit', ' Burk', ' nationalist', ' greens', 'greek', ' recorder', ' suit', ' Burk', ' nationalist', ' greens', 'greek', ' recorder']\n",
      "answers   are [' suit']\n",
      "incorrect are ['dll']\n",
      "6\n",
      "['<|endoftext|>', ' undergoing', 'HER', ' Westminster', ' addition', ' shook', ' genital', ' undergoing', 'HER', ' Westminster', ' addition', ' shook', ' genital', ' undergoing', 'HER', ' Westminster', ' addition', ' shook']\n",
      "answers   are [' genital']\n",
      "incorrect are ['ipple']\n",
      "['<|endoftext|>', ' undergoing', 'HER', ' Westminster', ' addition', ' shook', 'ipple', ' undergoing', 'HER', ' Westminster', ' addition', ' shook', 'ipple', ' undergoing', 'HER', ' Westminster', ' addition', ' shook']\n",
      "answers   are ['ipple']\n",
      "incorrect are [' genital']\n",
      "7\n",
      "['<|endoftext|>', '09', ' SARS', ' Bottom', 'results', ' Og', 'Bel', '09', ' SARS', ' Bottom', 'results', ' Og', 'Bel', '09', ' SARS', ' Bottom', 'results', ' Og']\n",
      "answers   are ['Bel']\n",
      "incorrect are [' Kyoto']\n",
      "['<|endoftext|>', '09', ' SARS', ' Bottom', 'results', ' Og', ' Kyoto', '09', ' SARS', ' Bottom', 'results', ' Og', ' Kyoto', '09', ' SARS', ' Bottom', 'results', ' Og']\n",
      "answers   are [' Kyoto']\n",
      "incorrect are ['Bel']\n",
      "8\n",
      "['<|endoftext|>', 'sim', ' Mumbai', ' Juda', 'Master', ' barr', 'ontrol', 'sim', ' Mumbai', ' Juda', 'Master', ' barr', 'ontrol', 'sim', ' Mumbai', ' Juda', 'Master', ' barr']\n",
      "answers   are ['ontrol']\n",
      "incorrect are [' sm']\n",
      "['<|endoftext|>', 'sim', ' Mumbai', ' Juda', 'Master', ' barr', ' sm', 'sim', ' Mumbai', ' Juda', 'Master', ' barr', ' sm', 'sim', ' Mumbai', ' Juda', 'Master', ' barr']\n",
      "answers   are [' sm']\n",
      "incorrect are ['ontrol']\n",
      "9\n",
      "['<|endoftext|>', 'duc', 'atura', 'Archive', ' Dig', ' age', ' nitric', 'duc', 'atura', 'Archive', ' Dig', ' age', ' nitric', 'duc', 'atura', 'Archive', ' Dig', ' age']\n",
      "answers   are [' nitric']\n",
      "incorrect are ['orms']\n",
      "['<|endoftext|>', 'duc', 'atura', 'Archive', ' Dig', ' age', 'orms', 'duc', 'atura', 'Archive', ' Dig', ' age', 'orms', 'duc', 'atura', 'Archive', ' Dig', ' age']\n",
      "answers   are ['orms']\n",
      "incorrect are [' nitric']\n"
     ]
    }
   ],
   "source": [
    "def decode_and_encode(tokenizer, tokens):\n",
    "    '''\n",
    "    Gets rid of weird encoding issues by encoding and decoding\n",
    "    The tokens will be different that's okay and intentional\n",
    "    '''\n",
    "    prompt = tokenizer.decode(tokens).encode(\"ascii\", \"ignore\").decode(\"ascii\", \"ignore\")\n",
    "    return tokenizer.encode(prompt)\n",
    "\n",
    "def copy_data_generator(tokenizer, num_patching_pairs, copy_seq_len, num_repeats):\n",
    "    '''\n",
    "    Generates copy_seq_len random tokens, repeated twice (with the last token cut off)\n",
    "    This is just a test to see if it can copy the repeated sequence from before\n",
    "\n",
    "    for example, \n",
    "\n",
    "    uncorrupted:\n",
    "    a b c d a b c (answer is d)\n",
    "    corrupted:\n",
    "    a b c e a b c (answer is e)\n",
    "    '''\n",
    "    first_len = None\n",
    "\n",
    "    # ignore special tokens like BOS or PAD\n",
    "    special_token_ids = set()\n",
    "    for special_token_name, token_str in model.tokenizer.special_tokens_map.items():\n",
    "        special_token_ids.add(model.tokenizer.convert_tokens_to_ids([token_str])[0])\n",
    "    valid_ids = []\n",
    "    for tok in range(tokenizer.vocab_size):\n",
    "        if not tok in special_token_ids:\n",
    "            valid_ids.append(tok)    \n",
    "    valid_ids = torch.tensor(valid_ids)\n",
    "    for i in list(range(num_patching_pairs)):\n",
    "        while True:\n",
    "            # sample without replacement\n",
    "            # one extra so corrupted isn't in sequence\n",
    "            data = valid_ids[torch.randperm(len(valid_ids))[:copy_seq_len+1]].flatten()\n",
    "            corrupted_id = data[-1]\n",
    "            data_repeating = data[:-1]\n",
    "            uncorrupted_data = torch.concatenate([data_repeating]*num_repeats + [data_repeating[:-1]])\n",
    "            corrupted_data = torch.concatenate([data_repeating[:-1], torch.tensor([corrupted_id])]*num_repeats + [data_repeating[:-1]])\n",
    "            # make sure it encodes and decodes properly\n",
    "            uncorrupted_prompt = tokenizer.decode(uncorrupted_data).encode(\"ascii\", \"ignore\").decode(\"ascii\", \"ignore\")\n",
    "            corrupted_prompt = tokenizer.decode(corrupted_data).encode(\"ascii\", \"ignore\").decode(\"ascii\", \"ignore\")\n",
    "            uncorrupted_answer = tokenizer.decode(data_repeating[-1]).encode(\"ascii\", \"ignore\").decode(\"ascii\", \"ignore\")\n",
    "            corrupted_answer = tokenizer.decode(torch.tensor([corrupted_id])).encode(\"ascii\", \"ignore\").decode(\"ascii\", \"ignore\")\n",
    "            reencoded_uncorrupted_data = torch.tensor(tokenizer.encode(uncorrupted_prompt))\n",
    "            reencoded_corrupted_data = torch.tensor(tokenizer.encode(corrupted_prompt))\n",
    "            if reencoded_uncorrupted_data.size() == uncorrupted_data.size() and reencoded_corrupted_data.size() == corrupted_data.size() and torch.all(reencoded_uncorrupted_data == uncorrupted_data) and torch.all(reencoded_corrupted_data == corrupted_data):\n",
    "                break\n",
    "        yield uncorrupted_prompt, [uncorrupted_answer], [corrupted_answer]\n",
    "        yield corrupted_prompt, [corrupted_answer], [uncorrupted_answer]\n",
    "\n",
    "num_patching_pairs = 200\n",
    "seed = 51\n",
    "valid_seed = 51\n",
    "constrain_to_answers = False\n",
    "has_symmetric_patching = True\n",
    "varying_data_lengths = False\n",
    "copy_seq_len = 6\n",
    "num_repeats = 2 # just immediately copying seems only somewhat doable by this model\n",
    "\n",
    "data = generate_dataset(model=model,\n",
    "              data_generator=copy_data_generator,\n",
    "              num_patching_pairs=num_patching_pairs,\n",
    "              seed=seed,\n",
    "              valid_seed=valid_seed,\n",
    "              constrain_to_answers=constrain_to_answers,\n",
    "              has_symmetric_patching=has_symmetric_patching, \n",
    "              varying_data_lengths=varying_data_lengths,\n",
    "              copy_seq_len=copy_seq_len,\n",
    "              num_repeats=num_repeats)\n",
    "\n",
    "for i in list(range(10)):\n",
    "    uncorrupted_i = i*2\n",
    "    corrupted_i = i*2+1\n",
    "    uncorrupted = data.data[uncorrupted_i][:data.last_token_position[uncorrupted_i]+1]\n",
    "    corrupted = data.data[corrupted_i][:data.last_token_position[corrupted_i]+1]\n",
    "    print(i)\n",
    "    print(model.to_str_tokens(uncorrupted))\n",
    "    print(f\"answers   are {model.to_str_tokens(data.correct[uncorrupted_i])}\")\n",
    "    print(f\"incorrect are {model.to_str_tokens(data.incorrect[uncorrupted_i])}\")\n",
    "    print(model.to_str_tokens(corrupted))\n",
    "    print(f\"answers   are {model.to_str_tokens(data.correct[corrupted_i])}\")\n",
    "    print(f\"incorrect are {model.to_str_tokens(data.incorrect[corrupted_i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"printing example data points:\")\n",
    "for b in range(4):\n",
    "    pad_token = get_pad_token(model.tokenizer)\n",
    "    # because there is padding if lengths vary, this only fetches the tokens that are part of the sequence\n",
    "    toks = data.data[b][:data.last_token_position[b]+1]\n",
    "    print(model.tokenizer.decode(toks))\n",
    "    for ind, tok in enumerate(data.correct[b]):\n",
    "        if tok != pad_token:\n",
    "            print(f\"  correct answer: {repr(model.tokenizer.decode([tok.item()]))}\")\n",
    "    for ind, tok in enumerate(data.incorrect[b]):\n",
    "        if tok != pad_token:\n",
    "            print(f\"  incorrect answer: {repr(model.tokenizer.decode([tok.item()]))}\")\n",
    "\n",
    "TOP_K = 5\n",
    "def logging_incorrect_metric(data: ACDCEvalData):\n",
    "    pad_token = get_pad_token(model.tokenizer)\n",
    "    for data_subset in [data.patched, data.corrupted]:\n",
    "        batch, _ = data_subset.data.size()\n",
    "        for b in range(batch):\n",
    "            if not data_subset.top_is_correct[b].item():\n",
    "                if not data.constrain_to_answers:\n",
    "                    logits = data_subset.logits[b]\n",
    "                    prs = torch.nn.functional.softmax(logits, dim=0)\n",
    "                    top = torch.argsort(-logits)\n",
    "                toks = data_subset.data[b][:data_subset.last_token_position[b]+1]\n",
    "                print(\"failed on this data point:\")\n",
    "                print(model.to_str_tokens(toks))\n",
    "                print(\"correct prs:\")\n",
    "                for i, tok in enumerate(data_subset.correct[b]):\n",
    "                    if tok.item() != pad_token:\n",
    "                        print(data_subset.correct_prs[b,i].item(), model.tokenizer.decode([tok.item()]))\n",
    "                        if not data.constrain_to_answers:\n",
    "                            top_k_pos = (top==tok.item()).nonzero().item()\n",
    "                            print(f\" top k pos of {top_k_pos}\")\n",
    "                print(\"incorrect prs:\")\n",
    "                for i, tok in enumerate(data_subset.incorrect[b]):\n",
    "                    if tok.item() != pad_token:\n",
    "                        print(data_subset.incorrect_prs[b,i].item(), model.tokenizer.decode([tok.item()]))\n",
    "                        if not data.constrain_to_answers:\n",
    "                            top_k_pos = (top==tok.item()).nonzero().item()\n",
    "                            print(f\" top k pos of {top_k_pos}\")\n",
    "                if not data.constrain_to_answers:\n",
    "                    for i, tok in enumerate(top[:TOP_K]):\n",
    "                        if tok.item() in [x.item() for x in data_subset.correct[b]]:\n",
    "                            print(f\"  correct   top {i} token {tok} = {repr(model.tokenizer.decode([tok]))} logit {logits[tok]} prs {prs[tok]}\")\n",
    "                        elif tok.item() in [x.item() for x in data_subset.incorrect[b]]:\n",
    "                            print(f\"  incorrect top {i} token {tok} = {repr(model.tokenizer.decode([tok]))} logit {logits[tok]} prs {prs[tok]}\")\n",
    "                        else:\n",
    "                            print(f\"  other     top {i} token {tok} = {repr(model.tokenizer.decode([tok]))} logit {logits[tok]} prs {prs[tok]}\")\n",
    "    return data.patched.correct_prs[:,0]\n",
    "\n",
    "pr_correct = data.eval(model=model, batch_size=10, metric=logging_incorrect_metric)\n",
    "print(pr_correct)\n",
    "print(torch.mean(pr_correct))\n",
    "accuracy = data.eval(model=model, batch_size=10, metric=accuracy_metric)\n",
    "print(accuracy)\n",
    "print(torch.mean(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for hparams of dataset to get good enough accuracy that we have some leeway for circuit analysis (also, is it sensitive? Seems no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_patching_pairs = 100\n",
    "seed = 42\n",
    "valid_seed = 41\n",
    "constrain_to_answers = False\n",
    "has_symmetric_patching = True\n",
    "varying_data_lengths = False\n",
    "\n",
    "max_seq_len = 40\n",
    "max_num_repeats = 10\n",
    "\n",
    "output_data = torch.zeros([max_seq_len-1, max_num_repeats-1])\n",
    "for i, copy_seq_len in enumerate(range(1,max_seq_len)):\n",
    "    print(copy_seq_len)\n",
    "    for j, num_repeats in enumerate(tqdm(range(1,max_num_repeats))):\n",
    "        seed = i+j*num_repeats\n",
    "        data = generate_dataset(model=model,\n",
    "                    data_generator=copy_data_generator,\n",
    "                    num_patching_pairs=num_patching_pairs,\n",
    "                    seed=seed,\n",
    "                    valid_seed=valid_seed,\n",
    "                    constrain_to_answers=constrain_to_answers,\n",
    "                    has_symmetric_patching=has_symmetric_patching, \n",
    "                    varying_data_lengths=varying_data_lengths,\n",
    "                    copy_seq_len=copy_seq_len,\n",
    "                    num_repeats=num_repeats)\n",
    "        output_data[i,j] = torch.mean(data.eval(model=model, batch_size=20, metric=accuracy_metric)).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(output_data.T, x=[str(x) for x in range(1,max_seq_len)], y=[str(y) for y in range(1,max_num_repeats)], fix_size=True, font_size=9, title='accuracy of mamba-370m on copy task', xaxis='num tokens in sequence', yaxis='num times repeated')\n",
    "\n",
    "# here we see that it is very good as long as sequence longer than 3 and it's repeated at least twice. It's alright for repeated once but I'd prefer starting with a higher accuracy.\n",
    "# I'll do 6 tokens in sequence, and repeat it 2 times (so 3 times total)\n",
    "# 1 hr 5 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_uncorrupted = model.tokenizer.decode(data.data[0][1:])\n",
    "prompt_corrupted = model.tokenizer.decode(data.data[1][1:])\n",
    "uncorrupted_answer = model.tokenizer.decode(data.correct[0])\n",
    "corrupted_answer = model.tokenizer.decode(data.correct[1])\n",
    "\n",
    "print(prompt_uncorrupted)\n",
    "print(prompt_corrupted)\n",
    "print(uncorrupted_answer)\n",
    "print(corrupted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from neel nanda's examples\n",
    "\n",
    "H_N_PATCHING_LAYER = 39\n",
    "\n",
    "# default settings aren't very good, these are better\n",
    "plot_args = {\n",
    "    'width': 800,\n",
    "    'height': 600,\n",
    "    \"autosize\": False,\n",
    "    'showlegend': True,\n",
    "    'margin': {\"l\":0,\"r\":0,\"t\":100,\"b\":0}\n",
    "}\n",
    "\n",
    "# you can modify this to only run things on a subset of layers\n",
    "limited_layers = list(range(model.cfg.n_layers))\n",
    "\n",
    "\n",
    "answer_tokens = sorted(list(set([model.tokenizer.encode(uncorrupted_answer)[0], model.tokenizer.encode(corrupted_answer)[0]])))\n",
    "\n",
    "prompt_uncorrupted_tokens = model.to_tokens(prompt_uncorrupted)\n",
    "prompt_corrupted_tokens = model.to_tokens(prompt_corrupted)\n",
    "\n",
    "# logits should be [B,L,V] \n",
    "def uncorrupted_logit_minus_corrupted_logit(logits, uncorrupted_answer, corrupted_answer):\n",
    "    uncorrupted_index = model.to_single_token(uncorrupted_answer)\n",
    "    corrupted_index = model.to_single_token(corrupted_answer)\n",
    "    return logits[0, -1, uncorrupted_index] - logits[0, -1, corrupted_index]\n",
    "\n",
    "# prs should be [B,L,V] \n",
    "def uncorrupted_pr_minus_corrupted_pr(prs, uncorrupted_answer, corrupted_answer):\n",
    "    uncorrupted_index = model.to_single_token(uncorrupted_answer)\n",
    "    corrupted_index = model.to_single_token(corrupted_answer)\n",
    "    return prs[0, -1, uncorrupted_index] - prs[0, -1, corrupted_index]\n",
    "\n",
    "\n",
    "\n",
    "# [B,L,V]\n",
    "corrupted_logits, corrupted_activations = model.run_with_cache(prompt_corrupted_tokens, only_use_these_layers=limited_layers)\n",
    "corrupted_logit_diff = uncorrupted_logit_minus_corrupted_logit(logits=corrupted_logits, uncorrupted_answer=uncorrupted_answer, corrupted_answer=corrupted_answer)\n",
    "corrupted_prs = torch.softmax(corrupted_logits, dim=2)\n",
    "corrupted_pr_diff = uncorrupted_pr_minus_corrupted_pr(prs=corrupted_prs, uncorrupted_answer=uncorrupted_answer, corrupted_answer=corrupted_answer)\n",
    "\n",
    "\n",
    "# [B,L,V]\n",
    "uncorrupted_logits = model.run_with_hooks(prompt_uncorrupted_tokens, only_use_these_layers=limited_layers)\n",
    "uncorrupted_logit_diff = uncorrupted_logit_minus_corrupted_logit(logits=uncorrupted_logits, uncorrupted_answer=uncorrupted_answer, corrupted_answer=corrupted_answer)\n",
    "uncorrupted_prs = torch.softmax(uncorrupted_logits, dim=2)\n",
    "uncorrupted_pr_diff = uncorrupted_pr_minus_corrupted_pr(prs=uncorrupted_prs, uncorrupted_answer=uncorrupted_answer, corrupted_answer=corrupted_answer)\n",
    "\n",
    "uncorrupted_index = model.to_single_token(uncorrupted_answer)\n",
    "corrupted_index = model.to_single_token(corrupted_answer)\n",
    "print(f'uncorrupted prompt\\n{prompt_uncorrupted}')\n",
    "print(f\"{repr(uncorrupted_answer)} logit {uncorrupted_logits[0,-1,uncorrupted_index]}\")\n",
    "print(f\"{repr(uncorrupted_answer)} pr {uncorrupted_prs[0,-1,uncorrupted_index]}\")\n",
    "print(f\"{repr(corrupted_answer)} logit {uncorrupted_logits[0,-1,corrupted_index]}\")\n",
    "print(f\"{repr(corrupted_answer)} pr {uncorrupted_prs[0,-1,corrupted_index]}\")\n",
    "print(f'\\ncorrupted prompt\\n{prompt_corrupted}')\n",
    "print(f\"{repr(uncorrupted_answer)} logit {corrupted_logits[0,-1,uncorrupted_index]}\")\n",
    "print(f\"{repr(uncorrupted_answer)} pr {corrupted_prs[0,-1,uncorrupted_index]}\")\n",
    "print(f\"{repr(corrupted_answer)} logit {corrupted_logits[0,-1,corrupted_index]}\")\n",
    "print(f\"{repr(corrupted_answer)} pr {corrupted_prs[0,-1,corrupted_index]}\")\n",
    "\n",
    "# We make a tensor to store the results for each patching run. We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.\n",
    "L = len(prompt_uncorrupted_tokens[0])\n",
    "if len(prompt_corrupted_tokens[0]) != len(prompt_uncorrupted_tokens[0]):\n",
    "    raise Exception(\"Prompts are not the same length\") # feel free to comment this out, you can patch for different sized prompts its just a lil sus\n",
    "\n",
    "# diff is logit of uncorrupted_answer - logit of corrupted_answer\n",
    "# we expect corrupted_diff to have a negative value (as corrupted should put high pr on corrupted_answer)\n",
    "# we expect uncorrupted to have a positive value (as uncorrupted should put high pr on uncorrupted_answer)\n",
    "# thus we can treat these as (rough) min and max possible values\n",
    "min_logit_diff = corrupted_logit_diff\n",
    "max_logit_diff = uncorrupted_logit_diff\n",
    "\n",
    "min_pr_diff = corrupted_pr_diff\n",
    "max_pr_diff = uncorrupted_pr_diff\n",
    "\n",
    "\n",
    "\n",
    "# make token labels that describe the patch\n",
    "corrupted_str_tokens = model.to_str_tokens(prompt_corrupted_tokens)\n",
    "uncorrupted_str_tokens = model.to_str_tokens(prompt_uncorrupted_tokens)\n",
    "token_labels = []\n",
    "for index, (corrupted_token, uncorrupted_token) in enumerate(zip(corrupted_str_tokens, uncorrupted_str_tokens)):\n",
    "    if corrupted_token == uncorrupted_token:\n",
    "        token_labels.append(f\"{corrupted_token}_{index}\")\n",
    "    else:\n",
    "        token_labels.append(f\"{uncorrupted_token}->{corrupted_token}_{index}\")\n",
    "\n",
    "def run_patching(patching_type,\n",
    "                 patching_hook_name_func,\n",
    "                 patching_hook_func,\n",
    "                 batch_size,\n",
    "                 show_options, \n",
    "                 min_logit_diff,\n",
    "                 max_logit_diff,\n",
    "                 min_pr_diff,\n",
    "                 max_pr_diff,\n",
    "                 token_labels,\n",
    "                 prompt_uncorrupted_tokens,\n",
    "                 uncorrupted_answer,\n",
    "                 corrupted_answer,\n",
    "                 always_hooks=None,\n",
    "                 show_plot=True,\n",
    "                 **kwargs):\n",
    "    _, L = prompt_uncorrupted_tokens.size()\n",
    "    torch.cuda.empty_cache()\n",
    "    hook_title = patching_hook_name_func(layer='{layer}', position='{position}')\n",
    "    print(f\"running patching {patching_type}, using hook {hook_title}\")\n",
    "    global patching_result_logits, patching_result_prs # if you want to access it once this is done running\n",
    "    n_layers = len(limited_layers)\n",
    "\n",
    "    num_results = n_layers\n",
    "    if patching_type == H_N_PATCHING:\n",
    "        print(f\"on layer H_N_PATCHING_LAYER={H_N_PATCHING_LAYER}\")\n",
    "        N = model.cfg.N\n",
    "        num_results = N\n",
    "    elif patching_type == CONV_FILTERS_PATCHING:\n",
    "        D_conv = model.cfg.D_conv\n",
    "        num_results = (D_conv-1)*n_layers # -1 because the zero one is always zero so we ignore it\n",
    "    \n",
    "    patching_result_normalized_logits = torch.zeros((num_results, L), device=model.cfg.device)\n",
    "    patching_result_normalized_prs = torch.zeros((num_results, L), device=model.cfg.device)\n",
    "\n",
    "    num_answers = len(answer_tokens)\n",
    "    patching_result_logits = torch.zeros((num_results, L, num_answers), device=model.cfg.device)\n",
    "    patching_result_prs = torch.zeros((num_results, L, num_answers), device=model.cfg.device)\n",
    "    \n",
    "    hooks = []\n",
    "    # skipping h needs A_bar stored, so also add that hook\n",
    "    if patching_type == SKIPPING_H_PATCHING:\n",
    "        for i, layer in list(enumerate(limited_layers)):\n",
    "            hooks.append((f'blocks.{layer}.hook_A_bar', partial(A_bar_storage_hook_for_skipping_h, layer=layer)))\n",
    "\n",
    "    # skipping layer needs layer_input (resid_pre) stored, so also add that hook\n",
    "    if patching_type == LAYER_SKIPPING:\n",
    "        for i, layer in list(enumerate(limited_layers)):\n",
    "            hooks.append((f'blocks.{layer}.hook_resid_pre', partial(layer_input_storage_hook, layer=layer)))\n",
    "    \n",
    "    # conv filters works via initializing things, then storing all the stuff we want to hook, then doing all that in place at the same time\n",
    "    if patching_type == CONV_FILTERS_PATCHING:\n",
    "        for i, layer in list(enumerate(limited_layers)):\n",
    "            # reset the storage to empty/initialize stuff\n",
    "            hooks.append((f'blocks.{layer}.hook_layer_input', better_conv_patching_init_hook))\n",
    "            # doing all the patches we have stored (below) at the same time\n",
    "            hooks.append((f'blocks.{layer}.hook_conv', partial(better_conv_patching_hook, input_hook_name=f'blocks.{layer}.hook_in_proj', layer=layer)))\n",
    "\n",
    "    if not always_hooks is None:\n",
    "        hooks += always_hooks\n",
    "\n",
    "    initial_num_hooks = len(hooks)\n",
    "\n",
    "    \n",
    "    if patching_type == H_N_PATCHING:\n",
    "        batch = 0\n",
    "        indices = []\n",
    "        for n in range(N):\n",
    "            for position in range(L):\n",
    "                patching_hook_name = patching_hook_name_func(layer=H_N_PATCHING_LAYER, position=position)\n",
    "                if batch_size != BATCH_SIZE_ALL: batch = batch % int(batch_size)\n",
    "                patching_hook = partial(patching_hook_func, layer=H_N_PATCHING_LAYER, position=position, n=n, batch=batch)\n",
    "                batch += 1\n",
    "                indices.append((n,position))\n",
    "                hooks.append((patching_hook_name, patching_hook))\n",
    "    elif patching_type == CONV_FILTERS_PATCHING:\n",
    "        batch = 0\n",
    "        indices = []\n",
    "        D_conv = model.cfg.D_conv\n",
    "        ind = 0\n",
    "        for i, layer in list(enumerate(limited_layers)):\n",
    "            for conv_filter_i in range(D_conv):\n",
    "                if conv_filter_i == 0: continue # this -d_conv-1 filter is always zero for some reason\n",
    "                for position in range(L):\n",
    "                    patching_hook_name = patching_hook_name_func(layer=layer, position=position)\n",
    "                    if batch_size != BATCH_SIZE_ALL: batch = batch % int(batch_size)\n",
    "                    hooks.append((f'blocks.{layer}.hook_in_proj', partial(better_conv_patching_storage_hook, position=position, layer=layer, conv_filter_i=conv_filter_i, batch=batch)))\n",
    "                    #patching_hook = partial(patching_hook_func, layer=layer, position=position, batch=batch, conv_filter_i=conv_filter_i)\n",
    "                    batch += 1\n",
    "                    indices.append((ind,position))\n",
    "                    #hooks.append((patching_hook_name, patching_hook))\n",
    "                ind += 1\n",
    "    else:\n",
    "        batch = 0\n",
    "        indices = []\n",
    "        for i, layer in list(enumerate(limited_layers)):\n",
    "            for position in range(L):\n",
    "                patching_hook_name = patching_hook_name_func(layer=layer, position=position)\n",
    "                if batch_size != BATCH_SIZE_ALL: batch = batch % int(batch_size)\n",
    "                patching_hook = partial(patching_hook_func, layer=layer, position=position, batch=batch)\n",
    "                batch += 1\n",
    "                indices.append((i,position))\n",
    "                hooks.append((patching_hook_name, patching_hook))\n",
    "\n",
    "    \n",
    "    if batch_size != BATCH_SIZE_ALL:\n",
    "        V = model.cfg.V\n",
    "        patched_logits = torch.zeros([len(indices), L, V])\n",
    "        for batch_start in tqdm(list(range(0, len(indices), int(batch_size)))):\n",
    "            batch_end = min(len(indices), batch_start+int(batch_size))\n",
    "            # always do the initial they are for storage\n",
    "            batch_hooks = hooks[:initial_num_hooks] + hooks[initial_num_hooks+batch_start:initial_num_hooks+batch_end]\n",
    "            cur_batch_size = batch_end-batch_start\n",
    "            patched_logits[batch_start:batch_end] = model.run_with_hooks(prompt_uncorrupted_tokens.expand(cur_batch_size,L), fwd_hooks=batch_hooks, only_use_these_layers=limited_layers, **kwargs)\n",
    "    else:\n",
    "        # [B,L,V]\n",
    "        patched_logits = model.run_with_hooks(prompt_uncorrupted_tokens.expand(batch,L), fwd_hooks=hooks, only_use_these_layers=limited_layers, **kwargs)\n",
    "   \n",
    "    # [B,L,V]\n",
    "    patched_prs = torch.softmax(patched_logits, dim=2)\n",
    "    print(\"finished patching, plotting...\")\n",
    "    for b, (i,position) in enumerate(indices):\n",
    "        if corrupted_answer != uncorrupted_answer:\n",
    "            patched_logit_diff = uncorrupted_logit_minus_corrupted_logit(logits=patched_logits[b:b+1],\n",
    "                                                                         uncorrupted_answer=uncorrupted_answer,\n",
    "                                                                         corrupted_answer=corrupted_answer)\n",
    "            # normalize it so\n",
    "            # 0 means min_logit_diff (so 0 means that it is acting like the corrupted model)\n",
    "            # 1 means max_logit_diff (so 1 means that it is acting like the uncorrupted model)\n",
    "            normalized_patched_logit_diff = (patched_logit_diff-min_logit_diff)/(max_logit_diff - min_logit_diff)\n",
    "            # now flip them, since most interventions will do nothing and thus act like uncorrupted model, visually its better to have that at 0\n",
    "            # so now\n",
    "            # 0 means that it is acting like the uncorrupted model\n",
    "            # 1 means that it is acting like the corrupted model\n",
    "            normalized_patched_logit_diff = 1.0 - normalized_patched_logit_diff\n",
    "            normalized_patched_logit_diff = normalized_patched_logit_diff #normalized_always_logit_diff - normalized_patched_logit_diff\n",
    "            patching_result_normalized_logits[i, position] = normalized_patched_logit_diff\n",
    "            \n",
    "            # same for pr\n",
    "            patched_pr_diff = uncorrupted_pr_minus_corrupted_pr(prs=patched_prs[b:b+1],\n",
    "                                                                uncorrupted_answer=uncorrupted_answer,\n",
    "                                                                corrupted_answer=corrupted_answer)\n",
    "            normalized_patched_pr_diff = 1.0-(patched_pr_diff-min_pr_diff)/(max_pr_diff - min_pr_diff)\n",
    "            normalized_patched_pr_diff = normalized_always_pr_diff - normalized_patched_pr_diff\n",
    "            patching_result_normalized_prs[i, position] = normalized_patched_pr_diff\n",
    "\n",
    "        for k, answer_token in enumerate(answer_tokens):\n",
    "            patching_result_logits[i, position, k] = patched_logits[b,-1,answer_token]\n",
    "            patching_result_prs[i, position, k] = patched_prs[b,-1,answer_token]\n",
    "    \n",
    "        \n",
    "    if patching_type == H_N_PATCHING:\n",
    "        layer_labels = [str(n) for n in range(N)]\n",
    "    elif patching_type == CONV_FILTERS_PATCHING:\n",
    "        layer_labels = []\n",
    "        for layer in limited_layers:\n",
    "            for conv_i in range(1, D_conv):\n",
    "                layer_labels.append(f\"layer {layer} conv {conv_i-D_conv+1}\")\n",
    "    else:\n",
    "        layer_labels = [str(layer) for layer in limited_layers]\n",
    "    figs = []\n",
    "    y_axis = 'Layer'\n",
    "    if patching_type == H_N_PATCHING:\n",
    "        y_axis = 'N'\n",
    "    if show_plot:\n",
    "        if corrupted_answer != uncorrupted_answer:\n",
    "            if show_options in [SHOW_LOGITS, SHOW_BOTH]:\n",
    "                figs.append(imshow(patching_result_normalized_logits, x=token_labels, y=layer_labels, xaxis=\"Position\", yaxis=y_axis, title=f\"Normalized logit difference after patching {patching_type} using hook {hook_title}\", font_size=8, show=False))\n",
    "            if show_options in [SHOW_PR, SHOW_BOTH]:\n",
    "                figs.append(imshow(patching_result_normalized_prs, x=token_labels, y=layer_labels, xaxis=\"Position\", yaxis=y_axis, title=f\"Normalized pr difference after patching {patching_type} using hook {hook_title}\", font_size=8, show=False))\n",
    "        \n",
    "        for k, answer_token in enumerate(answer_tokens):\n",
    "            if show_options in [SHOW_LOGITS, SHOW_BOTH]:\n",
    "                figs.append(imshow(patching_result_logits[:,:,k], color_continuous_midpoint=None, x=token_labels, y=layer_labels, xaxis=\"Position\", yaxis=\"Layer\", title=f\"Logit of uncorrupted answer {repr(model.tokenizer.decode([answer_token]))} after patching {patching_type} using hook {hook_title}\", font_size=8, show=False))\n",
    "            if show_options in [SHOW_PR, SHOW_BOTH]:\n",
    "                figs.append(imshow(patching_result_prs[:,:,k], x=token_labels, y=layer_labels, xaxis=\"Position\", yaxis=y_axis, title=f\"Pr of uncorrupted answer {repr(model.tokenizer.decode([answer_token]))} after patching {patching_type} using hook {hook_title}\", font_size=8, show=False)) \n",
    "        \n",
    "        for fig in figs:\n",
    "            plot_args_copy = dict(list(plot_args.items()))\n",
    "            if patching_type == CONV_FILTERS_PATCHING:\n",
    "                plot_args_copy['height'] *= D_conv\n",
    "            fig.update_layout(**plot_args_copy)\n",
    "            fig.update_layout(legend=dict(\n",
    "                yanchor=\"top\",\n",
    "                y=0.99,\n",
    "                xanchor=\"left\",\n",
    "                x=0.01\n",
    "            ))\n",
    "            fig.show()\n",
    "    else:\n",
    "        return layer_labels, y_axis, patching_result_normalized_logits, patching_result_normalized_prs, patching_result_logits, patching_result_prs\n",
    "\n",
    "## hooks for conv filter patching\n",
    "def conv_input_storage_hook(\n",
    "    conv_input: Float[torch.Tensor, \"B L E\"],\n",
    "    hook: HookPoint,\n",
    "    layer: int,\n",
    ") -> Float[torch.Tensor, \"B L E\"]:\n",
    "    global progress # it's slow enough that progress bar is useful\n",
    "    if layer == 0:\n",
    "        progress = tqdm(total=len(limited_layers))\n",
    "    else:\n",
    "        progress.update(1)\n",
    "    global storage\n",
    "    storage = {}\n",
    "    storage['conv_input'] = conv_input\n",
    "    return conv_input\n",
    "\n",
    "def conv_patching_hook(\n",
    "    conv_output: Float[torch.Tensor, \"B L E\"],\n",
    "    hook: HookPoint,\n",
    "    layer: int,\n",
    "    position: int,\n",
    "    batch: int,\n",
    "    conv_filter_i: int,\n",
    ") -> Float[torch.Tensor, \"B L E\"]:\n",
    "    global storage\n",
    "    conv_input = storage['conv_input']\n",
    "    B, L, E = conv_input.size()\n",
    "    conv_input = rearrange(conv_input, 'B L E -> B E L')\n",
    "    conv_input_corrupted = rearrange(corrupted_activations[f'blocks.{layer}.hook_in_proj'], 'B L E -> B E L')\n",
    "    \n",
    "    ### This is identical to what the conv is doing\n",
    "    # pad zeros in front\n",
    "    # [B,E,D_CONV-1+L]\n",
    "    D_CONV = model.cfg.d_conv\n",
    "    padded_input = torch.nn.functional.pad(conv_input, (D_CONV-1,0), mode='constant', value=0)\n",
    "    padded_input_corrupted = torch.nn.functional.pad(conv_input_corrupted, (D_CONV-1,0), mode='constant', value=0)\n",
    "    output = torch.zeros([B,E,L], device=model.cfg.device)\n",
    "    # [E,1,D_CONV]\n",
    "    conv_weight = model.blocks[layer].conv1d.weight\n",
    "    # [E]\n",
    "    conv_bias = model.blocks[layer].conv1d.bias\n",
    "    # this is inefficient because its recomputing things every time\n",
    "    # but I don't want to have to rely on the ordering of hooks because that's sus\n",
    "    # so this is good enough\n",
    "    for i in range(D_CONV):\n",
    "        filter_str = f'filter_{i}'\n",
    "        if not filter_str in storage:\n",
    "            # [B,E,L]                      [E,1]                      [B,E,L]\n",
    "            filter_contribution = conv_weight[:,0,i].view(E,1)*padded_input[:,:,i:i+L]\n",
    "            storage[filter_str] = filter_contribution\n",
    "        filter_contribution = storage[filter_str]\n",
    "        if i == conv_filter_i:\n",
    "            # [1,E,L]                                   [E,1]                          # [1,E,L]\n",
    "            corrupted_filter_contribution = conv_weight[:,0,i].view(E,1)*padded_input_corrupted[:,:,i:i+L]\n",
    "            # [E]                                                    [E]\n",
    "            filter_contribution[batch,:,position] = corrupted_filter_contribution[0,:,position]\n",
    "        storage[filter_str] = filter_contribution\n",
    "        output += filter_contribution\n",
    "        #output += conv_weight[:,0,i].view(E,1)*conv_input\n",
    "        #if i == D_CONV-1:\n",
    "        #    output += conv_weight[:,0,i].view(E,1)*conv_input\n",
    "\n",
    "    # bias is not dependent on input so no reason to patch on it, just apply it as normal\n",
    "    output += conv_bias.view(E, 1)\n",
    "    \n",
    "    output = rearrange(output, 'B E L -> B L E')\n",
    "    return output\n",
    "\n",
    "\n",
    "# we do a hacky thing where this first hook clears the global storage\n",
    "# second hook stores all the hooks\n",
    "# then third hook computes the output (over all the hooks)\n",
    "# this avoids recomputing and so is much faster\n",
    "global storage\n",
    "global conv_storage\n",
    "storage = {}\n",
    "conv_storage = {}\n",
    "CONV_HOOKS = \"conv hooks\"\n",
    "CONV_BATCHES = \"conv batches\"\n",
    "def better_conv_patching_init_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    **kwargs\n",
    "):\n",
    "    #print(\"init hook with layer\", hook.name)\n",
    "    # we need to clear this here\n",
    "    # i tried having a \"current layer\" variable in the conv_storage that only clears when it doesn't match\n",
    "    # but that doesn't work if you only patch the same layer over and over,\n",
    "    # as stuff gets carried over\n",
    "    # this way of doing things is much safer and lets us assume it'll be empty\n",
    "    # well not quite, note that conv_patching_hook will be called with different batch_start and batch_end inputs during one forward pass\n",
    "    # so we need to account for that in the keys we use\n",
    "    global conv_storage\n",
    "    global storage\n",
    "    storage = {}\n",
    "    conv_storage = {CONV_BATCHES: set()}\n",
    "    return x\n",
    "\n",
    "def better_conv_patching_storage_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    conv_filter_i: int,\n",
    "    position: int,\n",
    "    layer: int,\n",
    "    batch: int,\n",
    "    **kwargs,\n",
    "):\n",
    "    #print(\"append hook with layer\", hook.name, \"conv filter\", conv_filter_i, \"position\", position, \"layer\", layer, \"batch\", batch)\n",
    "    batch_start = batch\n",
    "    batch_end = batch+1\n",
    "    global storage\n",
    "    storage[hook.name] = x\n",
    "    global conv_storage\n",
    "    hooks_key = (CONV_HOOKS, batch_start, batch_end)\n",
    "    if not hooks_key in conv_storage:\n",
    "        conv_storage[hooks_key] = [] # we can't do this above because it'll be emptied again on the next batch before this is called\n",
    "    conv_storage[hooks_key].append({\"position\": position, \"conv_filter_i\": conv_filter_i})\n",
    "    conv_storage[CONV_BATCHES].add((batch_start, batch_end))\n",
    "    return x\n",
    "\n",
    "from jaxtyping import Float\n",
    "from einops import rearrange\n",
    "global corrupted_activations\n",
    "\n",
    "global conv_storage\n",
    "def better_conv_patching_hook(\n",
    "    conv_output: Float[torch.Tensor, \"B L E\"],\n",
    "    hook: HookPoint,\n",
    "    input_hook_name: str,\n",
    "    layer: int,\n",
    "    **kwargs,\n",
    ") -> Float[torch.Tensor, \"B L E\"]:\n",
    "    global conv_storage\n",
    "    global storage\n",
    "    ### This is identical to what the conv is doing\n",
    "    # but we break it apart so we can patch on individual filters\n",
    "    \n",
    "    D_CONV = model.cfg.d_conv\n",
    "\n",
    "    global corrupted_activations\n",
    "    # [E,1,D_CONV]\n",
    "    conv_weight = model.blocks[layer].conv1d.weight\n",
    "    # [E]\n",
    "    conv_bias = model.blocks[layer].conv1d.bias\n",
    "    \n",
    "    # don't recompute these if we don't need to\n",
    "    # because we stored all the hooks and batches in conv_storage, we can just do them all at once\n",
    "    output_key = f'output' # they need to share an output because they write to the same output tensor\n",
    "    if not output_key in conv_storage:\n",
    "        #print(\"layer\", layer, \"keys\", conv_storage)\n",
    "        apply_to_all_hooks = [] # this is important because otherwise the [0:None] would overwrite the previous results (or vice versa)\n",
    "        apply_to_all_key = (CONV_HOOKS, 0, None)\n",
    "        if apply_to_all_key in conv_storage:\n",
    "            apply_to_all_hooks = conv_storage[apply_to_all_key]\n",
    "        for batch_start, batch_end in conv_storage[CONV_BATCHES]:\n",
    "            if batch_start == 0 and batch_end == None: continue # we cover this in the apply to all hooks above\n",
    "            def get_filter_key(i):\n",
    "                return f'filter_{i}'\n",
    "            conv_input_uncorrupted = storage[input_hook_name][batch_start:batch_end]\n",
    "            conv_input_corrupted = corrupted_activations[input_hook_name]\n",
    "            B, L, E = conv_input_uncorrupted.size()\n",
    "            \n",
    "            conv_input_uncorrupted = rearrange(conv_input_uncorrupted, 'B L E -> B E L')\n",
    "            conv_input_corrupted = rearrange(conv_input_corrupted, 'B L E -> B E L')\n",
    "            \n",
    "            # pad zeros in front\n",
    "            # [B,E,D_CONV-1+L]\n",
    "            padded_input_uncorrupted = torch.nn.functional.pad(conv_input_uncorrupted, (D_CONV-1,0), mode='constant', value=0)\n",
    "            padded_input_corrupted = torch.nn.functional.pad(conv_input_corrupted, (D_CONV-1,0), mode='constant', value=0)\n",
    "    \n",
    "            # compute the initial filter values\n",
    "            for i in range(D_CONV):\n",
    "                filter_key = get_filter_key(i)\n",
    "                # [B,E,L]                      [E,1]                      [B,E,L]\n",
    "                filter_contribution = conv_weight[:,0,i].view(E,1)*padded_input_uncorrupted[:,:,i:i+L]\n",
    "                conv_storage[filter_key] = filter_contribution\n",
    "            \n",
    "            # apply all the hooks\n",
    "            for hook in conv_storage[(CONV_HOOKS, batch_start, batch_end)] + apply_to_all_hooks:\n",
    "                position = hook['position']\n",
    "                conv_filter_i = hook['conv_filter_i']\n",
    "                #print(f\"position {position} conv_filter_i {conv_filter_i} batch_start {batch_start} batch_end {batch_end}\")\n",
    "                filter_key = get_filter_key(conv_filter_i)\n",
    "                # [1,E,L]                                   [E,1]                          # [B,E,L]\n",
    "                corrupted_filter_contribution = conv_weight[:,0,conv_filter_i].view(E,1)*padded_input_corrupted[:,:,conv_filter_i:conv_filter_i+L]\n",
    "                filter_contribution = conv_storage[filter_key]\n",
    "                if position is None:\n",
    "                    # [B,E,L]                    [B,E,L]\n",
    "                    filter_contribution = corrupted_filter_contribution\n",
    "                else:\n",
    "                    # [B,E]                                                  [B,E]\n",
    "                    filter_contribution[:,:,position] = corrupted_filter_contribution[:,:,position]\n",
    "                conv_storage[filter_key] = filter_contribution\n",
    "            \n",
    "            # compute the output\n",
    "            output = torch.zeros([B,E,L], device=model.cfg.device)\n",
    "            #print(f'B {B} B2 {B2} E {E} L {L} conv_storage keys {conv_storage.keys()} filter sizes {[(k,v.size()) for (k,v) in conv_storage.items() if not type(v) is int]}')\n",
    "            for i in range(D_CONV):\n",
    "                filter_key = get_filter_key(i)\n",
    "                output += conv_storage[filter_key]\n",
    "                del conv_storage[filter_key] # clean up now we are done with it, just to be safe\n",
    "            # bias is not dependent on input so no reason to patch on it, just apply it as normal\n",
    "            output += conv_bias.view(E, 1)\n",
    "            output = rearrange(output, 'B E L -> B L E')\n",
    "            # interleave it back with the corrupted as every other\n",
    "            conv_output[batch_start:batch_end] = output\n",
    "        conv_storage[output_key] = conv_output\n",
    "    return conv_storage[output_key]\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## hooks for layer skipping\n",
    "def layer_input_storage_hook(\n",
    "    layer_input: Float[torch.Tensor, \"B L D\"],\n",
    "    hook: HookPoint,\n",
    "    layer: int,\n",
    ") -> Float[torch.Tensor, \"B L D\"]:\n",
    "    global storage\n",
    "    storage = {}\n",
    "    storage['layer_input'] = layer_input\n",
    "    return layer_input\n",
    "\n",
    "def layer_output_skipping_hook(\n",
    "    layer_output: Float[torch.Tensor, \"B L D\"],\n",
    "    hook: HookPoint,\n",
    "    position: int,\n",
    "    layer: int,\n",
    "    batch: int,\n",
    ") -> Float[torch.Tensor, \"B L D\"]:\n",
    "    global storage\n",
    "    layer_input = storage['layer_input']\n",
    "    # intervene on the batch at the position\n",
    "    layer_output[batch,position,:] = layer_input[batch,position,:]\n",
    "    return layer_output\n",
    "\n",
    "\n",
    "## hooks for h skipping\n",
    "def A_bar_storage_hook_for_skipping_h(\n",
    "    A_bar: Float[torch.Tensor, \"B L E N\"],\n",
    "    hook: HookPoint,\n",
    "    layer: int,\n",
    ") -> Float[torch.Tensor, \"B L E N\"]:\n",
    "    global storage\n",
    "    storage = {}\n",
    "    storage['A_bar'] = A_bar\n",
    "    return A_bar\n",
    "\n",
    "def skipping_h_hook(\n",
    "    h: Float[torch.Tensor, \"B E N\"],\n",
    "    hook: HookPoint,\n",
    "    position: int,\n",
    "    layer: int,\n",
    "    batch: int,\n",
    ") -> Float[torch.Tensor, \"B E N\"]:\n",
    "    #print(\"fetching\", storage[grab_pos][0,0,0:5], \"from position\", grab_pos)\n",
    "    #print(\"my value (being ignore) is\", h[0,0,0:5])\n",
    "    #print(f\"skipping ahead h at position {position}\")\n",
    "    global storage\n",
    "    B,E,N = h.size()\n",
    "    grab_pos = position-1\n",
    "    if grab_pos < 0:\n",
    "        h[batch,:,:] = torch.zeros((E,N), device=model.cfg.device)\n",
    "    else:\n",
    "        B,E,N = h.size()\n",
    "        A_contribution = torch.ones((E,N), device=model.cfg.device)\n",
    "        for missed_pos in range(grab_pos+1, position+1):\n",
    "            A_contribution *= storage['A_bar'][batch,missed_pos,:,:]\n",
    "        h_stored = storage[grab_pos][batch,:,:]\n",
    "        h[batch,:,:] = A_contribution*h_stored\n",
    "        #return A_contribution*storage[grab_pos]\n",
    "    storage[position] = h\n",
    "    return h\n",
    "\n",
    "\n",
    "## Regular patching hooks\n",
    "def position_patching_hook( # also works for B L E, B L E N, and B L N sized things\n",
    "    x: Float[torch.Tensor, \"B L D\"],\n",
    "    hook: HookPoint,\n",
    "    position: int,\n",
    "    layer: int, # we don't care about this\n",
    "    batch: int,\n",
    ") -> Float[torch.Tensor, \"B L D\"]:\n",
    "    # only intervene on the specific pos\n",
    "    corrupted_x = corrupted_activations[hook.name]\n",
    "    x[batch, position, :] = corrupted_x[0, position, :]\n",
    "    return x\n",
    "\n",
    "def h_patching_hook(\n",
    "    h: Float[torch.Tensor, \"B E N\"],\n",
    "    hook: HookPoint,\n",
    "    position: int,\n",
    "    layer: int,\n",
    "    batch: int,\n",
    ") -> Float[torch.Tensor, \"B E N\"]:\n",
    "    corrupted_h = corrupted_activations[hook.name]\n",
    "    h[batch] = corrupted_h[0]\n",
    "    return h\n",
    "\n",
    "def h_n_patching_hook(\n",
    "    h: Float[torch.Tensor, \"B E N\"],\n",
    "    hook: HookPoint,\n",
    "    position: int,\n",
    "    layer: int,\n",
    "    n: int,\n",
    "    batch: int,\n",
    ") -> Float[torch.Tensor, \"B E N\"]:\n",
    "    corrupted_h = corrupted_activations[hook.name]\n",
    "    h[batch,:,n] = corrupted_h[0,:,n]\n",
    "    return h\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_always_hooks():\n",
    "    hooks = []\n",
    "\n",
    "    LAYER = 39\n",
    "    _, L = prompt_uncorrupted_tokens.size()\n",
    "    #for pos in range(L):\n",
    "        # slice(None, None) is same as : (which means all)\n",
    "        #hooks.append((f'blocks.{LAYER}.hook_h.{pos}', partial(h_patching_hook, layer=LAYER, position=pos, batch=slice(None, None))))\n",
    "    #    hooks.append((f'blocks.{LAYER}.hook_layer_input', partial(position_patching_hook, layer=LAYER, position=pos, batch=slice(None, None))))\n",
    "\n",
    "    #ABLATE_POS = 3\n",
    "    #hooks.append((f'blocks.{35}.hook_layer_input', partial(position_patching_hook, layer=LAYER, position=3, batch=slice(None, None))))\n",
    "    #hooks.append((f'blocks.{40}.hook_layer_input', partial(position_patching_hook, layer=LAYER, position=3, batch=slice(None, None))))\n",
    "    #FINAL_POS = 19\n",
    "    #hooks.append((f'blocks.{47}.hook_layer_input', partial(position_patching_hook, layer=LAYER, position=FINAL_POS, batch=slice(None, None))))\n",
    "    return hooks\n",
    "always_hooks = generate_always_hooks()\n",
    "\n",
    "always_logits = model.run_with_hooks(prompt_uncorrupted_tokens, fwd_hooks=always_hooks, only_use_these_layers=limited_layers, fast_ssm=False, fast_conv=False)\n",
    "\n",
    "\n",
    "always_prs = torch.softmax(always_logits, dim=2)\n",
    "always_logit_diff = uncorrupted_logit_minus_corrupted_logit(logits=always_logits,\n",
    "                                                                uncorrupted_answer=uncorrupted_answer,\n",
    "                                                                corrupted_answer=corrupted_answer)\n",
    "# normalize it so\n",
    "# 0 means min_logit_diff (so 0 means that it is acting like the corrupted model)\n",
    "# 1 means max_logit_diff (so 1 means that it is acting like the uncorrupted model)\n",
    "normalized_always_logit_diff = (always_logit_diff-min_logit_diff)/(max_logit_diff - min_logit_diff)\n",
    "# now flip them, since most interventions will do nothing and thus act like uncorrupted model, visually its better to have that at 0\n",
    "# so now\n",
    "# 0 means that it is acting like the uncorrupted model\n",
    "# 1 means that it is acting like the corrupted model\n",
    "normalized_always_logit_diff = 1.0 - normalized_always_logit_diff\n",
    "\n",
    "# same for pr\n",
    "always_pr_diff = uncorrupted_pr_minus_corrupted_pr(prs=always_prs,\n",
    "                                                    uncorrupted_answer=uncorrupted_answer,\n",
    "                                                    corrupted_answer=corrupted_answer)\n",
    "normalized_always_pr_diff = 1.0-(always_pr_diff-min_pr_diff)/(max_pr_diff - min_pr_diff)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SKIPPING_H_PATCHING = 'skipping h'\n",
    "H_N_PATCHING = 'h_n'\n",
    "LAYER_SKIPPING = 'skipping layer'\n",
    "CONV_FILTERS_PATCHING = 'conv filters'\n",
    "\n",
    "patching_types = {\n",
    "    'resid pre': (lambda layer, position: f'blocks.{layer}.hook_resid_pre', position_patching_hook),\n",
    "    'layer input': (lambda layer, position: f'blocks.{layer}.hook_layer_input', position_patching_hook),\n",
    "    'normalized input': (lambda layer, position: f'blocks.{layer}.hook_normalized_input', position_patching_hook),\n",
    "    'skip': (lambda layer, position: f'blocks.{layer}.hook_skip', position_patching_hook), \n",
    "    'in proj': (lambda layer, position: f'blocks.{layer}.hook_in_proj', position_patching_hook), \n",
    "    CONV_FILTERS_PATCHING: (lambda layer, position: f'blocks.{layer}.hook_conv', conv_patching_hook),\n",
    "    'conv': (lambda layer, position: f'blocks.{layer}.hook_conv', position_patching_hook), \n",
    "    'delta 1': (lambda layer, position: f'blocks.{layer}.hook_delta_1', position_patching_hook), \n",
    "    'delta 2': (lambda layer, position: f'blocks.{layer}.hook_delta_2', position_patching_hook), \n",
    "    'delta': (lambda layer, position: f'blocks.{layer}.hook_delta', position_patching_hook), \n",
    "    'A_bar': (lambda layer, position: f'blocks.{layer}.hook_A_bar', position_patching_hook), \n",
    "    'B': (lambda layer, position: f'blocks.{layer}.hook_B', position_patching_hook), \n",
    "    'B_bar': (lambda layer, position: f'blocks.{layer}.hook_B_bar', position_patching_hook), \n",
    "    'C': (lambda layer, position: f'blocks.{layer}.hook_C', position_patching_hook), \n",
    "    'ssm input': (lambda layer, position: f'blocks.{layer}.hook_ssm_input', position_patching_hook),\n",
    "    SKIPPING_H_PATCHING: (lambda layer, position: f'blocks.{layer}.hook_h.{position}', skipping_h_hook),\n",
    "    'h': (lambda layer, position: f'blocks.{layer}.hook_h.{position}', h_patching_hook),\n",
    "    H_N_PATCHING: (lambda layer, position: f'blocks.{layer}.hook_h.{position}', h_n_patching_hook),\n",
    "    'y': (lambda layer, position: f'blocks.{layer}.hook_y', position_patching_hook),\n",
    "    'ssm output': (lambda layer, position: f'blocks.{layer}.hook_ssm_output', position_patching_hook),\n",
    "    'after skip': (lambda layer, position: f'blocks.{layer}.hook_after_skip', position_patching_hook),\n",
    "    'out proj': (lambda layer, position: f'blocks.{layer}.hook_out_proj', position_patching_hook),\n",
    "    'resid post': (lambda layer, position: f'blocks.{layer}.hook_resid_post', position_patching_hook),\n",
    "    LAYER_SKIPPING: (lambda layer, position: f'blocks.{layer}.hook_resid_post', layer_output_skipping_hook),\n",
    "}\n",
    "\n",
    "patching_types_keys = list(patching_types.keys())\n",
    "\n",
    "def choose_patching_type(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_patching_type.patching_type = change['new'] # hack, gives this function the patching_type attribute\n",
    "\n",
    "choose_patching_type.patching_type = patching_types_keys[0]\n",
    "\n",
    "patching_type_dropdown = ipywidgets.Dropdown(\n",
    "    options=patching_types_keys,\n",
    "    value=patching_types_keys[0],\n",
    "    description='patching type',\n",
    ")\n",
    "patching_type_dropdown.observe(choose_patching_type)\n",
    "display(patching_type_dropdown)\n",
    "\n",
    "BATCH_SIZE_ALL = 'all'\n",
    "batch_size_keys = [BATCH_SIZE_ALL] + [str(b) for b in range(model.cfg.n_layers*model.cfg.D_conv*L)]\n",
    "\n",
    "def choose_batch_size(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_batch_size.batch_size = change['new']\n",
    "\n",
    "choose_batch_size.batch_size = batch_size_keys[0]\n",
    "\n",
    "choose_batch_size_dropdown = ipywidgets.Dropdown(\n",
    "    options=batch_size_keys,\n",
    "    value=batch_size_keys[0],\n",
    "    description='batch size',\n",
    ")\n",
    "choose_batch_size_dropdown.observe(choose_batch_size)\n",
    "display(choose_batch_size_dropdown)\n",
    "\n",
    "fast_conv_keys = ['True', 'False']\n",
    "\n",
    "def choose_fast_conv(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_fast_conv.fast_conv = change['new'] == 'True'\n",
    "\n",
    "choose_fast_conv.fast_conv = fast_conv_keys[0] == 'True'\n",
    "\n",
    "choose_fast_conv_dropdown = ipywidgets.Dropdown(\n",
    "    options=fast_conv_keys,\n",
    "    value=fast_conv_keys[0],\n",
    "    description='fast conv',\n",
    ")\n",
    "choose_fast_conv_dropdown.observe(choose_fast_conv)\n",
    "display(choose_fast_conv_dropdown)\n",
    "\n",
    "\n",
    "fast_ssm_keys = ['False', 'True']\n",
    "\n",
    "def choose_fast_ssm(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_fast_ssm.fast_ssm = change['new'] == 'True'\n",
    "\n",
    "choose_fast_ssm.fast_ssm = fast_ssm_keys[0] == 'True'\n",
    "\n",
    "choose_fast_ssm_dropdown = ipywidgets.Dropdown(\n",
    "    options=fast_ssm_keys,\n",
    "    value=fast_ssm_keys[0],\n",
    "    description='fast ssm',\n",
    ")\n",
    "choose_fast_ssm_dropdown.observe(choose_fast_ssm)\n",
    "display(choose_fast_ssm_dropdown)\n",
    "\n",
    "SHOW_PR = 'Pr'\n",
    "SHOW_LOGITS = 'Logits'\n",
    "SHOW_BOTH = 'Both'\n",
    "show_options = [SHOW_LOGITS, SHOW_PR, SHOW_BOTH]\n",
    "\n",
    "def choose_show_options(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_show_options.show_options = change['new']\n",
    "\n",
    "choose_show_options.show_options = show_options[0]\n",
    "\n",
    "show_options_dropdown = ipywidgets.Dropdown(\n",
    "    options=show_options,\n",
    "    value=show_options[0],\n",
    "    description='logits or pr',\n",
    ")\n",
    "show_options_dropdown.observe(choose_show_options)\n",
    "display(show_options_dropdown)\n",
    "\n",
    "\n",
    "\n",
    "def do_patching(arg, show_plot=True):\n",
    "    with output: # this lets the stuff we output here be visible\n",
    "        clear_output()\n",
    "        patching_type = choose_patching_type.patching_type\n",
    "        hook_name_func, hook_func = patching_types[patching_type]\n",
    "        return run_patching(\n",
    "                     patching_type=patching_type,\n",
    "                     patching_hook_name_func=hook_name_func,\n",
    "                     patching_hook_func=hook_func,\n",
    "                     batch_size=choose_batch_size.batch_size,\n",
    "                     fast_ssm=choose_fast_ssm.fast_ssm,\n",
    "                     fast_conv=choose_fast_conv.fast_conv,\n",
    "                     show_options=choose_show_options.show_options,\n",
    "                     show_plot=show_plot,\n",
    "                     min_logit_diff=min_logit_diff,\n",
    "                     max_logit_diff=max_logit_diff,\n",
    "                     min_pr_diff=min_pr_diff,\n",
    "                     max_pr_diff=max_pr_diff,\n",
    "                     token_labels=token_labels,\n",
    "                     prompt_uncorrupted_tokens=prompt_uncorrupted_tokens,\n",
    "                     uncorrupted_answer=uncorrupted_answer,\n",
    "                     corrupted_answer=corrupted_answer,\n",
    "                     always_hooks=generate_always_hooks())\n",
    "\n",
    "patching_button = ipywidgets.Button(description = 'Run Patching')\n",
    "patching_button.on_click(do_patching)\n",
    "display(patching_button)\n",
    "\n",
    "# you can't just display stuff inside a widget callback, you need a wrap any display code in this\n",
    "output = ipywidgets.Output()\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Patchings over multiple data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_uncorrupted = \"\".join(model.tokenizer.decode(data.data[0][1:]))\n",
    "prompt_corrupted = \"\".join(model.tokenizer.decode(data.data[1][1:]))\n",
    "# note the spaces are important due to tokenization\n",
    "uncorrupted_answer = \"\".join(model.tokenizer.decode([data.correct[0][0]]))\n",
    "corrupted_answer = \"\".join(model.tokenizer.decode([data.correct[1][0]]))\n",
    "\n",
    "print(prompt_uncorrupted)\n",
    "print(prompt_corrupted)\n",
    "print(uncorrupted_answer)\n",
    "print(corrupted_answer)\n",
    "\n",
    "def choose_patching_type(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_patching_type.patching_type = change['new'] # hack, gives this function the patching_type attribute\n",
    "\n",
    "choose_patching_type.patching_type = patching_types_keys[0]\n",
    "\n",
    "patching_type_dropdown = ipywidgets.Dropdown(\n",
    "    options=patching_types_keys,\n",
    "    value=patching_types_keys[0],\n",
    "    description='patching type',\n",
    ")\n",
    "patching_type_dropdown.observe(choose_patching_type)\n",
    "display(patching_type_dropdown)\n",
    "\n",
    "BATCH_SIZE_ALL = 'all'\n",
    "batch_size_keys = [BATCH_SIZE_ALL] + [str(b) for b in range(model.cfg.n_layers*model.cfg.D_conv*L)]\n",
    "\n",
    "def choose_batch_size(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_batch_size.batch_size = change['new']\n",
    "\n",
    "choose_batch_size.batch_size = batch_size_keys[0]\n",
    "\n",
    "choose_batch_size_dropdown = ipywidgets.Dropdown(\n",
    "    options=batch_size_keys,\n",
    "    value=batch_size_keys[0],\n",
    "    description='batch size',\n",
    ")\n",
    "choose_batch_size_dropdown.observe(choose_batch_size)\n",
    "display(choose_batch_size_dropdown)\n",
    "\n",
    "fast_conv_keys = ['False', 'True']\n",
    "\n",
    "def choose_fast_conv(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_fast_conv.fast_conv = change['new'] == 'True'\n",
    "\n",
    "choose_fast_conv.fast_conv = fast_conv_keys[0] == 'True'\n",
    "\n",
    "choose_fast_conv_dropdown = ipywidgets.Dropdown(\n",
    "    options=fast_conv_keys,\n",
    "    value=fast_conv_keys[0],\n",
    "    description='fast conv',\n",
    ")\n",
    "choose_fast_conv_dropdown.observe(choose_fast_conv)\n",
    "display(choose_fast_conv_dropdown)\n",
    "\n",
    "\n",
    "fast_ssm_keys = ['False', 'True']\n",
    "\n",
    "def choose_fast_ssm(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_fast_ssm.fast_ssm = change['new'] == 'True'\n",
    "\n",
    "choose_fast_ssm.fast_ssm = fast_ssm_keys[0] == 'True'\n",
    "\n",
    "choose_fast_ssm_dropdown = ipywidgets.Dropdown(\n",
    "    options=fast_ssm_keys,\n",
    "    value=fast_ssm_keys[0],\n",
    "    description='fast ssm',\n",
    ")\n",
    "choose_fast_ssm_dropdown.observe(choose_fast_ssm)\n",
    "display(choose_fast_ssm_dropdown)\n",
    "\n",
    "SHOW_PR = 'Pr'\n",
    "SHOW_LOGITS = 'Logits'\n",
    "SHOW_BOTH = 'Both'\n",
    "show_options = [SHOW_LOGITS, SHOW_PR, SHOW_BOTH]\n",
    "\n",
    "def choose_show_options(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_show_options.show_options = change['new']\n",
    "\n",
    "choose_show_options.show_options = show_options[0]\n",
    "\n",
    "show_options_dropdown = ipywidgets.Dropdown(\n",
    "    options=show_options,\n",
    "    value=show_options[0],\n",
    "    description='logits or pr',\n",
    ")\n",
    "show_options_dropdown.observe(choose_show_options)\n",
    "display(show_options_dropdown)\n",
    "H_N_PATCHING_LAYER = 39\n",
    "\n",
    "\n",
    "global corrupted_activations\n",
    "\n",
    "def do_patching(arg, show_plot=True):\n",
    "    patching_result_normalized_logits_total = None\n",
    "    with output:\n",
    "        clear_output()\n",
    "        iters = 0\n",
    "        for i in range(data.data.size()[0]//2):\n",
    "            uncorrupted_i = i*2\n",
    "            corrupted_i = i*2+1\n",
    "            uncorrupted = data.data[uncorrupted_i]\n",
    "            corrupted = data.data[corrupted_i]\n",
    "    \n",
    "            uncorrupted_answer = model.tokenizer.decode([data.correct[uncorrupted_i][0]])\n",
    "            corrupted_answer = model.tokenizer.decode([data.correct[corrupted_i][0]])\n",
    "            answer_tokens = sorted(list(set([model.tokenizer.encode(uncorrupted_answer)[0], model.tokenizer.encode(corrupted_answer)[0]])))\n",
    "        \n",
    "            prompt_uncorrupted_tokens = uncorrupted[:data.last_token_position[uncorrupted_i]+1].view(1,-1)\n",
    "            prompt_corrupted_tokens = corrupted[:data.last_token_position[corrupted_i]+1].view(1,-1)\n",
    "\n",
    "            #print(model.to_str_tokens(prompt_uncorrupted_tokens))\n",
    "            #print(model.to_str_tokens(prompt_corrupted_tokens))\n",
    "            #print(\"answers\")\n",
    "            #print(uncorrupted_answer, corrupted_answer)\n",
    "            # [B,L,V]\n",
    "            global corrupted_activations\n",
    "            corrupted_logits, corrupted_activations = model.run_with_cache(prompt_corrupted_tokens, only_use_these_layers=limited_layers)\n",
    "            corrupted_logit_diff = uncorrupted_logit_minus_corrupted_logit(logits=corrupted_logits, uncorrupted_answer=uncorrupted_answer, corrupted_answer=corrupted_answer)\n",
    "            corrupted_prs = torch.softmax(corrupted_logits, dim=2)\n",
    "            corrupted_pr_diff = uncorrupted_pr_minus_corrupted_pr(prs=corrupted_prs, uncorrupted_answer=uncorrupted_answer, corrupted_answer=corrupted_answer)\n",
    "            \n",
    "            # [B,L,V]\n",
    "            uncorrupted_logits = model(prompt_uncorrupted_tokens, only_use_these_layers=limited_layers)\n",
    "            uncorrupted_logit_diff = uncorrupted_logit_minus_corrupted_logit(logits=uncorrupted_logits, uncorrupted_answer=uncorrupted_answer, corrupted_answer=corrupted_answer)\n",
    "            uncorrupted_prs = torch.softmax(uncorrupted_logits, dim=2)\n",
    "            uncorrupted_pr_diff = uncorrupted_pr_minus_corrupted_pr(prs=uncorrupted_prs, uncorrupted_answer=uncorrupted_answer, corrupted_answer=corrupted_answer)\n",
    "            \n",
    "            uncorrupted_index = model.to_single_token(uncorrupted_answer)\n",
    "            corrupted_index = model.to_single_token(corrupted_answer)\n",
    "            \n",
    "            # We make a tensor to store the results for each patching run. We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.\n",
    "            L = len(prompt_uncorrupted_tokens[0])\n",
    "            if len(prompt_corrupted_tokens[0]) != len(prompt_uncorrupted_tokens[0]):\n",
    "                raise Exception(\"Prompts are not the same length\") # feel free to comment this out, you can patch for different sized prompts its just a lil sus\n",
    "        \n",
    "            # diff is logit of uncorrupted_answer - logit of corrupted_answer\n",
    "            # we expect corrupted_diff to have a negative value (as corrupted should put high pr on corrupted_answer)\n",
    "            # we expect uncorrupted to have a positive value (as uncorrupted should put high pr on uncorrupted_answer)\n",
    "            # thus we can treat these as (rough) min and max possible values\n",
    "            min_logit_diff = corrupted_logit_diff\n",
    "            max_logit_diff = uncorrupted_logit_diff\n",
    "            \n",
    "            min_pr_diff = corrupted_pr_diff\n",
    "            max_pr_diff = uncorrupted_pr_diff\n",
    "            \n",
    "            # make token labels that describe the patch\n",
    "            corrupted_str_tokens = model.to_str_tokens(prompt_corrupted_tokens)\n",
    "            uncorrupted_str_tokens = model.to_str_tokens(prompt_uncorrupted_tokens)\n",
    "            token_labels = []\n",
    "            for index, (corrupted_token, uncorrupted_token) in enumerate(zip(corrupted_str_tokens, uncorrupted_str_tokens)):\n",
    "                if corrupted_token == uncorrupted_token:\n",
    "                    token_labels.append(f\"{corrupted_token}_{index}\")\n",
    "                else:\n",
    "                    token_labels.append(f\"{uncorrupted_token}->{corrupted_token}_{index}\")\n",
    "        \n",
    "                \n",
    "            print(i, \"/\", data.data.size()[0]//2)\n",
    "            patching_type = choose_patching_type.patching_type\n",
    "            hook_name_func, hook_func = patching_types[patching_type]\n",
    "            layer_labels, y_axis, patching_result_normalized_logits, patching_result_normalized_prs, patching_result_logits, patching_result_prs = run_patching(\n",
    "                         patching_type=patching_type,\n",
    "                         patching_hook_name_func=hook_name_func,\n",
    "                         patching_hook_func=hook_func,\n",
    "                         batch_size=choose_batch_size.batch_size,\n",
    "                         fast_ssm=choose_fast_ssm.fast_ssm,\n",
    "                         fast_conv=choose_fast_conv.fast_conv,\n",
    "                         show_options=choose_show_options.show_options,\n",
    "                         show_plot=False,\n",
    "                         min_logit_diff=min_logit_diff,\n",
    "                         max_logit_diff=max_logit_diff,\n",
    "                         min_pr_diff=min_pr_diff,\n",
    "                         max_pr_diff=max_pr_diff,\n",
    "                         token_labels=token_labels,\n",
    "                         prompt_uncorrupted_tokens=prompt_uncorrupted_tokens.view(1,-1),\n",
    "                         uncorrupted_answer=uncorrupted_answer,\n",
    "                         corrupted_answer=corrupted_answer)\n",
    "            iters += 1\n",
    "            if patching_result_normalized_logits_total is None:\n",
    "                patching_result_normalized_logits_total = patching_result_normalized_logits\n",
    "            else:\n",
    "                patching_result_normalized_logits_total += patching_result_normalized_logits\n",
    "        fig = imshow(patching_result_normalized_logits_total/iters, x=token_labels, y=layer_labels, xaxis=\"Position\", yaxis=y_axis, title=f\"Normalized logit difference after patching {patching_type} using hook {hook_name_func(layer='{layer}', position='{position}')}\", font_size=8, show=False)\n",
    "        plot_args_copy = dict(list(plot_args.items()))\n",
    "        # this code can be used to make it taller so its more readable, if u plot them all\n",
    "        if patching_type == CONV_FILTERS_PATCHING:\n",
    "            plot_args_copy['height'] *= model.cfg.d_conv\n",
    "        \n",
    "        fig.update_layout(**plot_args_copy)\n",
    "        fig.update_layout(legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=0.01\n",
    "        ))\n",
    "        fig.show()\n",
    "        if patching_type == CONV_FILTERS_PATCHING: # also plot just layer 39\n",
    "            layer_labels = [(i, label) for (i, label) in enumerate(layer_labels) if '39' in label]\n",
    "            patching_result_normalized_logits_total = patching_result_normalized_logits_total[torch.tensor([i for (i, label) in layer_labels])]\n",
    "            layer_labels = [label for (i,label) in layer_labels]\n",
    "            plot_args_copy = dict(list(plot_args.items()))\n",
    "            fig = imshow(patching_result_normalized_logits_total/iters, x=token_labels, y=layer_labels, xaxis=\"Position\", yaxis=y_axis, title=f\"Normalized logit difference after patching {patching_type} using hook {hook_name_func(layer='{layer}', position='{position}')}\", font_size=8, show=False)\n",
    "    \n",
    "            fig.update_layout(**plot_args_copy)\n",
    "            fig.update_layout(legend=dict(\n",
    "                yanchor=\"top\",\n",
    "                y=0.99,\n",
    "                xanchor=\"left\",\n",
    "                x=0.01\n",
    "            ))\n",
    "            fig.show()\n",
    "        \n",
    "\n",
    "patching_button = ipywidgets.Button(description = 'Run Patching')\n",
    "patching_button.on_click(do_patching)\n",
    "display(patching_button)\n",
    "    \n",
    "# you can't just display stuff inside a widget callback, you need a wrap any display code in this\n",
    "output = ipywidgets.Output()\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional EAP with integrated gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMEMBER TO MANUALLY PATCH TL LENS TO HAVE register_full_backward_hook\n",
    "\n",
    "model_kwargs = {\n",
    "    'fast_ssm': False,\n",
    "    'fast_conv': False\n",
    "}\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# removes all hooks including \"leftover\" ones that might stick around due to interrupting the model at certain times\n",
    "clean_hooks(model)\n",
    "\n",
    "def normalized_logit_diff_metric(patched_logits, unpatched_logits, corrupted_logits, patched_correct, corrupted_correct, also_return_acc=False):\n",
    "    B,V = patched_logits.size()\n",
    "\n",
    "    #print(data.unpatched.logits.size(), data.patched.logits.size(), data.corrupted.logits.size())\n",
    "    A_logits_unpatched = unpatched_logits[torch.arange(B), patched_correct]\n",
    "    A_logits_patched = patched_logits[torch.arange(B), patched_correct]\n",
    "    A_logits_corrupted = corrupted_logits[torch.arange(B), patched_correct]\n",
    "\n",
    "    B_logits_unpatched = unpatched_logits[torch.arange(B), corrupted_correct]\n",
    "    B_logits_patched = patched_logits[torch.arange(B), corrupted_correct]\n",
    "    B_logits_corrupted = corrupted_logits[torch.arange(B), corrupted_correct]\n",
    "\n",
    "    # A and B are two potential outputs\n",
    "    # if A patched > B patched, we are correct\n",
    "    # else we are incorrect\n",
    "\n",
    "    # thus we could just return A_logits_patched - B_logits_patched\n",
    "\n",
    "    # however it is useful to \"normalize\" these values\n",
    "\n",
    "    # in the worst case, our patching causes us to act like corrupted, and our diff will be\n",
    "    # A_logits_corrupted - B_logits_corrupted\n",
    "    # this will result in a small, negative value\n",
    "    \n",
    "    # in the best case, our patching will do nothing (cause us to act like unpatched), and our diff will be\n",
    "    # A_logits_unpatched - B_logits_unpatched\n",
    "    # this will result in a large, positive value\n",
    "\n",
    "    # thus we can treat those as the \"min\" and \"max\" and normalize accordingly\n",
    "    \n",
    "    min_diff = A_logits_corrupted - B_logits_corrupted\n",
    "    max_diff = A_logits_unpatched - B_logits_unpatched\n",
    "\n",
    "    # the abs ensures that if it's wrong we don't try and make it more wrong\n",
    "    possible_range = torch.abs(max_diff-min_diff)\n",
    "    possible_range[possible_range == 0] = 1.0 # prevent divide by zero\n",
    "    \n",
    "    diff = A_logits_patched - B_logits_patched\n",
    "    \n",
    "    normalized_diff = (diff-min_diff)/possible_range\n",
    "\n",
    "    # as described, 1.0 corresponds to acting like unpatched,\n",
    "    # and 0.0 corresponds to acting like corrupted\n",
    "    res = torch.mean(normalized_diff)\n",
    "    \n",
    "    if also_return_acc:\n",
    "        num_correct = A_logits_patched > B_logits_patched\n",
    "        acc = torch.sum(num_correct)/B\n",
    "        return res, acc\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "\n",
    "# there's a subtle bug if you aren't careful:\n",
    "# consider what happens when we do edge attribution patching and patch every edge\n",
    "# what we want to happen is that it's identical to corrupted\n",
    "# however this is not what happens!\n",
    "# Start with layer 0:\n",
    "# layer 0 will be patched\n",
    "#    we subtract uncorrupted embed and add corrupted embed\n",
    "#    in other words, the embed input to layer 0 will be from the corrupted run\n",
    "# this results in layer 0 having an output of corrupted, as desired\n",
    "# now consider layer 1\n",
    "#    subtract uncorrupted embed and add corrupted embed\n",
    "#      this is fine and results in embed input to layer 1 of corrupted\n",
    "#    subtract uncorrupted layer 0 and add corrupted layer 0\n",
    "#      layer 0 is already corrupted, so this has the effect of the output of layer 0 being\n",
    "#          2*corrupted layer 0 - uncorrupted layer 0\n",
    "#          this is not the same as corrupted!\n",
    "\n",
    "# two ways to fix this:\n",
    "# 1. fetch stored layer_input from uncorrupted run, and use that instead of the layer_input given in fwd_diff\n",
    "#   this works, but then the gradients won't propogate properly (maybe? todo: test)\n",
    "# 2. mark which edges are patched and don't \"double patch\" them\n",
    "#   if we are patching all edges, this means that we simply apply only the embed diff to all layers,\n",
    "#   as that'll result in all layers being patched\n",
    "# 3. just subtract the outputs in the same forward pass, instead of a cached \"unpatched\" run\n",
    "#  we do 3\n",
    "\n",
    "global alpha\n",
    "alpha = 1\n",
    "\n",
    "def cache_output_hook( # hook_layer_output\n",
    "    layer_output : Float[torch.Tensor, \"B L D\"],\n",
    "    hook : HookPoint,\n",
    "    layer : int,\n",
    "    cached_outputs : Float[torch.Tensor, \"NLayers+1 B L D\"]):\n",
    "    global alpha\n",
    "    cached_outputs[layer+1] = layer_output\n",
    "    return layer_output\n",
    "\n",
    "def fwd_diff_hook( # hook_layer_input\n",
    "    layer_input : Float[torch.Tensor, \"B L D\"],\n",
    "    hook : HookPoint,\n",
    "    layer : int,\n",
    "    cached_outputs : Float[torch.Tensor, \"NLayers+1 B L D\"],\n",
    "    corrupted_outputs : Float[torch.Tensor, \"NLayers+1 B L D\"]):\n",
    "    global alpha\n",
    "    return layer_input + (-cached_outputs[:layer+1]+corrupted_outputs[:layer+1]).sum(dim=0)*alpha\n",
    "\n",
    "def bwd_diff_hook( # hook_layer_input\n",
    "    grad : Float[torch.Tensor, \"B L D\"],\n",
    "    hook: HookPoint,\n",
    "    layer : int,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    cached_outputs : Float[torch.Tensor, \"NLayers+1 B L D\"],\n",
    "    corrupted_outputs : Float[torch.Tensor, \"NLayers+1 B L D\"]):\n",
    "    #print(f\"running bwd with alpha {alpha} and layer {layer}\") \n",
    "    \n",
    "    # [N_upstream, B, L, D]\n",
    "    upstream_diffs = (-cached_outputs[:layer+1]+corrupted_outputs[:layer+1])\n",
    "    # grad is [B,L,D]\n",
    "\n",
    "    # to do a taylor approximation of metric with respect to diff_0, we\n",
    "    # multiply diffs and grad, then\n",
    "    # sum over the L and D dimensions (this is doing a dot product of vectors of size L*D)\n",
    "    # now we have attr of size [N_upstream, B, L]\n",
    "    attr = (grad*upstream_diffs).sum(dim=-1)\n",
    "    # [B, N_upstream, L]           [N_upstream, B, L]\n",
    "    attr         =   torch.transpose(attr, 0, 1)\n",
    "    # [B, N_upstream, L]\n",
    "    attr = attr.clone().detach()\n",
    "    if POSITIONS:\n",
    "        attributions[batch_start:batch_end,:layer+1,layer+1] += attr\n",
    "    else:\n",
    "        #[B, N_upstream]\n",
    "        attr = attr.sum(dim=-1)\n",
    "        attributions[batch_start:batch_end,:layer+1,layer+1] += attr\n",
    "\n",
    "\n",
    "\n",
    "def cache_conv_hook(\n",
    "    conv_input: Float[torch.Tensor, \"B L E\"],\n",
    "    hook: HookPoint,\n",
    "    layer: int,\n",
    "    cached_conv_inputs : Float[torch.Tensor, \"NLayers B L E\"]):\n",
    "    cached_conv_inputs.append(conv_input)\n",
    "    return conv_input\n",
    "\n",
    "# for each conv slice,\n",
    "# we have \n",
    "global filter_terms\n",
    "def conv_patching_hook(\n",
    "    conv_output: Float[torch.Tensor, \"B L E\"],\n",
    "    hook: HookPoint,\n",
    "    layer: int,\n",
    "    cached_conv_inputs : Float[torch.Tensor, \"NLayers B L E\"],\n",
    "    corrupted_conv_inputs : Float[torch.Tensor, \"NLayers B L E\"],\n",
    "    slice_terms : [],\n",
    ") -> Float[torch.Tensor, \"B L E\"]:\n",
    "    ### This is identical to what the conv is doing\n",
    "    # but we break it apart so we can patch on individual slices\n",
    "\n",
    "    # we have two input hooks, the second one is the one we want\n",
    "    D_CONV = model.cfg.d_conv\n",
    "\n",
    "    # [E,1,D_CONV]\n",
    "    conv_weight = model.blocks[layer].conv1d.weight\n",
    "    # [E]\n",
    "    conv_bias = model.blocks[layer].conv1d.bias\n",
    "    uncorrupted_input = rearrange(cached_conv_inputs[layer], 'B L E -> B E L')\n",
    "    corrupted_input = rearrange(corrupted_conv_inputs[layer], 'B L E -> B E L')\n",
    "    B,E,L = uncorrupted_input.size()\n",
    "    uncorrupted_input = torch.nn.functional.pad(uncorrupted_input, (D_CONV-1,0), mode='constant', value=0)\n",
    "    corrupted_input = torch.nn.functional.pad(corrupted_input, (D_CONV-1,0), mode='constant', value=0)\n",
    "    #print(uncorrupted_input.size(), corrupted_input.size())\n",
    "    # we want some \"patch hook\" thing that uses autodiff or somethin\n",
    "    output = torch.zeros([B,E,L], device=model.cfg.device)\n",
    "    global alpha\n",
    "    layer_slices = []\n",
    "    for i in range(D_CONV):\n",
    "        # [B,E,L]                      [E,1]                      [B,E,L]\n",
    "        uncorrupted_contribution = conv_weight[:,0,i].view(E,1)*uncorrupted_input[:,:,i:i+L]\n",
    "        corrupted_contribution = conv_weight[:,0,i].view(E,1)*corrupted_input[:,:,i:i+L]\n",
    "        slice_term = uncorrupted_contribution*(1-alpha)+corrupted_contribution*(alpha)\n",
    "        diff = -uncorrupted_contribution+corrupted_contribution\n",
    "        slice_term.retain_grad()\n",
    "        #slice_term.requires_grad = True\n",
    "        layer_slices.append((diff, slice_term))\n",
    "        output += slice_term\n",
    "    output = rearrange(output, 'B E L -> B L E')\n",
    "    output += conv_bias\n",
    "    slice_terms.append(layer_slices)\n",
    "    return output\n",
    "\n",
    "def bwd_conv_hook(\n",
    "    grad : Float[torch.Tensor, \"B L E\"], # we don't use this, hook is just so we are present in backward pass\n",
    "    hook: HookPoint,\n",
    "    layer : int,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    cached_conv_inputs : Float[torch.Tensor, \"NLayers B L E\"],\n",
    "    corrupted_conv_inputs : Float[torch.Tensor, \"NLayers B L E\"],\n",
    "    slice_terms: list # one list for each layer, each list contains one tensor for each conv slice\n",
    "):\n",
    "    # [B, L, E]\n",
    "    #diffs = (-cached_conv_inputs[layer]+corrupted_conv_inputs[layer])\n",
    "    layer_slice_terms = slice_terms[layer]\n",
    "    for slice_i, (diff, slice_term) in enumerate(layer_slice_terms):\n",
    "        # [B,E,L]\n",
    "        slice_grad = slice_term.grad\n",
    "        # dot product over the E dimension\n",
    "        # [B,L]         [B,E,L]   [B,E,L]\n",
    "        slice_attr = (slice_grad * diff).sum(dim=-2) # sum over E dimension\n",
    "        if POSITIONS:\n",
    "            # [B,L]\n",
    "            conv_attributions[batch_start:batch_end,layer,slice_i] += slice_attr \n",
    "        else:\n",
    "            #[B]\n",
    "            slice_attr = slice_attr.sum(dim=-1)\n",
    "            conv_attributions[batch_start:batch_end,layer,slice_i] += slice_attr\n",
    "\n",
    "B,L = data.data.size()\n",
    "# our data is pairs of unpatched, corrupted\n",
    "n_patching_pairs = B//2\n",
    "D_CONV = model.cfg.d_conv\n",
    "\n",
    "# attributions[b,i+1,j+1] is the (i->j) edge attribution for patching pair b\n",
    "# attributions[b,0,j] is the (embed->j) edge attribution for patching pair b\n",
    "# attributions[b,i,-1] is the (i->output) edge attribution for patching pair b\n",
    "POSITIONS = True\n",
    "if POSITIONS:\n",
    "    attributions = torch.zeros([n_patching_pairs, model.cfg.n_layers+2, model.cfg.n_layers+2, L], device=model.cfg.device)\n",
    "    conv_attributions = torch.zeros([n_patching_pairs, model.cfg.n_layers, D_CONV, L], device=model.cfg.device)\n",
    "else:\n",
    "    attributions = torch.zeros([n_patching_pairs, model.cfg.n_layers+2, model.cfg.n_layers+2], device=model.cfg.device)\n",
    "    conv_attributions = torch.zeros([n_patching_pairs, model.cfg.n_layers, D_CONV], device=model.cfg.device)\n",
    "\n",
    "\n",
    "\n",
    "input_names = [f'blocks.{i}.hook_layer_input' for i in range(model.cfg.n_layers)]\n",
    "output_names = [f'blocks.{i}.hook_out_proj' for i in range(model.cfg.n_layers)]\n",
    "conv_input_names = [f'blocks.{i}.hook_in_proj' for i in range(model.cfg.n_layers)]\n",
    "conv_names = [f'blocks.{i}.hook_conv' for i in range(model.cfg.n_layers)]\n",
    "\n",
    "for batch_start in range(0, n_patching_pairs, BATCH_SIZE):\n",
    "    batch_end = min(batch_start + BATCH_SIZE, n_patching_pairs)\n",
    "    print(batch_start, batch_end)\n",
    "    # we don't need grad for these forward passes\n",
    "    torch.set_grad_enabled(False)\n",
    "    embed_name = 'hook_embed'\n",
    "\n",
    "    clean_hooks(model)\n",
    "    # forward passes to get unpatched and corrupted\n",
    "    unpatched_logits, unpatched_layer_outputs = model.run_with_cache(data.data[::2][batch_start:batch_end], names_filter=[embed_name] + output_names + conv_input_names, **model_kwargs)\n",
    "    corrupted_logits, corrupted_layer_outputs = model.run_with_cache(data.data[1::2][batch_start:batch_end], names_filter=[embed_name] + output_names + conv_input_names, **model_kwargs)\n",
    "    \n",
    "    batch_size,L,D = unpatched_layer_outputs[output_names[0]].size()\n",
    "    _,_,E = unpatched_layer_outputs[conv_input_names[0]].size()\n",
    "    \n",
    "    # get only the last token position (logit for next predicted token)\n",
    "    # this is needed to support data of varying lengths\n",
    "    unpatched_logits = unpatched_logits[torch.arange(batch_size), data.last_token_position[::2][batch_start:batch_end]]\n",
    "    corrupted_logits = corrupted_logits[torch.arange(batch_size), data.last_token_position[1::2][batch_start:batch_end]]\n",
    "    \n",
    "    clean_hooks(model)\n",
    "\n",
    "    # backward pass to compute grad of diff\n",
    "    torch.set_grad_enabled(True)\n",
    "        \n",
    "    corrupted_outputs = torch.zeros([model.cfg.n_layers+1,batch_size,L,D], device=model.cfg.device)\n",
    "    corrupted_conv_inputs = torch.zeros([model.cfg.n_layers,batch_size,L,E], device=model.cfg.device)\n",
    "    corrupted_outputs.requires_grad = False\n",
    "    # first one is for embed\n",
    "    corrupted_outputs[0] = corrupted_layer_outputs[embed_name]\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        output_name = output_names[layer]\n",
    "        corrupted_outputs[layer+1] = corrupted_layer_outputs[output_name]\n",
    "        corrupted_conv_inputs[layer] = corrupted_layer_outputs[conv_input_names[layer]]    \n",
    "    # cleanup\n",
    "    del corrupted_layer_outputs\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "        param.grad = None # reset grads\n",
    "    \n",
    "    last_layer = model.cfg.n_layers-1\n",
    "    # forward pass to do partial patches\n",
    "    cached_outputs = torch.zeros([model.cfg.n_layers+1,batch_size,L,D], device=model.cfg.device)\n",
    "    cached_outputs.requires_grad = False\n",
    "    \n",
    "    # EAP layer hooks\n",
    "    cache_output_hooks = [(embed_name,\n",
    "                           partial(cache_output_hook,\n",
    "                                   layer=-1,\n",
    "                                   cached_outputs=cached_outputs))]\n",
    "    \n",
    "    cache_output_hooks += [(output_names[layer],\n",
    "                            partial(cache_output_hook,\n",
    "                                    layer=layer,\n",
    "                                    cached_outputs=cached_outputs)) for layer in range(model.cfg.n_layers)]\n",
    "    \n",
    "    fwd_hooks = cache_output_hooks\n",
    "    fwd_hooks += [(input_names[layer],\n",
    "                  partial(\n",
    "                      fwd_diff_hook,\n",
    "                      layer=layer,\n",
    "                      cached_outputs=cached_outputs,\n",
    "                      corrupted_outputs=corrupted_outputs,\n",
    "                  )) for layer in range(model.cfg.n_layers)]\n",
    "    bwd_hooks = [(input_names[layer],\n",
    "                  partial(bwd_diff_hook,\n",
    "                          layer=layer,\n",
    "                          cached_outputs=cached_outputs,\n",
    "                          corrupted_outputs=corrupted_outputs,\n",
    "                          batch_start=batch_start,\n",
    "                          batch_end=batch_end)) for layer in range(model.cfg.n_layers)]\n",
    "    # extra hook for the very last layer\n",
    "    fwd_hooks.append((f'blocks.{last_layer}.hook_resid_post',\n",
    "                      partial(fwd_diff_hook,\n",
    "                              layer=last_layer+1,\n",
    "                              cached_outputs=cached_outputs,\n",
    "                              corrupted_outputs=corrupted_outputs,\n",
    "                             )))\n",
    "    bwd_hooks.append((f'blocks.{last_layer}.hook_resid_post',\n",
    "                      partial(bwd_diff_hook,\n",
    "                              layer=last_layer+1,\n",
    "                              cached_outputs=cached_outputs,\n",
    "                              corrupted_outputs=corrupted_outputs,\n",
    "                              batch_start=batch_start,\n",
    "                              batch_end=batch_end,\n",
    "                             )))\n",
    "\n",
    "    # Conv hooks\n",
    "    cached_conv_inputs = []\n",
    "    slice_terms = []\n",
    "\n",
    "    fwd_hooks += [(conv_input_names[layer],\n",
    "                  partial(\n",
    "                      cache_conv_hook,\n",
    "                      layer=layer,\n",
    "                      cached_conv_inputs=cached_conv_inputs,\n",
    "                  )) for layer in range(model.cfg.n_layers)]\n",
    "    \n",
    "    \n",
    "    fwd_hooks += [(conv_names[layer],\n",
    "                  partial(\n",
    "                      conv_patching_hook,\n",
    "                      layer=layer,\n",
    "                      cached_conv_inputs=cached_conv_inputs,\n",
    "                      corrupted_conv_inputs=corrupted_conv_inputs,\n",
    "                      slice_terms=slice_terms,\n",
    "                  )) for layer in range(model.cfg.n_layers)]\n",
    "    bwd_hooks += [(input_names[layer], # anywhere before the slice terms works, so we'll just pick start of layer since that backward is called\n",
    "                  partial(\n",
    "                      bwd_conv_hook,\n",
    "                      layer=layer,\n",
    "                      cached_conv_inputs=cached_conv_inputs,\n",
    "                      corrupted_conv_inputs=corrupted_conv_inputs,\n",
    "                      slice_terms=slice_terms,\n",
    "                      batch_start=batch_start,\n",
    "                      batch_end=batch_end,\n",
    "                  )) for layer in range(model.cfg.n_layers)]\n",
    "    \n",
    "    for fwd in fwd_hooks:\n",
    "        model.add_hook(*fwd, \"fwd\")\n",
    "\n",
    "    for bwd in bwd_hooks:\n",
    "        model.add_hook(*bwd, \"bwd\")\n",
    "    \n",
    "    # with integrated gradients\n",
    "    # simply sums over doing \"partial patches\" like 0.2 patch and 0.8 unpatched \n",
    "    # ITERS = 1 is just edge attribution patching (without integraded gradients)\n",
    "    ITERS = 5\n",
    "    for i in range(ITERS+1):\n",
    "        global alpha\n",
    "        # alpha ranges from 0 to 1\n",
    "        if ITERS > 1:\n",
    "            alpha = i/float(ITERS-1)\n",
    "        elif ITERS == 1: # no integrated gradients, set alpha to 1\n",
    "            alpha = 1.0\n",
    "\n",
    "        # it tries to propogate gradients to these, detach them\n",
    "        slice_terms.clear()\n",
    "        cached_conv_inputs.clear()\n",
    "        torch.cuda.empty_cache()\n",
    "        cached_outputs[:] = 0\n",
    "        cached_outputs.grad = None\n",
    "        cached_outputs.detach_()\n",
    "        corrupted_outputs.grad = None\n",
    "        corrupted_outputs.detach_()\n",
    "        #cached_conv_inputs.grad = None\n",
    "        #cached_conv_inputs.detach_()\n",
    "        corrupted_conv_inputs.grad = None\n",
    "        corrupted_conv_inputs.detach_()\n",
    "        model.zero_grad()\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        if i == ITERS:\n",
    "            torch.cuda.empty_cache()\n",
    "            break # we just use this for cleaning up\n",
    "        logits = model(data.data[::2][batch_start:batch_end], **model_kwargs)\n",
    "        logits = logits[torch.arange(batch_size), data.last_token_position[::2][batch_start:batch_end]]\n",
    "        metric = normalized_logit_diff_metric(\n",
    "            patched_logits=logits,\n",
    "            unpatched_logits=unpatched_logits,\n",
    "            corrupted_logits=corrupted_logits,\n",
    "            patched_correct=data.correct[::2][batch_start:batch_end][:,0],\n",
    "            corrupted_correct=data.correct[1::2][batch_start:batch_end][:,0]\n",
    "        )\n",
    "        print(f\"alpha {alpha} metric {metric}\")\n",
    "        # run backward pass, which adds to attributions\n",
    "        metric.backward()\n",
    "        #conv_attrs = conv_attributions.mean(dim=0)\n",
    "        #print(conv_attrs)\n",
    "        #attrs = attributions.mean(dim=0)\n",
    "        #print(attrs)\n",
    "\n",
    "# todo: maybe the diffs should have alpha in the backward pass? No, that would mean alpha of 0 gives all zero attrs\n",
    "\n",
    "# average over all the samples\n",
    "attributions[:] = attributions[:]/ITERS\n",
    "conv_attributions[:] = conv_attributions[:]/ITERS\n",
    "\n",
    "\n",
    "# don't need grad for rest of this\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "clean_hooks(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Search to find Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    'fast_ssm': False,\n",
    "    'fast_conv': False\n",
    "}\n",
    "\n",
    "global storage\n",
    "def storage_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    **kwargs,\n",
    "):\n",
    "    global storage\n",
    "    #if hook.name == 'hook_embed':\n",
    "    #    for k in list(storage.keys()):\n",
    "    #        del storage[k]\n",
    "    storage[hook.name] = x\n",
    "    return x\n",
    "\n",
    "def resid_patching_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    input_hook_name: str,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    position: int = None,\n",
    "):\n",
    "    global storage\n",
    "    x_uncorrupted = storage[input_hook_name][batch_start:batch_end:2]\n",
    "    x_corrupted = storage[input_hook_name][batch_start+1:batch_end:2]\n",
    "    if position is None: # if position not specified, apply to all positions\n",
    "        x[batch_start:batch_end:2] = x[batch_start:batch_end:2] - x_uncorrupted + x_corrupted\n",
    "    else:\n",
    "        x[batch_start:batch_end:2,position] = x[batch_start:batch_end:2,position] - x_uncorrupted[:,position] + x_corrupted[:,position]\n",
    "    return x\n",
    "\n",
    "def overwrite_patching_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    input_hook_name: str,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    position: int = None,\n",
    "):\n",
    "    x_corrupted = x[batch_start+1:batch_end:2]\n",
    "    if position is None: # if position not specified, apply to all positions\n",
    "        x[batch_start:batch_end:2] = x_corrupted\n",
    "    else:\n",
    "        if x_corrupted.size()[1] != L: raise ValueError(f'warning: in hook {hook.name} with input_hook_name {input_hook_name} you are patching on position in the second index but size is {x_corrupted.size()}')\n",
    "        x[batch_start:batch_end:2,position] = x_corrupted[:,position]\n",
    "    return x\n",
    "\n",
    "\n",
    "def overwrite_h_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    input_hook_name: str,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    position: int = None,\n",
    "):\n",
    "    x[batch_start:batch_end:2] = x[batch_start+1:batch_end:2]\n",
    "    return x\n",
    "\n",
    "################## CONV ##############\n",
    "\n",
    "# we do a hacky thing where this first hook clears the global storage\n",
    "# second hook stores all the hooks\n",
    "# then third hook computes the output (over all the hooks)\n",
    "# this avoids recomputing and so is much faster\n",
    "CONV_HOOKS = \"conv hooks\"\n",
    "CONV_BATCHES = \"conv batches\"\n",
    "def conv_patching_init_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    **kwargs\n",
    "):\n",
    "    # we need to clear this here\n",
    "    # i tried having a \"current layer\" variable in the conv_storage that only clears when it doesn't match\n",
    "    # but that doesn't work if you only patch the same layer over and over,\n",
    "    # as stuff gets carried over\n",
    "    # this way of doing things is much safer and lets us assume it'll be empty\n",
    "    # well not quite, note that conv_patching_hook will be called with different batch_start and batch_end inputs during one forward pass\n",
    "    # so we need to account for that in the keys we use\n",
    "    global conv_storage\n",
    "    conv_storage = {CONV_BATCHES: set()}\n",
    "    return x\n",
    "\n",
    "# hook h has a weird index!!!!!\n",
    "\n",
    "def conv_patching_storage_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    conv_filter_i: int,\n",
    "    position: int,\n",
    "    layer: int,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    **kwargs,\n",
    "):\n",
    "    global storage\n",
    "    storage[hook.name] = x\n",
    "    global conv_storage\n",
    "    hooks_key = (CONV_HOOKS, batch_start, batch_end)\n",
    "    if not hooks_key in conv_storage:\n",
    "        conv_storage[hooks_key] = [] # we can't do this above because it'll be emptied again on the next batch before this is called\n",
    "    conv_storage[hooks_key].append({\"position\": position, \"conv_filter_i\": conv_filter_i})\n",
    "    conv_storage[CONV_BATCHES].add((batch_start, batch_end))\n",
    "    return x\n",
    "\n",
    "\n",
    "global storage_for_grad\n",
    "\n",
    "global conv_storage\n",
    "def conv_patching_hook(\n",
    "    conv_output: Float[torch.Tensor, \"B L E\"],\n",
    "    hook: HookPoint,\n",
    "    input_hook_name: str,\n",
    "    layer: int,\n",
    "    **kwargs,\n",
    ") -> Float[torch.Tensor, \"B L E\"]:\n",
    "    global conv_storage\n",
    "    global storage\n",
    "    ### This is identical to what the conv is doing\n",
    "    # but we break it apart so we can patch on individual filters\n",
    "\n",
    "    # we have two input hooks, the second one is the one we want\n",
    "    input_hook_name = input_hook_name[1]\n",
    "    \n",
    "    D_CONV = model.cfg.d_conv\n",
    "\n",
    "    # given batches like [(2,4), (5,6)] and total size 7 this returns (0,2), (4,5), (6,7) \n",
    "    def get_missing_batches(batches, B):\n",
    "        covered_i = torch.zeros([B])\n",
    "        for batch_start, batch_end in batches:\n",
    "            covered_i[batch_start:batch_end] = 1\n",
    "    \n",
    "        missing_batches = []\n",
    "        missing_start = 0\n",
    "        for i in range(B):\n",
    "            if covered_i[i] == 1:\n",
    "                if i != missing_start:\n",
    "                    missing_batches.append((missing_start, i))\n",
    "                missing_start = i+1\n",
    "        if covered_i[B-1] == 0:\n",
    "            missing_batches.append((missing_start, B))\n",
    "        return missing_batches\n",
    "    \n",
    "\n",
    "    \n",
    "    # [E,1,D_CONV]\n",
    "    conv_weight = model.blocks[layer].conv1d.weight\n",
    "    # [E]\n",
    "    conv_bias = model.blocks[layer].conv1d.bias\n",
    "    \n",
    "    # don't recompute these if we don't need to\n",
    "    # because we stored all the hooks and batches in conv_storage, we can just do them all at once\n",
    "    output_key = f'output' # they need to share an output because they write to the same output tensor\n",
    "    if not output_key in conv_storage:\n",
    "        #print(\"layer\", layer, \"keys\", conv_storage)\n",
    "        apply_to_all_hooks = [] # this is important because otherwise the [0:None] would overwrite the previous results (or vice versa)\n",
    "        apply_to_all_key = (CONV_HOOKS, 0, None)\n",
    "        if apply_to_all_key in conv_storage:\n",
    "            apply_to_all_hooks = conv_storage[apply_to_all_key]\n",
    "            # we need to do this so it applies to the other batches that we aren't otherwise patching\n",
    "            for batch_start, batch_end in get_missing_batches(conv_storage[CONV_BATCHES], conv_output.size()[0]):\n",
    "                conv_storage[CONV_BATCHES].add(batch_start, batch_end)\n",
    "                conv_storage[(CONV_HOOKS, batch_start, batch_end)] = []\n",
    "        for batch_start, batch_end in conv_storage[CONV_BATCHES]:\n",
    "            if batch_start == 0 and batch_end == None: continue # we cover this in the apply to all hooks above\n",
    "            def get_filter_key(i):\n",
    "                return f'filter_{i}'\n",
    "            conv_input_uncorrupted = storage[input_hook_name][batch_start:batch_end:2]\n",
    "            conv_input_corrupted = storage[input_hook_name][batch_start+1:batch_end:2]\n",
    "            B, L, E = conv_input_uncorrupted.size()\n",
    "            \n",
    "            conv_input_uncorrupted = rearrange(conv_input_uncorrupted, 'B L E -> B E L')\n",
    "            conv_input_corrupted = rearrange(conv_input_corrupted, 'B L E -> B E L')\n",
    "            \n",
    "            # pad zeros in front\n",
    "            # [B,E,D_CONV-1+L]\n",
    "            padded_input_uncorrupted = torch.nn.functional.pad(conv_input_uncorrupted, (D_CONV-1,0), mode='constant', value=0)\n",
    "            padded_input_corrupted = torch.nn.functional.pad(conv_input_corrupted, (D_CONV-1,0), mode='constant', value=0)\n",
    "    \n",
    "            # compute the initial filter values\n",
    "            for i in range(D_CONV):\n",
    "                filter_key = get_filter_key(i)\n",
    "                # [B,E,L]                      [E,1]                      [B,E,L]\n",
    "                filter_contribution = conv_weight[:,0,i].view(E,1)*padded_input_uncorrupted[:,:,i:i+L]\n",
    "                conv_storage[filter_key] = filter_contribution\n",
    "            \n",
    "            # apply all the hooks\n",
    "            for hook in conv_storage[(CONV_HOOKS, batch_start, batch_end)] + apply_to_all_hooks:\n",
    "                position = hook['position']\n",
    "                conv_filter_i = hook['conv_filter_i']\n",
    "                #print(f\"position {position} conv_filter_i {conv_filter_i} batch_start {batch_start} batch_end {batch_end}\")\n",
    "                filter_key = get_filter_key(conv_filter_i)\n",
    "                # [1,E,L]                                   [E,1]                          # [B,E,L]\n",
    "                corrupted_filter_contribution = conv_weight[:,0,conv_filter_i].view(E,1)*padded_input_corrupted[:,:,conv_filter_i:conv_filter_i+L]\n",
    "                filter_contribution = conv_storage[filter_key]\n",
    "                if position is None:\n",
    "                    # [B,E,L]                    [B,E,L]\n",
    "                    filter_contribution = corrupted_filter_contribution\n",
    "                else:\n",
    "                    # [B,E]                                                  [B,E]\n",
    "                    filter_contribution[:,:,position] = corrupted_filter_contribution[:,:,position]\n",
    "                conv_storage[filter_key] = filter_contribution\n",
    "            \n",
    "            # compute the output\n",
    "            output = sum([conv_storage[get_filter_key(i)] for i in range(D_CONV)])\n",
    "            #output = torch.zeros([B,E,L], device=model.cfg.device)\n",
    "            #print(f'B {B} B2 {B2} E {E} L {L} conv_storage keys {conv_storage.keys()} filter sizes {[(k,v.size()) for (k,v) in conv_storage.items() if not type(v) is int]}')\n",
    "            for i in range(D_CONV):\n",
    "                filter_key = get_filter_key(i)\n",
    "                #output += conv_storage[filter_key]\n",
    "                del conv_storage[filter_key] # clean up now we are done with it, just to be safe\n",
    "                \n",
    "            # bias is not dependent on input so no reason to patch on it, just apply it as normal\n",
    "            output += conv_bias.view(E, 1)\n",
    "            output = rearrange(output, 'B E L -> B L E')\n",
    "            # interleave it back with the corrupted as every other\n",
    "            conv_output[batch_start:batch_end:2] = output\n",
    "        conv_storage[output_key] = conv_output\n",
    "    return conv_storage[output_key]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "limited_layers = list(range(model.cfg.n_layers))\n",
    "\n",
    "## Setup edges for ACDC\n",
    "edges = []\n",
    "B,L = data.data.size()\n",
    "#positions = list(range(L)) # \n",
    "\n",
    "if POSITIONS:\n",
    "    positions = list(range(L))\n",
    "else:\n",
    "    positions = [None]\n",
    "\n",
    "INPUT_HOOK = f'hook_embed'\n",
    "INPUT_NODE = 'embed'\n",
    "\n",
    "last_layer = model.cfg.n_layers-1\n",
    "OUTPUT_HOOK = f'blocks.{last_layer}.hook_resid_post'\n",
    "OUTPUT_NODE = 'output'\n",
    "\n",
    "def input(layer):\n",
    "    return f'{layer}.i'\n",
    "\n",
    "def output(layer):\n",
    "    return f'{layer}.o'\n",
    "\n",
    "def conv(layer):\n",
    "    return f'{layer}.conv'\n",
    "\n",
    "def skip(layer):\n",
    "    return f'{layer}.skip'\n",
    "\n",
    "def ssm(layer):\n",
    "    return f'{layer}.ssm'\n",
    "\n",
    "# important to have storage be global and not passed into the hooks! Otherwise it gets very slow (tbh, i don't know why)\n",
    "global storage\n",
    "storage = {}\n",
    "\n",
    "\n",
    "attrs = attributions.mean(dim=0)\n",
    "conv_attrs = conv_attributions.mean(dim=0)\n",
    "ALWAYS_KEEP_WEIGHT = -10\n",
    "for pos in positions:\n",
    "    # direct connections from embed to output\n",
    "    edges.append(Edge(\n",
    "            label=str(pos),\n",
    "            input_node=INPUT_NODE,\n",
    "            input_hook=(INPUT_HOOK, storage_hook),\n",
    "            output_node=OUTPUT_NODE,\n",
    "            output_hook=(OUTPUT_HOOK, partial(resid_patching_hook, position=pos)),\n",
    "            score_diff_when_patched=attrs[0,model.cfg.n_layers+1,pos].flatten()[0],\n",
    "    ))\n",
    "\n",
    "for layer in limited_layers:\n",
    "    for pos_i, pos in enumerate(positions):\n",
    "        # edge from embed to layer input\n",
    "        edges.append(Edge(\n",
    "                label=str(pos),\n",
    "                input_node=INPUT_NODE,\n",
    "                input_hook=(INPUT_HOOK, partial(storage_hook)),\n",
    "                output_node=input(layer),\n",
    "                output_hook=(f'blocks.{layer}.hook_layer_input', partial(resid_patching_hook, position=pos)),\n",
    "                score_diff_when_patched=attrs[0,layer+1,pos].flatten()[0],\n",
    "        ))\n",
    "\n",
    "\n",
    "        # input to conv\n",
    "        for conv_i in range(model.cfg.d_conv):\n",
    "            edges.append(Edge(\n",
    "                    label=(f'[{pos}:{conv_i-model.cfg.d_conv+1}]'.replace(\"None:\", \"\")), # [-D_CONV+1, -D_CONV+2, ..., -2, -1, 0]\n",
    "                    input_node=input(layer),\n",
    "                    input_hook=[\n",
    "                        (f'blocks.{layer}.hook_layer_input', conv_patching_init_hook),\n",
    "                        (f'blocks.{layer}.hook_in_proj', partial(conv_patching_storage_hook, position=pos, layer=layer, conv_filter_i=conv_i))\n",
    "                    ],\n",
    "                    output_node=conv(layer),\n",
    "                    output_hook=(f'blocks.{layer}.hook_conv', partial(conv_patching_hook, position=pos, layer=layer, conv_filter_i=conv_i)),\n",
    "                    score_diff_when_patched=conv_attrs[layer,conv_i,pos].flatten()[0],\n",
    "            ))\n",
    "        \n",
    "        # conv to ssm\n",
    "        if pos is None:\n",
    "            # we need a seperate hook for each pos, but put them all into one edge\n",
    "            hooks = []\n",
    "            for other_pos in range(L):\n",
    "                hooks.append((f'blocks.{layer}.hook_h.{other_pos}', overwrite_h_hook))\n",
    "            edges.append(Edge(\n",
    "                    input_node=conv(layer),\n",
    "                    output_node=ssm(layer),\n",
    "                    output_hook=hooks,\n",
    "                    score_diff_when_patched=ALWAYS_KEEP_WEIGHT,\n",
    "            ))\n",
    "        else:\n",
    "            edges.append(Edge(\n",
    "                    label=f'{pos}',\n",
    "                    input_node=conv(layer),\n",
    "                    output_node=ssm(layer),\n",
    "                    output_hook=(f'blocks.{layer}.hook_h.{pos}', overwrite_h_hook),\n",
    "                    score_diff_when_patched=ALWAYS_KEEP_WEIGHT\n",
    "            ))\n",
    "\n",
    "        if pos_i == 0: # we only need one of these\n",
    "            # ssm to output\n",
    "            edges.append(Edge(\n",
    "                    input_node=ssm(layer),\n",
    "                    output_node=output(layer),\n",
    "                    score_diff_when_patched=ALWAYS_KEEP_WEIGHT\n",
    "            ))\n",
    "        \n",
    "        # input to skip\n",
    "        edges.append(Edge(\n",
    "                label=f'{pos}',\n",
    "                input_node=input(layer),\n",
    "                output_node=skip(layer),\n",
    "                output_hook=(f'blocks.{layer}.hook_skip', partial(overwrite_patching_hook, position=pos)),\n",
    "                score_diff_when_patched=ALWAYS_KEEP_WEIGHT\n",
    "        ))\n",
    "\n",
    "        if pos_i == 0: # we only need one of these\n",
    "            # skip to output\n",
    "            edges.append(Edge(\n",
    "                    input_node=skip(layer),\n",
    "                    output_node=output(layer),\n",
    "                    score_diff_when_patched=ALWAYS_KEEP_WEIGHT\n",
    "            ))\n",
    "        \n",
    "        for later_layer in limited_layers:\n",
    "                if layer < later_layer:\n",
    "                    # edge from layer output to other layer input\n",
    "                    edges.append(Edge(\n",
    "                            label=str(pos),\n",
    "                            input_node=output(layer),\n",
    "                            input_hook=(f'blocks.{layer}.hook_out_proj', storage_hook),\n",
    "                            output_node=input(later_layer),\n",
    "                            output_hook=(f'blocks.{later_layer}.hook_layer_input', partial(resid_patching_hook, position=pos)),\n",
    "                            score_diff_when_patched=attrs[layer+1,later_layer+1,pos].flatten()[0],\n",
    "                    ))\n",
    "        \n",
    "        # edge from layer output to final layer output\n",
    "        edges.append(Edge(\n",
    "                label=str(pos),\n",
    "                input_node=output(layer),\n",
    "                input_hook=(f'blocks.{layer}.hook_out_proj', storage_hook),\n",
    "                output_node=OUTPUT_NODE,\n",
    "                output_hook=(OUTPUT_HOOK, partial(resid_patching_hook, position=pos)),\n",
    "                score_diff_when_patched=attrs[layer+1,model.cfg.n_layers+1,pos].flatten()[0],\n",
    "        ))\n",
    "\n",
    "\n",
    "\n",
    "def normalized_logit_diff_acdc_metric(data: ACDCEvalData, printing=False):\n",
    "    B,V = data.patched.logits.size()\n",
    "\n",
    "    # [batch_size]\n",
    "    patched_correct = data.patched.correct[:,0]\n",
    "    #print(data.unpatched.logits.size(), data.patched.logits.size(), data.corrupted.logits.size())\n",
    "    A_logits_unpatched = data.unpatched.logits[torch.arange(B), patched_correct]\n",
    "    A_logits_patched = data.patched.logits[torch.arange(B), patched_correct]\n",
    "    A_logits_corrupted = data.corrupted.logits[torch.arange(B), patched_correct]\n",
    "\n",
    "    corrupted_correct = data.corrupted.correct[:,0]\n",
    "    B_logits_unpatched = data.unpatched.logits[torch.arange(B), corrupted_correct]\n",
    "    B_logits_patched = data.patched.logits[torch.arange(B), corrupted_correct]\n",
    "    B_logits_corrupted = data.corrupted.logits[torch.arange(B), corrupted_correct]\n",
    "\n",
    "    # A and B are two potential outputs\n",
    "    # if A patched > B patched, we are correct\n",
    "    # else we are incorrect\n",
    "\n",
    "    # thus we could just return A_logits_patched - B_logits_patched\n",
    "\n",
    "    # however it is useful to \"normalize\" these values\n",
    "\n",
    "    # in the worst case, our patching causes us to act like corrupted, and our diff will be\n",
    "    # A_logits_corrupted - B_logits_corrupted\n",
    "    # this will result in a small, negative value\n",
    "    \n",
    "    # in the best case, our patching will do nothing (cause us to act like unpatched), and our diff will be\n",
    "    # A_logits_unpatched - B_logits_unpatched\n",
    "    # this will result in a large, positive value\n",
    "\n",
    "    # thus we can treat those as the \"min\" and \"max\" and normalize accordingly\n",
    "    \n",
    "    min_diff = A_logits_corrupted - B_logits_corrupted\n",
    "    max_diff = A_logits_unpatched - B_logits_unpatched\n",
    "\n",
    "    possible_range = (max_diff-min_diff)\n",
    "    possible_range[possible_range == 0] = 1.0 # prevent divide by zero\n",
    "    \n",
    "    diff = A_logits_patched - B_logits_patched\n",
    "    normalized_diff = (diff-min_diff)/torch.abs(possible_range) # abs prevents incorrect data from wanting to be more incorrect\n",
    "\n",
    "    if printing:\n",
    "        print(f\"A corrupted {A_logits_corrupted}\")\n",
    "        print(f\"B corrupted {B_logits_corrupted}\")\n",
    "        print(f\"A unpatched {A_logits_unpatched}\")\n",
    "        print(f\"B unpatched {B_logits_unpatched}\")\n",
    "        print(f\"A patched {A_logits_patched}\")\n",
    "        print(f\"B patched {B_logits_patched}\")\n",
    "        print(f\"min diff {min_diff}\")\n",
    "        print(f\"max diff {max_diff}\")\n",
    "        print(f\"possible range {possible_range}\")\n",
    "        print(f\"diff {diff}\")\n",
    "        print(f\"normalized diff {normalized_diff}\")\n",
    "    # as described, 1.0 corresponds to acting like unpatched,\n",
    "    # and 0.0 corresponds to acting like corrupted\n",
    "\n",
    "    return torch.mean(normalized_diff)\n",
    "import acdc\n",
    "cfg = ACDCConfig(\n",
    "    ckpt_directory = \"blah\",\n",
    "    thresh = 0.0001,\n",
    "    rollback_thresh = 0.0001,\n",
    "    metric=normalized_logit_diff_acdc_metric,\n",
    "    # extra inference args\n",
    "    model_kwargs=model_kwargs,\n",
    "    # these are needed for doing graph pruning\n",
    "    input_node=INPUT_NODE,\n",
    "    output_node=OUTPUT_NODE,\n",
    "    # batch size for evaluating data points\n",
    "    batch_size=1,\n",
    "    log_level=LOG_LEVEL_INFO,\n",
    "    # if False, will be equivalent to batch_size=1\n",
    "    batched = True,\n",
    "    # set these two to false to use traditional ACDC\n",
    "    # recursive will try patching multiple at a time (this is faster sometimes)\n",
    "    recursive = True,\n",
    "    # try_patching_multiple_at_same_time will evaluate many different patchings before commiting to any\n",
    "    # and includes a rollback scheme if after patching one, the others get worse\n",
    "    try_patching_multiple_at_same_time = True,\n",
    "    ## if true, you metric will also have the logits from a run with no patching available\n",
    "    # (useful for normalized logit diff)\n",
    "    store_unpatched_logits = True,\n",
    ")\n",
    "from acdc import get_currently_patched_edge_hooks, eval_acdc, wrap_run_with_hooks, get_logits_of_predicted_next_token\n",
    "unpatched_logits = get_logits_of_predicted_next_token(\n",
    "        model=model,\n",
    "        data=data.valid_data,\n",
    "        last_token_position=data.valid_last_token_position,\n",
    "        **model_kwargs\n",
    ")\n",
    "\n",
    "valid_unpatched_logits = get_logits_of_predicted_next_token(\n",
    "        model=model,\n",
    "        data=data.valid_data,\n",
    "        last_token_position=data.valid_last_token_position,\n",
    "        **model_kwargs\n",
    ")\n",
    "\n",
    "def eval_edges(edges_keeping, all_edges, valid=False):\n",
    "    for edge in all_edges:\n",
    "        edge.patching = True\n",
    "        edge.checked = True\n",
    "    for edge in edges_keeping:\n",
    "        edge.patching = False\n",
    "        edge.checked = True\n",
    "    currently_patched_edge_hooks = get_currently_patched_edge_hooks(cfg=cfg, edges=edges)\n",
    "    if valid:\n",
    "        return eval_acdc(\n",
    "                model=wrap_run_with_hooks(model=model, fwd_hooks=currently_patched_edge_hooks, **cfg.model_kwargs),\n",
    "                data=data.valid_data,\n",
    "                last_token_position=data.valid_last_token_position,\n",
    "                correct=data.valid_correct,\n",
    "                incorrect=data.valid_incorrect,\n",
    "                metric=cfg.metric,\n",
    "                num_edges=1,\n",
    "                constrain_to_answers=data.constrain_to_answers,\n",
    "                unpatched_logits=valid_unpatched_logits)[0].item()\n",
    "    else:\n",
    "        return eval_acdc(\n",
    "            model=wrap_run_with_hooks(model=model, fwd_hooks=currently_patched_edge_hooks, **cfg.model_kwargs),\n",
    "            data=data.data,\n",
    "            last_token_position=data.last_token_position,\n",
    "            correct=data.correct,\n",
    "            incorrect=data.incorrect,\n",
    "            metric=cfg.metric,\n",
    "            num_edges=1,\n",
    "            constrain_to_answers=data.constrain_to_answers,\n",
    "            unpatched_logits=unpatched_logits)[0].item()\n",
    "\n",
    "# can also do this instead to include negative contributions but I find the graph is larger, ymmv\n",
    "#edges.sort(key=lambda x: -abs(x[0]))\n",
    "edges.sort(key=lambda edge: edge.score_diff_when_patched)\n",
    "\n",
    "import math\n",
    "def test_pos(pos):\n",
    "    print(f\"testing pos {pos}\")\n",
    "    edges_to_keep = edges[:pos]\n",
    "    metric = eval_edges(edges_to_keep, edges)\n",
    "    print(f\"testing pos {pos} got metric {metric}\")\n",
    "    return metric\n",
    "\n",
    "# from https://en.wikipedia.org/wiki/Binary_search_algorithm\n",
    "def binary_search(n, T):\n",
    "    L = 0\n",
    "    R = n - 1\n",
    "    while L != R:\n",
    "        m = math.ceil((L + R) / 2)\n",
    "        if test_pos(m) > T:\n",
    "            R = m - 1\n",
    "        else:\n",
    "            L = m\n",
    "    # go one further because this gives us below thresh\n",
    "    return min(n-1, L+1)\n",
    "\n",
    "ACC_THRESH = 0.4\n",
    "torch.set_grad_enabled(False)\n",
    "'''\n",
    "cutoff = binary_search(len(edges), T=ACC_THRESH)\n",
    "\n",
    "edges_to_keep = edges[:cutoff]\n",
    "scores = [edge.score_diff_when_patched for edge in edges[:cutoff]]\n",
    "print(f\"keeping top {cutoff} edges\")\n",
    "metric = eval_edges(edges_to_keep, edges)\n",
    "print(f\"got metric {metric}\")    \n",
    "\n",
    "metric = eval_edges(edges_to_keep, edges, valid=True)\n",
    "print(f\"got valid metric {metric}\")    \n",
    "# .85 has 3061 edges for 5 ITERS\n",
    "# .85 has 2876 edges for 30 ITERS\n",
    "\n",
    "\n",
    "# patching format 0:\n",
    "# 2162 edges (2016 always keep)\n",
    "'''\n",
    "num_always_keep = 0\n",
    "for edge in edges:\n",
    "    if edge.score_diff_when_patched == ALWAYS_KEEP_WEIGHT:\n",
    "        num_always_keep += 1\n",
    "print(f\"always keep {num_always_keep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,10):\n",
    "    target = i/10.0\n",
    "    cutoff = binary_search(len(edges), T=target)\n",
    "    edges_to_keep = edges[:cutoff]\n",
    "    adj_mat = compute_adj_mat(edges_to_keep)\n",
    "    pruned_adj_mat = better_prune_edges(adj_mat)\n",
    "\n",
    "    position_map = {}\n",
    "    for l in range(L):\n",
    "        position_map[l] = f'pos{l}{toks[l]}'\n",
    "\n",
    "    import graphviz\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        layer_attrs = []\n",
    "        for l in range(L):\n",
    "            for filter_i in range(model.cfg.d_conv):\n",
    "                filter_offset = -filter_i\n",
    "                index = l*model.cfg.d_conv + filter_i\n",
    "                attr = by_filters[index, layer]\n",
    "                if attr < 0:\n",
    "                    layer_attrs.append((position_map[l+filter_offset], attr, filter_offset))\n",
    "        for tok, attr, filter_offset in layer_attrs:\n",
    "            attr = -attr*1000\n",
    "\n",
    "    def map_layer(layer):\n",
    "        if layer == 0: return 'embed'\n",
    "        if layer == model.cfg.n_layers+1: return 'output'\n",
    "        else: return str(layer-1)\n",
    "\n",
    "    for layer in range(model.cfg.n_layers+2):\n",
    "        layer_attrs = []\n",
    "        for other_layer in range(model.cfg.n_layers+2):\n",
    "            for pos in range(L):\n",
    "                if pruned_adj_mat[pos, layer, other_layer] != 0:\n",
    "                    layer_attrs.append((position_map[pos], f\"->{map_layer(other_layer)}\", pruned_adj_mat[pos, layer, other_layer]))\n",
    "                if pruned_adj_mat[pos, other_layer, layer] != 0:\n",
    "                    layer_attrs.append((position_map[pos], f\"{map_layer(other_layer)}->\", pruned_adj_mat[pos, other_layer, layer]))\n",
    "        for tok, label, attr in layer_attrs:\n",
    "            attr = -attr*1000\n",
    "\n",
    "\n",
    "    def layer_to_i(layer):\n",
    "        if layer == 'embed': return 0\n",
    "        elif layer == 'output': return model.cfg.n_layers+1\n",
    "        else: return int(layer)+1\n",
    "\n",
    "    forbidden = [0,1]\n",
    "    pruned_adj_mat = better_prune_edges(adj_mat)\n",
    "    for i in forbidden:\n",
    "        pruned_adj_mat[:,i,:] = 0\n",
    "        pruned_adj_mat[:,:,i] = 0\n",
    "    present_layers = set()\n",
    "    for i in range(model.cfg.n_layers):\n",
    "        for j in range(model.cfg.n_layers):\n",
    "            if not i+1 in forbidden and not j+1 in forbidden:\n",
    "                if torch.any(pruned_adj_mat[:,i+1,j+1] != 0):\n",
    "                    present_layers.add(i)\n",
    "                    present_layers.add(j)\n",
    "                    #print(f\"present edge {i}->{j}\")\n",
    "    pruned_dot = graphviz.Digraph('graph')\n",
    "    for edge in edges_to_keep:\n",
    "        is_between_layers, layer_input, layer_output = between_layers_info(edge)\n",
    "        if is_between_layers:\n",
    "            ini, outi = layer_to_i(layer_input), layer_to_i(layer_output)\n",
    "            pos = int(edge.label)\n",
    "            if pruned_adj_mat[pos, ini, outi] != 0:\n",
    "                if not ini in forbidden and not outi in forbidden:\n",
    "                    pruned_dot.edge(edge.input_node, edge.output_node, label=position_map[int(edge.label)])\n",
    "        else:\n",
    "            layer = edge.input_node.split(\".\")[0]\n",
    "            if not layer in ['embed', 'output']:\n",
    "                layer = int(layer)\n",
    "            if layer in present_layers: # don't display stuff that are disconnected\n",
    "                if '.ssm' in edge.output_node or '.skip' in edge.output_node or '.skip' in edge.input_node: # don't need all these since they are all forced on\n",
    "                    if edge.label == 0 or edge.label == \"\" or edge.label is None or edge.label == \"0\":\n",
    "                        pruned_dot.edge(edge.input_node, edge.output_node)\n",
    "                else:\n",
    "                    pruned_dot.edge(edge.input_node, edge.output_node, label=edge.label)\n",
    "    from pathlib import Path\n",
    "    Path(f\"seed {seed} no embed or zero/\").mkdir(parents=True, exist_ok=True)\n",
    "    output_name = f\"seed {seed} no embed or zero/pruned dot {target}\"\n",
    "    # too big:\n",
    "    from IPython.display import Image, FileLink\n",
    "    pruned_dot.render(output_name, format=\"png\") # it automatically appends png\n",
    "    #display(Image(filename=output_name + \".png\"))\n",
    "    #display(FileLink(output_name + \".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune and Display EAP Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = edges_to_keep\n",
    "L = data.data.size()[1]\n",
    "print(f\"got L of {L}\")\n",
    "if POSITIONS:\n",
    "    by_filters = torch.zeros([model.cfg.d_conv*L, model.cfg.n_layers])\n",
    "else:\n",
    "    by_filters = torch.zeros([model.cfg.d_conv, model.cfg.n_layers])\n",
    "\n",
    "# change this to get the different plots\n",
    "CAP = 0.0001\n",
    "# 1.0 is boolean (present or not)\n",
    "# 0.0 is just give score\n",
    "\n",
    "def filter_score(score):\n",
    "    if CAP == 0.0: return score\n",
    "    if CAP == 1.0: return 1.0\n",
    "    return max(score, -CAP)\n",
    "def torch_filter_score(score):\n",
    "    if CAP == 0.0: return score\n",
    "    if CAP == 1.0: return (score != 0) + 0.0\n",
    "    return torch.clamp(score, min=-CAP)\n",
    "for edge in edges:\n",
    "    if not edge.patching and edge.checked:\n",
    "        if '.conv' in edge.output_node:\n",
    "            if \":\" in edge.label:\n",
    "                pos = int(edge.label.split(\":\")[0][1:])\n",
    "                rest = edge.label.split(\":\")[1]\n",
    "            else:\n",
    "                pos = None\n",
    "                rest = edge.label[1:]\n",
    "            filter = int(rest.split(\"]\")[0])        \n",
    "            layer = edge.output_node.split(\".\")[0]\n",
    "            if POSITIONS:\n",
    "                try:\n",
    "                    by_filters[model.cfg.d_conv*pos + abs(int(filter)),int(layer)] = filter_score(edge.score_diff_when_patched)\n",
    "                except:\n",
    "                    print(L, pos, abs(int(filter)))\n",
    "                    raise\n",
    "            else:\n",
    "                by_filters[abs(int(filter)),int(layer)] = filter_score(edge.score_diff_when_patched)\n",
    "        if '.skip' in edge.output_node and False:        \n",
    "            layer = edge.output_node.split(\".\")[0]\n",
    "            by_filters[model.cfg.d_conv,int(layer)] = filter_score(edge.score_diff_when_patched)\n",
    "        if '.ssm' in edge.output_node and False:\n",
    "            layer = edge.output_node.split(\".\")[0]\n",
    "            by_filters[model.cfg.d_conv+1,int(layer)] = filter_score(edge.score_diff_when_patched)\n",
    "            \n",
    "\n",
    "def layer_to_i(node):\n",
    "    if node == INPUT_NODE:\n",
    "        return 0\n",
    "    elif node == OUTPUT_NODE:\n",
    "        return model.cfg.n_layers+1 # because embed is 0\n",
    "    else:\n",
    "        return int(node)+1 # because embed is 0\n",
    "\n",
    "\n",
    "def between_layers_info(edge):\n",
    "    if edge.patching: return False, None, None\n",
    "    is_between_layers = False\n",
    "    layer_input = None\n",
    "    layer_output = None\n",
    "    if edge.input_node == INPUT_NODE:\n",
    "        is_between_layers = True\n",
    "        layer_input = INPUT_NODE\n",
    "    if edge.output_node == OUTPUT_NODE:\n",
    "        is_between_layers = True\n",
    "        layer_output = OUTPUT_NODE\n",
    "\n",
    "    if '.' in edge.input_node:\n",
    "        input_layer, input_type = edge.input_node.split(\".\")\n",
    "        if edge.input_node == output(input_layer):\n",
    "            is_between_layers = True\n",
    "            layer_input = str(input_layer)\n",
    "    if '.' in edge.output_node:\n",
    "        output_layer, output_type = edge.output_node.split(\".\")\n",
    "        if edge.output_node == input(output_layer):\n",
    "            is_between_layers = True\n",
    "            layer_output = str(output_layer)\n",
    "    return is_between_layers, layer_input, layer_output\n",
    "\n",
    "def compute_adj_mat(edges):\n",
    "    if POSITIONS:\n",
    "        adj_mat = torch.zeros([L, model.cfg.n_layers+2, model.cfg.n_layers+2])\n",
    "    else:\n",
    "        adj_mat = torch.zeros([model.cfg.n_layers+2, model.cfg.n_layers+2])\n",
    "    for edge in edges:\n",
    "        is_between_layers, layer_input, layer_output = between_layers_info(edge)\n",
    "        if is_between_layers:\n",
    "            if POSITIONS:\n",
    "                pos = int(edge.label)\n",
    "                adj_mat[pos, layer_to_i(layer_input), layer_to_i(layer_output)] = edge.score_diff_when_patched\n",
    "            else:\n",
    "                adj_mat[layer_to_i(layer_input), layer_to_i(layer_output)] = edge.score_diff_when_patched\n",
    "    return adj_mat\n",
    "\n",
    "\n",
    "L = data.data.size()[1]\n",
    "import networkx as nx\n",
    "def better_get_nx_graph(adj_mat) -> nx.DiGraph:\n",
    "    '''\n",
    "    Converts the edges into a networkx graph\n",
    "    only edges that have checked == True and patching == False are included\n",
    "    if include_unchecked=True, any edge that has checked == False is also included\n",
    "    '''\n",
    "    num_nodes = adj_mat.size()[-1]\n",
    "    G = nx.DiGraph()\n",
    "    edges = []\n",
    "    if POSITIONS:\n",
    "        for pos in range(L):\n",
    "            for i in range(num_nodes):\n",
    "                for j in range(num_nodes):\n",
    "                    if adj_mat[pos,i,j] != 0:\n",
    "                        G.add_edge(str(i), str(j))\n",
    "                        edges.append((i,j,pos,adj_mat[pos,i,j]))\n",
    "    else:\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(num_nodes):\n",
    "                if adj_mat[i,j] != 0:\n",
    "                    G.add_edge(str(i), str(j))\n",
    "                    edges.append((i,j,None,adj_mat[i,j]))\n",
    "    return G, edges\n",
    "\n",
    "def better_prune_edges(adj_mat):\n",
    "    num_nodes = adj_mat.size()[-1]\n",
    "    input_node = 0\n",
    "    output_node = num_nodes-1\n",
    "    import networkx as nx\n",
    "    G, edges = better_get_nx_graph(adj_mat=adj_mat)\n",
    "    pruned_adj_mat = torch.zeros(adj_mat.size())\n",
    "    pruned_edges = []\n",
    "    for i,j,pos,attr in edges:\n",
    "        connected_to_input = False\n",
    "        try:\n",
    "            to_input = nx.shortest_path(G, source=str(input_node), target=str(i))\n",
    "            connected_to_input = True\n",
    "        except nx.NetworkXNoPath:\n",
    "            pass\n",
    "        except nx.NodeNotFound:\n",
    "            raise ValueError(f\"Graph does not have input node {input_node}\")\n",
    "        \n",
    "        connected_to_output = False\n",
    "        try:\n",
    "            to_output = nx.shortest_path(G, source=str(j), target=str(output_node))\n",
    "            connected_to_output = True\n",
    "        except nx.NetworkXNoPath:\n",
    "            pass\n",
    "        except nx.NodeNotFound:\n",
    "            raise ValueError(f\"Graph does not have output node {output_node}\")\n",
    "        if connected_to_input and connected_to_output:\n",
    "            pruned_adj_mat[pos,i,j] = attr\n",
    "        else:\n",
    "            print(f\"pruning {pos} {i}->{j} {connected_to_input} {connected_to_output}\")\n",
    "    return pruned_adj_mat\n",
    "\n",
    "adj_mat = compute_adj_mat(edges_to_keep)\n",
    "pruned_adj_mat = better_prune_edges(adj_mat)\n",
    "\n",
    "def to_str(lis):\n",
    "    return [str(x) for x in lis]\n",
    "x_labels = to_str([-x for x in range(model.cfg.d_conv)])\n",
    "toks = model.to_str_tokens(data.data[0])\n",
    "x_labels_all_pos = []\n",
    "for l in range(L):\n",
    "    x_labels_all_pos += [label + toks[l+int(label)] + \"_\" + str(l) for label in x_labels]\n",
    "print(x_labels, by_filters.size())\n",
    "print(len(x_labels_all_pos))\n",
    "title = f'which parts of layer using (conv filters, skip, and/or conv), capped to {-CAP}'\n",
    "if CAP == 1.0:\n",
    "    title = f'which parts of layer using (conv filters, skip, and/or conv)'\n",
    "print(x_labels_all_pos)\n",
    "imshow(by_filters.T, y=to_str(range(model.cfg.n_layers)), x=x_labels_all_pos, title=title, font_size=7, fix_size=True)\n",
    "#for layer in range(model.cfg.n_layers):\n",
    "#    imshow(by_filters[:,layer:layer+1].T, y=to_str([layer]), x=x_labels_all_pos, title=str(layer), font_size=9)\n",
    "\n",
    "labels = ['embed'] + [str(x) for x in range(model.cfg.n_layers)] + ['output']\n",
    "if POSITIONS:\n",
    "    for position in range(L):\n",
    "        if torch.any(torch.abs(pruned_adj_mat[position]) > CAP/10.0):\n",
    "            imshow(torch_filter_score(pruned_adj_mat[position]), y=labels, x=labels, title=f'pos {position} tok {toks[position]} adjacency matrix clamped to {CAP}', font_size=13, zmin=-CAP, zmax=CAP, fix_size=True, height=800)\n",
    "else:\n",
    "    imshow(pruned_adj_mat, y=labels, x=labels, title=f'adjacency matrix clamped to {CAP}', font_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_map = {}\n",
    "for l in range(L):\n",
    "    position_map[l] = f'pos{l}{toks[l]}'\n",
    "\n",
    "import graphviz\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    layer_attrs = []\n",
    "    for l in range(L):\n",
    "        for filter_i in range(model.cfg.d_conv):\n",
    "            filter_offset = -filter_i\n",
    "            index = l*model.cfg.d_conv + filter_i\n",
    "            attr = by_filters[index, layer]\n",
    "            if attr < 0:\n",
    "                layer_attrs.append((position_map[l+filter_offset], attr, filter_offset))\n",
    "    print(layer)\n",
    "    for tok, attr, filter_offset in layer_attrs:\n",
    "        attr = -attr*1000\n",
    "        print(f\"  {tok}[{filter_offset}] {attr:.5f}\")\n",
    "\n",
    "def map_layer(layer):\n",
    "    if layer == 0: return 'embed'\n",
    "    if layer == model.cfg.n_layers+1: return 'output'\n",
    "    else: return str(layer-1)\n",
    "\n",
    "for layer in range(model.cfg.n_layers+2):\n",
    "    layer_attrs = []\n",
    "    for other_layer in range(model.cfg.n_layers+2):\n",
    "        for pos in range(L):\n",
    "            if pruned_adj_mat[pos, layer, other_layer] != 0:\n",
    "                layer_attrs.append((position_map[pos], f\"->{map_layer(other_layer)}\", pruned_adj_mat[pos, layer, other_layer]))\n",
    "            if pruned_adj_mat[pos, other_layer, layer] != 0:\n",
    "                layer_attrs.append((position_map[pos], f\"{map_layer(other_layer)}->\", pruned_adj_mat[pos, other_layer, layer]))\n",
    "    print(map_layer(layer))\n",
    "    for tok, label, attr in layer_attrs:\n",
    "        attr = -attr*1000\n",
    "        print(f\"  {tok} {label} {attr:.5f}\")\n",
    "\n",
    "\n",
    "def layer_to_i(layer):\n",
    "    if layer == 'embed': return 0\n",
    "    elif layer == 'output': return model.cfg.n_layers+1\n",
    "    else: return int(layer)+1\n",
    "\n",
    "forbidden = [0,1,40]\n",
    "pruned_adj_mat = better_prune_edges(adj_mat)\n",
    "for i in forbidden:\n",
    "    pruned_adj_mat[:,i,:] = 0\n",
    "    pruned_adj_mat[:,:,i] = 0\n",
    "present_layers = set()\n",
    "for i in range(model.cfg.n_layers):\n",
    "    for j in range(model.cfg.n_layers):\n",
    "        if not i+1 in forbidden and not j+1 in forbidden:\n",
    "            if torch.any(pruned_adj_mat[:,i+1,j+1] != 0):\n",
    "                present_layers.add(i)\n",
    "                present_layers.add(j)\n",
    "                #print(f\"present edge {i}->{j}\")\n",
    "print(present_layers)\n",
    "pruned_dot = graphviz.Digraph('graph')\n",
    "for edge in edges_to_keep:\n",
    "    is_between_layers, layer_input, layer_output = between_layers_info(edge)\n",
    "    if is_between_layers:\n",
    "        ini, outi = layer_to_i(layer_input), layer_to_i(layer_output)\n",
    "        pos = int(edge.label)\n",
    "        if pruned_adj_mat[pos, ini, outi] != 0:\n",
    "            if not ini in forbidden and not outi in forbidden:\n",
    "                pruned_dot.edge(edge.input_node, edge.output_node, label=position_map[int(edge.label)])\n",
    "    else:\n",
    "        layer = int(edge.input_node.split(\".\")[0])\n",
    "        if layer in present_layers: # don't display stuff that are disconnected\n",
    "            if '.ssm' in edge.output_node or '.skip' in edge.output_node or '.skip' in edge.input_node: # don't need all these since they are all forced on\n",
    "                if edge.label == 0 or edge.label == \"\" or edge.label is None or edge.label == \"0\":\n",
    "                    pruned_dot.edge(edge.input_node, edge.output_node)\n",
    "            else:\n",
    "                pruned_dot.edge(edge.input_node, edge.output_node, label=edge.label)\n",
    "output_name = f'pruned dot {ACC_THRESH}'\n",
    "# too big:\n",
    "from IPython.display import Image, FileLink\n",
    "pruned_dot.render(output_name, format=\"png\") # it automatically appends png\n",
    "display(Image(filename=output_name + \".png\"))\n",
    "display(FileLink(output_name + \".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE EAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SAES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry190.txtblocks.1.hook_out_proj/hook_blocks.1.hook_out_proj.pt\n",
      "2\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry191.txtblocks.2.hook_out_proj/hook_blocks.2.hook_out_proj.pt\n",
      "3\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry192.txtblocks.3.hook_out_proj/hook_blocks.3.hook_out_proj.pt\n",
      "4\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry193.txtblocks.4.hook_out_proj/hook_blocks.4.hook_out_proj.pt\n",
      "5\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry194.txtblocks.5.hook_out_proj/hook_blocks.5.hook_out_proj.pt\n",
      "6\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry195.txtblocks.6.hook_out_proj/hook_blocks.6.hook_out_proj.pt\n",
      "7\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry196.txtblocks.7.hook_out_proj/hook_blocks.7.hook_out_proj.pt\n",
      "8\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry200.txtblocks.8.hook_out_proj/hook_blocks.8.hook_out_proj.pt\n",
      "9\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry201.txtblocks.9.hook_out_proj/hook_blocks.9.hook_out_proj.pt\n",
      "10\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry202.txtblocks.10.hook_out_proj/hook_blocks.10.hook_out_proj.pt\n",
      "11\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry203.txtblocks.11.hook_out_proj/hook_blocks.11.hook_out_proj.pt\n",
      "12\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry204.txtblocks.12.hook_out_proj/hook_blocks.12.hook_out_proj.pt\n",
      "13\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry205.txtblocks.13.hook_out_proj/hook_blocks.13.hook_out_proj.pt\n",
      "14\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry206.txtblocks.14.hook_out_proj/hook_blocks.14.hook_out_proj.pt\n",
      "15\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry210.txtblocks.15.hook_out_proj/hook_blocks.15.hook_out_proj.pt\n",
      "16\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry211.txtblocks.16.hook_out_proj/hook_blocks.16.hook_out_proj.pt\n",
      "17\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry212.txtblocks.17.hook_out_proj/hook_blocks.17.hook_out_proj.pt\n",
      "18\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry213.txtblocks.18.hook_out_proj/hook_blocks.18.hook_out_proj.pt\n",
      "19\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry214.txtblocks.19.hook_out_proj/hook_blocks.19.hook_out_proj.pt\n",
      "20\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry215.txtblocks.20.hook_out_proj/hook_blocks.20.hook_out_proj.pt\n",
      "21\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry216.txtblocks.21.hook_out_proj/hook_blocks.21.hook_out_proj.pt\n",
      "22\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry260.txtblocks.22.hook_out_proj/hook_blocks.22.hook_out_proj.pt\n",
      "23\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry261.txtblocks.23.hook_out_proj/hook_blocks.23.hook_out_proj.pt\n",
      "24\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry262.txtblocks.24.hook_out_proj/hook_blocks.24.hook_out_proj.pt\n",
      "25\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry263.txtblocks.25.hook_out_proj/hook_blocks.25.hook_out_proj.pt\n",
      "26\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry264.txtblocks.26.hook_out_proj/hook_blocks.26.hook_out_proj.pt\n",
      "27\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry265.txtblocks.27.hook_out_proj/hook_blocks.27.hook_out_proj.pt\n",
      "28\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry266.txtblocks.28.hook_out_proj/hook_blocks.28.hook_out_proj.pt\n",
      "29\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry270.txtblocks.29.hook_out_proj/hook_blocks.29.hook_out_proj.pt\n",
      "30\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry271.txtblocks.30.hook_out_proj/hook_blocks.30.hook_out_proj.pt\n",
      "31\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry272.txtblocks.31.hook_out_proj/hook_blocks.31.hook_out_proj.pt\n",
      "32\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry273.txtblocks.32.hook_out_proj/hook_blocks.32.hook_out_proj.pt\n",
      "33\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry274.txtblocks.33.hook_out_proj/hook_blocks.33.hook_out_proj.pt\n",
      "34\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry275.txtblocks.34.hook_out_proj/hook_blocks.34.hook_out_proj.pt\n",
      "35\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry276.txtblocks.35.hook_out_proj/hook_blocks.35.hook_out_proj.pt\n",
      "36\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry281.txtblocks.36.hook_out_proj/hook_blocks.36.hook_out_proj.pt\n",
      "37\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry282.txtblocks.37.hook_out_proj/hook_blocks.37.hook_out_proj.pt\n",
      "38\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry283.txtblocks.38.hook_out_proj/hook_blocks.38.hook_out_proj.pt\n",
      "39\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry284.txtblocks.39.hook_out_proj/hook_blocks.39.hook_out_proj.pt\n",
      "40\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry285.txtblocks.40.hook_out_proj/hook_blocks.40.hook_out_proj.pt\n",
      "41\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry286.txtblocks.41.hook_out_proj/hook_blocks.41.hook_out_proj.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load SAEs\n",
    "import sys\n",
    "if not \"/home/dev/sae-k-sparse-mamba\" in sys.path:\n",
    "    sys.path.append(\"/home/dev/sae-k-sparse-mamba\")\n",
    "import os\n",
    "os.chdir('/home/dev/sae-k-sparse-mamba')\n",
    "saes = [None] # don't do layer 0 because that clutters the attributions\n",
    "from sae.sae import Sae\n",
    "\n",
    "SAE_LAYERS = list(range(1,42))\n",
    "\n",
    "ckpt_dir = \"/home/dev/sae-k-sparse-mamba/\"\n",
    "for i in SAE_LAYERS:\n",
    "    print(i)\n",
    "    hook = f'blocks.{i}.hook_out_proj'\n",
    "    path = [ckpt_dir + f for f in sorted(list(os.listdir(ckpt_dir))) if hook in f][0] + \"/\" + f'hook_{hook}.pt'\n",
    "    #path = f'/home/dev/sae-k-sparse-mamba/blocks.{i}.hook_resid_pre/hook_blocks.{i}.hook_resid_pre.pt'\n",
    "    print(path)\n",
    "    if i != 34:\n",
    "        saes.append(None)\n",
    "    else:\n",
    "        saes.append(Sae.load_from_disk(path, hook=f'blocks.{i}.hook_out_proj', device=model.cfg.device))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval with SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sae_hook(\n",
    "    x,\n",
    "    hook,\n",
    "    layer,\n",
    "):\n",
    "    B,L,D = x.size()\n",
    "    K = D//2\n",
    "    sae = saes[layer]\n",
    "    uncorrupted_features = sae.encode(x)\n",
    "    weighted_features = uncorrupted_features\n",
    "    top_acts, top_indices = weighted_features.topk(K, sorted=False)\n",
    "    # kernel can't handle doing all token positions at same time by default\n",
    "    # but if we make it think B*L is a single batch index it works fine\n",
    "    top_acts_flattened = top_acts.flatten(start_dim=0, end_dim=1)\n",
    "    top_indices_flattened = top_indices.flatten(start_dim=0, end_dim=1)\n",
    "    sae_out = sae.decode(top_acts_flattened, top_indices_flattened)\n",
    "    sae_out = sae_out.unflatten(dim=0, sizes=(B,L))\n",
    "    return sae_out\n",
    "\n",
    "\n",
    "sae_hooks = [(f'blocks.{layer}.hook_out_proj', partial(sae_hook, layer=layer)) for layer in SAE_LAYERS]\n",
    "\n",
    "model_kwargs = {\n",
    "    \"fast_ssm\": True,\n",
    "    \"fast_conv\": True\n",
    "}\n",
    "\n",
    "from acdc import accuracy_metric, wrap_run_with_hooks\n",
    "acc = data.eval(model=wrap_run_with_hooks(model=model, fwd_hooks=sae_hooks, **model_kwargs),\n",
    "                batch_size=10,\n",
    "                metric=accuracy_metric).mean()\n",
    "acc_no_sae = data.eval(model=wrap_run_with_hooks(model=model, fwd_hooks=[], **model_kwargs),\n",
    "                batch_size=10,\n",
    "                metric=accuracy_metric).mean()\n",
    "print(f\"patching with SAEs, got accuracy {acc}\")\n",
    "print(f\"baseline accuracy without patches, {acc_no_sae}\")\n",
    "\n",
    "from mamba_lens.input_dependent_hooks import clean_hooks\n",
    "\n",
    "for i in SAE_LAYERS:\n",
    "    clean_hooks(model)\n",
    "    sae_hooks = [(f'blocks.{i}.hook_out_proj', partial(sae_hook, layer=i)) for layer in range(1)]\n",
    "    #sae_hooks = [(f'hook_embed', partial(sae_hook, layer=layer)) for layer in range(i,i+1)]\n",
    "    \n",
    "    model_kwargs = {\n",
    "        \"fast_ssm\": True,\n",
    "        \"fast_conv\": True\n",
    "    }\n",
    "    \n",
    "    from acdc import accuracy_metric, wrap_run_with_hooks\n",
    "    acc = data.eval(model=wrap_run_with_hooks(model=model, fwd_hooks=sae_hooks, **model_kwargs),\n",
    "                    batch_size=10,\n",
    "                    metric=accuracy_metric).mean()\n",
    "    print(f\"layer {i}\")\n",
    "    print(f\"patching with SAEs, got accuracy {acc}\")\n",
    "    #break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE EAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha 0.0 metric 1.0 acc 1.0\n",
      "alpha 0.25 metric 0.9782790541648865 acc 1.0\n",
      "alpha 0.5 metric 0.9524111151695251 acc 1.0\n",
      "alpha 0.75 metric 0.924477756023407 acc 1.0\n",
      "alpha 1.0 metric 0.8985557556152344 acc 1.0\n",
      "20 40\n",
      "alpha 0.0 metric 1.0 acc 1.0\n",
      "alpha 0.25 metric 0.976324737071991 acc 1.0\n",
      "alpha 0.5 metric 0.9469570517539978 acc 1.0\n",
      "alpha 0.75 metric 0.9145744442939758 acc 1.0\n",
      "alpha 1.0 metric 0.8856685757637024 acc 1.0\n",
      "40 60\n",
      "alpha 0.0 metric 1.0 acc 1.0\n",
      "alpha 0.25 metric 0.9737293124198914 acc 1.0\n",
      "alpha 0.5 metric 0.9377811551094055 acc 1.0\n",
      "alpha 0.75 metric 0.8978981375694275 acc 1.0\n",
      "alpha 1.0 metric 0.8646696209907532 acc 1.0\n",
      "60 80\n",
      "alpha 0.0 metric 1.0 acc 1.0\n",
      "alpha 0.25 metric 0.9764624834060669 acc 1.0\n",
      "alpha 0.5 metric 0.9442872405052185 acc 1.0\n",
      "alpha 0.75 metric 0.9128745198249817 acc 1.0\n",
      "alpha 1.0 metric 0.8820773363113403 acc 1.0\n",
      "80 100\n",
      "alpha 0.0 metric 1.0 acc 1.0\n",
      "alpha 0.25 metric 0.9747900366783142 acc 1.0\n",
      "alpha 0.5 metric 0.94378262758255 acc 1.0\n",
      "alpha 0.75 metric 0.9111061096191406 acc 1.0\n",
      "alpha 1.0 metric 0.8816251754760742 acc 1.0\n",
      "100 120\n",
      "alpha 0.0 metric 1.0 acc 1.0\n",
      "alpha 0.25 metric 0.9738180041313171 acc 1.0\n",
      "alpha 0.5 metric 0.9420517086982727 acc 1.0\n",
      "alpha 0.75 metric 0.9100707173347473 acc 1.0\n",
      "alpha 1.0 metric 0.8790302276611328 acc 1.0\n",
      "120 140\n",
      "alpha 0.0 metric 1.0 acc 1.0\n",
      "alpha 0.25 metric 0.9759277701377869 acc 1.0\n",
      "alpha 0.5 metric 0.943084180355072 acc 1.0\n",
      "alpha 0.75 metric 0.9071030020713806 acc 1.0\n",
      "alpha 1.0 metric 0.8724527359008789 acc 1.0\n",
      "140 160\n",
      "alpha 0.0 metric 1.0 acc 1.0\n",
      "alpha 0.25 metric 0.974021852016449 acc 1.0\n",
      "alpha 0.5 metric 0.9387992024421692 acc 1.0\n",
      "alpha 0.75 metric 0.9026420712471008 acc 1.0\n",
      "alpha 1.0 metric 0.8683924078941345 acc 1.0\n",
      "160 180\n",
      "alpha 0.0 metric 1.0 acc 1.0\n",
      "alpha 0.25 metric 0.9782455563545227 acc 1.0\n",
      "alpha 0.5 metric 0.9508514404296875 acc 1.0\n",
      "alpha 0.75 metric 0.920078694820404 acc 1.0\n",
      "alpha 1.0 metric 0.89211505651474 acc 1.0\n",
      "180 200\n",
      "alpha 0.0 metric 1.0 acc 1.0\n",
      "alpha 0.25 metric 0.9718775749206543 acc 1.0\n",
      "alpha 0.5 metric 0.9374672174453735 acc 1.0\n",
      "alpha 0.75 metric 0.9027897119522095 acc 1.0\n",
      "alpha 1.0 metric 0.8708255887031555 acc 1.0\n",
      "200 220\n",
      "alpha 0.0 metric 1.0 acc 1.0\n",
      "alpha 0.25 metric 0.9748852849006653 acc 1.0\n",
      "alpha 0.5 metric 0.943303108215332 acc 1.0\n",
      "alpha 0.75 metric 0.9091581702232361 acc 1.0\n",
      "alpha 1.0 metric 0.8765384554862976 acc 0.949999988079071\n",
      "220 240\n",
      "alpha 0.0 metric 1.0 acc 1.0\n",
      "alpha 0.25 metric 0.975423276424408 acc 1.0\n",
      "alpha 0.5 metric 0.9423743486404419 acc 1.0\n",
      "alpha 0.75 metric 0.9098682403564453 acc 1.0\n",
      "alpha 1.0 metric 0.8792048692703247 acc 1.0\n",
      "240 260\n",
      "alpha 0.0 metric 1.0 acc 1.0\n",
      "alpha 0.25 metric 0.9743353128433228 acc 1.0\n",
      "alpha 0.5 metric 0.9405943751335144 acc 1.0\n",
      "alpha 0.75 metric 0.9061773419380188 acc 1.0\n",
      "alpha 1.0 metric 0.8760539293289185 acc 1.0\n",
      "260 280\n",
      "alpha 0.0 metric 1.0 acc 1.0\n",
      "alpha 0.25 metric 0.977754533290863 acc 1.0\n",
      "alpha 0.5 metric 0.9525207877159119 acc 1.0\n",
      "alpha 0.75 metric 0.9223243594169617 acc 1.0\n",
      "alpha 1.0 metric 0.8972395062446594 acc 1.0\n",
      "280 300\n",
      "alpha 0.0 metric 1.0 acc 1.0\n",
      "alpha 0.25 metric 0.9743899703025818 acc 1.0\n",
      "alpha 0.5 metric 0.9431642889976501 acc 1.0\n",
      "alpha 0.75 metric 0.9082309603691101 acc 1.0\n",
      "alpha 1.0 metric 0.875455379486084 acc 1.0\n",
      "300 320\n",
      "alpha 0.0 metric 1.0 acc 1.0\n",
      "alpha 0.25 metric 0.976448655128479 acc 1.0\n",
      "alpha 0.5 metric 0.9452592730522156 acc 1.0\n",
      "alpha 0.75 metric 0.9108982086181641 acc 1.0\n",
      "alpha 1.0 metric 0.8810368776321411 acc 0.949999988079071\n",
      "320 340\n",
      "alpha 0.0 metric 1.0 acc 1.0\n",
      "alpha 0.25 metric 0.9744068384170532 acc 1.0\n",
      "alpha 0.5 metric 0.9420963525772095 acc 1.0\n",
      "alpha 0.75 metric 0.9077807664871216 acc 1.0\n",
      "alpha 1.0 metric 0.8757312893867493 acc 1.0\n",
      "340 360\n",
      "alpha 0.0 metric 1.0 acc 1.0\n",
      "alpha 0.25 metric 0.9760710000991821 acc 1.0\n",
      "alpha 0.5 metric 0.9433626532554626 acc 1.0\n",
      "alpha 0.75 metric 0.9095600247383118 acc 1.0\n",
      "alpha 1.0 metric 0.8811621069908142 acc 0.949999988079071\n",
      "360 380\n",
      "alpha 0.0 metric 1.0 acc 1.0\n",
      "alpha 0.25 metric 0.9761883616447449 acc 1.0\n",
      "alpha 0.5 metric 0.948154628276825 acc 1.0\n",
      "alpha 0.75 metric 0.9171695709228516 acc 1.0\n",
      "alpha 1.0 metric 0.8895293474197388 acc 1.0\n",
      "380 400\n",
      "alpha 0.0 metric 1.0 acc 1.0\n",
      "alpha 0.25 metric 0.9695252776145935 acc 1.0\n",
      "alpha 0.5 metric 0.9291896820068359 acc 1.0\n",
      "alpha 0.75 metric 0.8850454688072205 acc 1.0\n",
      "alpha 1.0 metric 0.8442331552505493 acc 1.0\n"
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "from functools import partial\n",
    "from jaxtyping import Float\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from mamba_lens.input_dependent_hooks import clean_hooks\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "SAE_LAYERS = [34]\n",
    "\n",
    "model_kwargs = {\n",
    "    'fast_ssm': False,\n",
    "    'fast_conv': True\n",
    "}\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "# removes all hooks including \"leftover\" ones that might stick around due to interrupting the model at certain times\n",
    "clean_hooks(model)\n",
    "\n",
    "def normalized_logit_diff_metric(patched_logits, unpatched_logits, corrupted_logits, patched_correct, corrupted_correct, also_return_acc=False):\n",
    "    B,V = patched_logits.size()\n",
    "\n",
    "    #print(data.unpatched.logits.size(), data.patched.logits.size(), data.corrupted.logits.size())\n",
    "    A_logits_unpatched = unpatched_logits[torch.arange(B), patched_correct]\n",
    "    A_logits_patched = patched_logits[torch.arange(B), patched_correct]\n",
    "    A_logits_corrupted = corrupted_logits[torch.arange(B), patched_correct]\n",
    "\n",
    "    B_logits_unpatched = unpatched_logits[torch.arange(B), corrupted_correct]\n",
    "    B_logits_patched = patched_logits[torch.arange(B), corrupted_correct]\n",
    "    B_logits_corrupted = corrupted_logits[torch.arange(B), corrupted_correct]\n",
    "\n",
    "    # A and B are two potential outputs\n",
    "    # if A patched > B patched, we are correct\n",
    "    # else we are incorrect\n",
    "\n",
    "    # thus we could just return A_logits_patched - B_logits_patched\n",
    "\n",
    "    # however it is useful to \"normalize\" these values\n",
    "\n",
    "    # in the worst case, our patching causes us to act like corrupted, and our diff will be\n",
    "    # A_logits_corrupted - B_logits_corrupted\n",
    "    # this will result in a small, negative value\n",
    "    \n",
    "    # in the best case, our patching will do nothing (cause us to act like unpatched), and our diff will be\n",
    "    # A_logits_unpatched - B_logits_unpatched\n",
    "    # this will result in a large, positive value\n",
    "\n",
    "    # thus we can treat those as the \"min\" and \"max\" and normalize accordingly\n",
    "    \n",
    "    min_diff = A_logits_corrupted - B_logits_corrupted\n",
    "    max_diff = A_logits_unpatched - B_logits_unpatched\n",
    "\n",
    "    # the abs ensures that if it's wrong we don't try and make it more wrong\n",
    "    possible_range = torch.abs(max_diff-min_diff) # todo: try removing this\n",
    "    possible_range[possible_range == 0] = 1.0 # prevent divide by zero\n",
    "    \n",
    "    diff = A_logits_patched - B_logits_patched\n",
    "    normalized_diff = (diff-min_diff)/possible_range\n",
    "    \n",
    "    # as described, 1.0 corresponds to acting like unpatched,\n",
    "    # and 0.0 corresponds to acting like corrupted\n",
    "    res = torch.mean(normalized_diff)\n",
    "    \n",
    "    if also_return_acc:\n",
    "        num_correct = A_logits_patched > B_logits_patched\n",
    "        acc = torch.sum(num_correct)/B\n",
    "        return res, acc\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "global alpha\n",
    "alpha = 1\n",
    "\n",
    "global cached_outputs\n",
    "global corrupted_outputs\n",
    "\n",
    "\n",
    "####################### SAE HOOKS ##################\n",
    "\n",
    "global sae_terms\n",
    "global corrupted_sae_inputs\n",
    "\n",
    "\n",
    "# TODO: maybe try flowing gradient to all values instead of just topK? idk\n",
    "def sae_hook(\n",
    "    x,\n",
    "    hook,\n",
    "    layer,\n",
    "    saving\n",
    "):\n",
    "    global sae_terms\n",
    "    global alpha\n",
    "    global corrupted_sae_inputs\n",
    "    # s is [B,L,E]\n",
    "    K = saes[layer].cfg.k\n",
    "    #K = 32\n",
    "    sae = saes[layer]\n",
    "    token_outs = []\n",
    "    layer_sae_terms = []\n",
    "    B,L,D = x.size()\n",
    "    if saving=='blend':\n",
    "        uncorrupted_features = sae.encode(x)\n",
    "        corrupted_features = sae.encode(corrupted_sae_inputs[sae_layer_to_i[layer]])\n",
    "        weighted_features = uncorrupted_features*(1-alpha) + corrupted_features * alpha\n",
    "        weighted_features.retain_grad()\n",
    "    elif saving=='basic':\n",
    "        uncorrupted_features = sae.encode(x)\n",
    "        weighted_features = uncorrupted_features\n",
    "    elif saving=='cursed':\n",
    "        corrupted_features = sae.encode(corrupted_sae_inputs[sae_layer_to_i[layer]])\n",
    "        weighted_features = corrupted_features\n",
    "    top_acts, top_indices = weighted_features.topk(K, sorted=False)\n",
    "\n",
    "    # kernel can't handle doing all token positions at same time by default\n",
    "    # but if we make it think B*L is a single batch index it works fine\n",
    "    top_acts_flattened = top_acts.flatten(start_dim=0, end_dim=1)\n",
    "    top_indices_flattened = top_indices.flatten(start_dim=0, end_dim=1)\n",
    "    sae_out = sae.decode(top_acts_flattened, top_indices_flattened)\n",
    "    sae_out = sae_out.unflatten(dim=0, sizes=(B,L))\n",
    "    if saving == 'blend':\n",
    "        diff = (-uncorrupted_features + corrupted_features)\n",
    "        #print(f\"diff layer {layer} {torch.linalg.norm(diff[:,0], ord=2)}\")\n",
    "        sae_terms.append((diff, weighted_features, top_indices))\n",
    "    return sae_out\n",
    "    \n",
    "\n",
    "def bwd_sae_hook(\n",
    "    grad,\n",
    "    hook,\n",
    "    layer,\n",
    "    batch_start,\n",
    "    batch_end):\n",
    "    #print(f'{layer}')\n",
    "    global sae_terms\n",
    "    # [B, L, E]\n",
    "    #diffs = (-cached_conv_inputs[layer]+corrupted_conv_inputs[layer])\n",
    "    (diff, weighted_features, top_indices) = sae_terms[sae_layer_to_i[layer]] # Because we ignore 0\n",
    "    # weighted_features is size [B,L,NFeatures]\n",
    "    # diff is size [B,L,NFeatures]\n",
    "    #attr = weighted_features.grad*diff\n",
    "    B,L,D = diff.size()\n",
    "    #i1, i2, i3 = get_batched_index_into(top_indices)\n",
    "    global result_sae_attrs\n",
    "    global result_sae_counts\n",
    "    #attrs = (*).view(top_indices.size())\n",
    "    # [NLayers,L,D]\n",
    "    EPSILON = 0.00001\n",
    "    for b in range(B):\n",
    "        # [L,K]\n",
    "        with torch.no_grad():\n",
    "            indices = top_indices[b]\n",
    "            iL, iK = get_index_into(indices)\n",
    "            attrs = diff[b,iL,iK]*weighted_features.grad[b,iL,iK]\n",
    "            attrs[torch.abs(attrs)<EPSILON] = 0 # numerical imprecision makes things that should be zero slightly not zero\n",
    "            # [L,NFeatures]\n",
    "            result_sae_attrs[sae_layer_to_i[layer],iL,iK] += attrs\n",
    "            result_sae_counts[sae_layer_to_i[layer],iL,iK] += (torch.abs(attrs)>=EPSILON) + 0 # only increment those that are bigger than eps\n",
    "\n",
    "########## Compute Attributions ###############\n",
    "B,L = data.data.size()\n",
    "# our data is pairs of unpatched, corrupted\n",
    "n_patching_pairs = B//2\n",
    "D_CONV = model.cfg.d_conv\n",
    "\n",
    "POSITIONS = True\n",
    "NFeatures = saes[SAE_LAYERS[0]].W_dec.size()[0]\n",
    "\n",
    "sae_layer_to_i = dict([(layer, i) for (i,layer) in enumerate(SAE_LAYERS)])\n",
    "\n",
    "global result_sae_attrs\n",
    "global result_sae_counts\n",
    "result_sae_attrs = torch.zeros([len(SAE_LAYERS), L, NFeatures], device=model.cfg.device)\n",
    "result_sae_counts = torch.zeros([len(SAE_LAYERS), L, NFeatures], device=model.cfg.device)\n",
    "if POSITIONS:\n",
    "    attributions = torch.zeros([n_patching_pairs, model.cfg.n_layers+2, model.cfg.n_layers+2, L], device=model.cfg.device)\n",
    "    conv_attributions = torch.zeros([n_patching_pairs, model.cfg.n_layers, D_CONV, L], device=model.cfg.device)\n",
    "    # just a few defaultdicts\n",
    "    sae_attributions = defaultdict(lambda: # layer \n",
    "                                   defaultdict(lambda: # batch\n",
    "                                                       defaultdict(lambda: # token pos\n",
    "                                                                   defaultdict(lambda: # feature index\n",
    "                                                                               0)))) # attr\n",
    "    sae_counts = defaultdict(lambda: # layer \n",
    "                                   defaultdict(lambda: # batch\n",
    "                                                       defaultdict(lambda: # token pos\n",
    "                                                                   defaultdict(lambda: # feature index\n",
    "                                                                               0)))) # count\n",
    "else:\n",
    "    attributions = torch.zeros([n_patching_pairs, model.cfg.n_layers+2, model.cfg.n_layers+2], device=model.cfg.device)\n",
    "    conv_attributions = torch.zeros([n_patching_pairs, model.cfg.n_layers, D_CONV], device=model.cfg.device)\n",
    "    sae_attributions = defaultdict(lambda: # layer \n",
    "                                   defaultdict(lambda: # batch\n",
    "                                                       defaultdict(lambda: # feature index\n",
    "                                                                   defaultdict(lambda: 0)))) # attr\n",
    "    sae_counts = defaultdict(lambda: # layer \n",
    "                                   defaultdict(lambda: # batch\n",
    "                                                       defaultdict(lambda: # feature index\n",
    "                                                                   defaultdict(lambda: 0)))) # count\n",
    "\n",
    "global sae_attr\n",
    "sae_attr = []\n",
    "input_names = [f'blocks.{i}.hook_layer_input' for i in range(model.cfg.n_layers)]\n",
    "sae_input_names = [f'blocks.{i}.hook_out_proj' for i in range(model.cfg.n_layers)]\n",
    "output_names = [f'blocks.{i}.hook_out_proj' for i in range(model.cfg.n_layers)]\n",
    "\n",
    "def caching_sae_hook(x,\n",
    "                 hook,\n",
    "                layer):\n",
    "    \n",
    "    global corrupted_sae_inputs\n",
    "    corrupted_sae_inputs[sae_layer_to_i[layer]] = x.clone()\n",
    "    return x\n",
    "\n",
    "sae_fwd_hooks = [(sae_input_names[layer],\n",
    "                  partial(\n",
    "                      sae_hook,\n",
    "                      layer=layer,\n",
    "                      saving='basic',\n",
    "                  )) for layer in SAE_LAYERS]\n",
    "sae_fwd_hooks_corrupted = [(sae_input_names[layer],\n",
    "                  partial(\n",
    "                      sae_hook,\n",
    "                      layer=layer,\n",
    "                      saving='cursed',\n",
    "                  )) for layer in SAE_LAYERS]\n",
    "\n",
    "caching_hooks = [(sae_input_names[layer], partial(caching_sae_hook, layer=layer)) for layer in SAE_LAYERS]\n",
    "# THIS ORDER IS IMPORTANT\n",
    "sae_corruption_hooks = caching_hooks + sae_fwd_hooks\n",
    "\n",
    "# todo: maybe duplicate confuses it?\n",
    "for batch_start in range(0, n_patching_pairs, BATCH_SIZE):\n",
    "    batch_end = min(batch_start + BATCH_SIZE, n_patching_pairs)\n",
    "    print(batch_start, batch_end)\n",
    "    # we don't need grad for these forward passes\n",
    "    torch.set_grad_enabled(False)\n",
    "    embed_name = 'hook_embed'\n",
    "    clean_hooks(model)\n",
    "    # forward passes to get unpatched and corrupted\n",
    "    #unpatched_logits = model(data.data[::2][batch_start:batch_end], **model_kwargs)\n",
    "    unpatched_logits = model.run_with_hooks(data.data[::2][batch_start:batch_end], fwd_hooks=sae_fwd_hooks, **model_kwargs)\n",
    "    batch_size,L,_ = unpatched_logits.size()\n",
    "    names_to_store = sorted(list(set([embed_name] + output_names + sae_input_names))) # no duplicates\n",
    "    # for our other types of patching, this is identical to applying all the patches\n",
    "    # however for sae's that is not the case (as they are lossy)\n",
    "    # make sure to apply them in that case\n",
    "    global corrupted_sae_inputs    \n",
    "    D = model.cfg.d_model\n",
    "    corrupted_sae_inputs = torch.zeros([len(SAE_LAYERS), batch_size,L,D], device=model.cfg.device)\n",
    "    # this strategy results in non-zero diff for the first token, which is not what we want since there is no difference \n",
    "    # cache them, then insert those in so this is identical to alpha 1.0\n",
    "    #corrupted_logits = model.run_with_hooks(data.data[1::2][batch_start:batch_end], fwd_hooks=caching_hooks, **model_kwargs)\n",
    "    # pass in original as that's what happens when alpha 1.0\n",
    "    #corrupted_logits = model.run_with_hooks(data.data[::2][batch_start:batch_end], fwd_hooks=sae_fwd_hooks_corrupted, **model_kwargs)\n",
    "    # corrupted should just be cached when apply sae\n",
    "    # VERY IMPORTANT THAT CACHING HOOKS COME BEFORE SAE FWD HOOKS (otherwise will cache the result after SAE which is not what we want as that'll double apply later)\n",
    "    #corrupted_logits = model.run_with_hooks(data.data[1::2][batch_start:batch_end], fwd_hooks=caching_hooks+sae_fwd_hooks, **model_kwargs)\n",
    "    #corrupted_logits = model.run_with_hooks(data.data[::2][batch_start:batch_end], fwd_hooks=sae_fwd_hooks_corrupted, **model_kwargs)\n",
    "    corrupted_logits = model.run_with_hooks(data.data[1::2][batch_start:batch_end], fwd_hooks=caching_hooks, **model_kwargs)\n",
    "    \n",
    "    batch_size,L,_ = corrupted_logits.size()\n",
    "    \n",
    "    #batch_size,L,D = corrupted_layer_outputs[output_names[0]].size()\n",
    "    #_,_,E = corrupted_layer_outputs[conv_input_names[0]].size()\n",
    "    \n",
    "    # get only the last token position (logit for next predicted token)\n",
    "    # this is needed to support data of varying lengths\n",
    "    unpatched_logits = unpatched_logits[torch.arange(batch_size), data.last_token_position[::2][batch_start:batch_end]]\n",
    "    corrupted_logits = corrupted_logits[torch.arange(batch_size), data.last_token_position[1::2][batch_start:batch_end]]\n",
    "    \n",
    "    clean_hooks(model)\n",
    "\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "        param.grad = None # reset grads\n",
    "    \n",
    "    last_layer = model.cfg.n_layers-1\n",
    "    # forward pass to do partial patches\n",
    "    cached_outputs = torch.zeros([model.cfg.n_layers+1,batch_size,L,D], device=model.cfg.device)\n",
    "    cached_outputs.requires_grad = False\n",
    "    \n",
    "    ######### EAP layer hooks\n",
    "    # sae hooks\n",
    "    global sae_terms\n",
    "    sae_terms = []\n",
    "    fwd_hooks = [(sae_input_names[layer],\n",
    "                  partial(\n",
    "                      sae_hook,\n",
    "                      layer=layer,\n",
    "                      saving='blend',\n",
    "                  )) for layer in SAE_LAYERS]\n",
    "    \n",
    "    bwd_hooks = [(f'blocks.{layer}.hook_resid_pre', # anywhere before the slice terms works, so we'll just pick start of layer since that backward is called\n",
    "                  partial(\n",
    "                      bwd_sae_hook,\n",
    "                      layer=layer,\n",
    "                      batch_start=batch_start,\n",
    "                      batch_end=batch_end,\n",
    "                  )) for layer in SAE_LAYERS]\n",
    "\n",
    "\n",
    "    # add the hooks\n",
    "    \n",
    "    clean_hooks(model)\n",
    "    for fwd in fwd_hooks:\n",
    "        model.add_hook(*fwd, \"fwd\")\n",
    "\n",
    "    for bwd in bwd_hooks:\n",
    "        model.add_hook(*bwd, \"bwd\")\n",
    "    \n",
    "    # with integrated gradients\n",
    "    # simply sums over doing \"partial patches\" like 0.2 patch and 0.8 unpatched \n",
    "    # ITERS = 1 is just edge attribution patching (without integraded gradients)\n",
    "    ITERS = 5\n",
    "    for i in range(ITERS+1):\n",
    "        global alpha\n",
    "        # alpha ranges from 0 to 1\n",
    "        if ITERS > 1:\n",
    "            alpha = i/float(ITERS-1)\n",
    "        elif ITERS == 1: # no integrated gradients, set alpha to 1\n",
    "            alpha = 1.0\n",
    "\n",
    "        # it tries to propogate gradients to these, detach them\n",
    "        sae_terms.clear()\n",
    "        torch.cuda.empty_cache()\n",
    "        cached_outputs[:] = 0\n",
    "        cached_outputs.grad = None\n",
    "        cached_outputs.detach_()\n",
    "        corrupted_sae_inputs.grad = None\n",
    "        corrupted_sae_inputs.detach_()\n",
    "        model.zero_grad()\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        if i == ITERS:\n",
    "            torch.cuda.empty_cache()\n",
    "            break # we just use this for cleaning up\n",
    "        logits = model(data.data[::2][batch_start:batch_end], **model_kwargs)\n",
    "        logits = logits[torch.arange(batch_size), data.last_token_position[::2][batch_start:batch_end]]\n",
    "        metric,acc = normalized_logit_diff_metric(\n",
    "            patched_logits=logits,\n",
    "            unpatched_logits=unpatched_logits,\n",
    "            corrupted_logits=corrupted_logits,\n",
    "            patched_correct=data.correct[::2][batch_start:batch_end][:,0],\n",
    "            corrupted_correct=data.correct[1::2][batch_start:batch_end][:,0],\n",
    "            also_return_acc=True\n",
    "        )\n",
    "        print(f\"alpha {alpha} metric {metric} acc {acc}\")\n",
    "        # run backward pass, which adds to attributions\n",
    "        metric.backward()\n",
    "        #conv_attrs = conv_attributions.mean(dim=0)\n",
    "        #print(conv_attrs)\n",
    "        #attrs = attributions.mean(dim=0)\n",
    "        #print(attrs)\n",
    "    # average over all the iters\n",
    "    \n",
    "    #sae_attr[-1] = torch.mean(torch.stack(sae_attr[-1]), dim=0)\n",
    "\n",
    "# todo: maybe the diffs should have alpha in the backward pass? No, that would mean alpha of 0 gives all zero attrs\n",
    "\n",
    "# average over all the samples\n",
    "\n",
    "\n",
    "# don't need grad for rest of this\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "clean_hooks(model)\n",
    "\n",
    "# of note: maybe the diff*grad doesn't make sense, they have different argmaxes, we should really be working in original feature space or somethin\n",
    "\n",
    "# we don't get 0.0 for alpha of 1.0 because \"fully applying the patch\" doesn't apply it to embed or zero, so it's only going to look like corrupted for\n",
    "# so really, my corrupted should be only a partial patch\n",
    "# the layers that we patch on\n",
    "# I did this it's very cursed but I did it and now it's zero for alpha 1.0 as (desired??)\n",
    "\n",
    "# don't divide by zero\n",
    "result_sae_counts[result_sae_counts==0] = 1.0\n",
    "# average\n",
    "result_sae_attrs /= result_sae_counts\n",
    "\n",
    "\n",
    "# MAYBE I SHOULD TRY SCALING EVERYTHING? SO ROUNDING ISSUES CAUSE LESS PROBLEM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save((result_sae_counts, result_sae_attrs), \"feature_attrs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating edges:\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "layer 34 pos 0 features 0\n",
      "layer 34 pos 1 features 0\n",
      "layer 34 pos 2 features 0\n",
      "layer 34 pos 3 features 0\n",
      "layer 34 pos 4 features 0\n",
      "layer 34 pos 5 features 0\n",
      "layer 34 pos 6 features 0\n",
      "layer 34 pos 7 features 0\n",
      "layer 34 pos 8 features 0\n",
      "layer 34 pos 9 features 0\n",
      "layer 34 pos 10 features 0\n",
      "layer 34 pos 11 features 0\n",
      "layer 34 pos 12 features 10\n",
      "layer 34 pos 13 features 0\n",
      "layer 34 pos 14 features 0\n",
      "layer 34 pos 15 features 0\n",
      "layer 34 pos 16 features 0\n",
      "layer 34 pos 17 features 5081\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "getting unpatched\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting valid unpatched\n",
      "sorting\n",
      "check num always keep\n",
      "always keep 864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f9c0920b550>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#del result_sae_counts, result_sae_attrs\n",
    "#result_sae_counts, result_sae_attrs = torch.load(\"feature_attrs.pkl\")\n",
    "\n",
    "model_kwargs = {\n",
    "    'fast_ssm': True,\n",
    "    'fast_conv': True\n",
    "}\n",
    "\n",
    "total = data.data.size()[0]//2*ITERS\n",
    "#otal = 4160*20\n",
    "#total = 20000*25\n",
    "original_total = 400*5\n",
    "# 200 works well\n",
    "original_prop = 200/float(original_total)\n",
    "new_prop = total*original_prop\n",
    "from mamba_lens.input_dependent_hooks import clean_hooks\n",
    "clean_hooks(model)\n",
    "import sys\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import wandb\n",
    "import os\n",
    "import signal\n",
    "import acdc\n",
    "from tqdm import tqdm\n",
    "from typing import Any  \n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "###################################### SAE ###########################\n",
    "\n",
    "\n",
    "# we do a hacky thing where this first hook clears the global storage\n",
    "# second hook stores all the hooks\n",
    "# then third hook computes the output (over all the hooks)\n",
    "# this avoids recomputing and so is much faster\n",
    "SAE_HOOKS = \"sae hooks\"\n",
    "SAE_BATCHES = \"sae batches\"\n",
    "SAE_OUTPUT = \"sae output\"\n",
    "def sae_patching_storage_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    sae_feature_i: int,\n",
    "    dummy: bool,\n",
    "    position: int,\n",
    "    layer: int,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    **kwargs,\n",
    "):\n",
    "    global sae_storage\n",
    "    if not SAE_HOOKS in sae_storage:\n",
    "        sae_storage[SAE_HOOKS] = [] # we can't do this above because it'll be emptied again on the next batch before this is called\n",
    "    sae_storage[SAE_OUTPUT] = None # clear output\n",
    "    sae_storage[SAE_HOOKS].append({\"position\": position, \"sae_feature_i\": sae_feature_i, \"dummy\": dummy})\n",
    "    #print(f\"sae feature i {sae_feature_i} position {position} layer {layer}\")\n",
    "    return x\n",
    "\n",
    "from jaxtyping import Float\n",
    "from einops import rearrange\n",
    "\n",
    "global sae_storage\n",
    "sae_storage = {}\n",
    "def sae_patching_hook(\n",
    "    x: Float[torch.Tensor, \"B L E\"],\n",
    "    hook: HookPoint,\n",
    "    input_hook_name: str,\n",
    "    layer: int,\n",
    "    **kwargs,\n",
    ") -> Float[torch.Tensor, \"B L E\"]:\n",
    "    global sae_storage\n",
    "    ### This is identical to what the conv is doing\n",
    "    # but we break it apart so we can patch on individual filters\n",
    "    # we have two input hooks, the second one is the one we want\n",
    "    input_hook_name = input_hook_name[1]\n",
    "    # don't recompute these if we don't need to\n",
    "    # because we stored all the hooks and batches in conv_storage, we can just do them all at once\n",
    "    \n",
    "    # they need to share an output because they write to the same output tensor\n",
    "    if sae_storage[SAE_OUTPUT] is None:\n",
    "        #print(f\"running for layer {layer}\")\n",
    "        K = saes[layer].cfg.k\n",
    "        sae = saes[layer]\n",
    "        #print(f\"layer {layer} storage {sae_storage}\")\n",
    "        sae_output = torch.zeros(x.size(), device=model.cfg.device)\n",
    "        #print(\"layer\", layer, \"keys\", conv_storage)\n",
    "        def get_filter_key(i):\n",
    "            return f'filter_{i}'\n",
    "        sae_input_uncorrupted = x[::2]\n",
    "        sae_input_corrupted = x[1::2]\n",
    "        B, L, D = sae_input_uncorrupted.size()\n",
    "        for l in range(L):\n",
    "            # [B, NFeatures]                             [B,D]\n",
    "            uncorrupted_features = sae.encode(sae_input_uncorrupted[:,l])\n",
    "            # [B, NFeatures]                             [B,D]\n",
    "            corrupted_features = sae.encode(sae_input_corrupted[:,l])\n",
    "            patched_features = corrupted_features.clone()\n",
    "            #patched_features = torch.zeros(corrupted_features.size(), device=model.cfg.device) # patch everything except the features we are keeping around\n",
    "            # apply hooks (one hook applies to a single feature)\n",
    "            #print(f\"{len(sae_storage[SAE_HOOKS])} hooks\")\n",
    "            \n",
    "            for hook_data in sae_storage[SAE_HOOKS]:\n",
    "                position = hook_data['position']\n",
    "                sae_feature_i = hook_data['sae_feature_i']\n",
    "                dummy = hook_data['dummy']\n",
    "                if not dummy and (position == l or position is None): # position is None means all positions\n",
    "                    if copy_from_other:\n",
    "                        patched_features[:,sae_feature_i] = corrupted_features[:,sae_feature_i]\n",
    "                    else:\n",
    "                        patched_features[:,sae_feature_i] = uncorrupted_features[:,sae_feature_i]\n",
    "                    \n",
    "                    #print(f\"applying sae feature {sae_feature_i} to position {position} for layer {layer}\")\n",
    "                    #uncorrupted_features[:,sae_feature_i] = corrupted_features[:,sae_feature_i]\n",
    "            \n",
    "            # compute sae outputs\n",
    "            patched_top_acts, patched_top_indices = patched_features.topk(K, sorted=False)\n",
    "            corrupted_top_acts, corrupted_top_indices = corrupted_features.topk(K, sorted=False)      \n",
    "            sae_output[::2,l] = sae.decode(patched_top_acts, patched_top_indices)     \n",
    "            sae_output[1::2,l] = sae.decode(corrupted_top_acts, corrupted_top_indices)\n",
    "        sae_storage = {} # clean up and prepare for next layer\n",
    "        sae_storage[SAE_OUTPUT] = sae_output # store the output\n",
    "    return sae_storage[SAE_OUTPUT]\n",
    "\n",
    "\n",
    "\n",
    "limited_layers = list(range(model.cfg.n_layers))\n",
    "\n",
    "\n",
    "from acdc.data.ioi import ioi_data_generator, ABC_TEMPLATES, get_all_single_name_abc_patching_formats\n",
    "from acdc.data.utils import generate_dataset\n",
    "\n",
    "num_patching_pairs = 200\n",
    "seed = 27\n",
    "valid_seed = 28\n",
    "constrain_to_answers = True\n",
    "# this makes our data size 800, first 400 is each (a,b) pair, and then second 400 is each pair swapped to be (b,a)\n",
    "has_symmetric_patching = True\n",
    "\n",
    "\n",
    "## Setup edges for ACDC\n",
    "edges = []\n",
    "B,L = data.data.size()\n",
    "#positions = list(range(L)) # \n",
    "\n",
    "if POSITIONS:\n",
    "    positions = list(range(L))\n",
    "else:\n",
    "    positions = [None]\n",
    "\n",
    "INPUT_HOOK = f'hook_embed'\n",
    "INPUT_NODE = 'embed'\n",
    "\n",
    "last_layer = model.cfg.n_layers-1\n",
    "OUTPUT_HOOK = f'blocks.{last_layer}.hook_resid_post'\n",
    "OUTPUT_NODE = 'output'\n",
    "\n",
    "def input(layer):\n",
    "    return f'{layer}.i'\n",
    "\n",
    "def output(layer):\n",
    "    return f'{layer}.o'\n",
    "\n",
    "def sae(layer):\n",
    "    return f'{layer}.sae'\n",
    "\n",
    "def conv(layer):\n",
    "    return f'{layer}.conv'\n",
    "\n",
    "def skip(layer):\n",
    "    return f'{layer}.skip'\n",
    "\n",
    "def ssm(layer):\n",
    "    return f'{layer}.ssm'\n",
    "\n",
    "# important to have storage be global and not passed into the hooks! Otherwise it gets very slow (tbh, i don't know why)\n",
    "global storage\n",
    "storage = {}\n",
    "\n",
    "\n",
    "\n",
    "#attrs = attributions.mean(dim=0)\n",
    "#conv_attrs = conv_attributions.mean(dim=0)\n",
    "ALWAYS_KEEP_WEIGHT = -10\n",
    "\n",
    "ONLY_SAE = True\n",
    "def remove_non_sae(w):\n",
    "    return ALWAYS_KEEP_WEIGHT\n",
    "\n",
    "print(\"creating edges:\")\n",
    "for layer in limited_layers:\n",
    "    print(layer)\n",
    "    #print(f'layer {layer}')\n",
    "    for pos_i, pos in enumerate(positions):\n",
    "        num_copy = 0\n",
    "        #print(f'position {pos}')\n",
    "        # edge from embed to layer input\n",
    "        if layer in SAE_LAYERS:\n",
    "            # this is a bit gross in that we rely on the order that layers are inputted (so the edges above this have hooks applied before us, and later have hooks applied after us)\n",
    "            #if pos is None:\n",
    "            #    available_features = all_used_layer_sae_features[layer]\n",
    "            #else:\n",
    "            #    available_features = all_used_layer_sae_features[layer][pos]\n",
    "            #available_features = sorted(list(available_features)) # deterministic\n",
    "            #print(f'{len(available_features)} features')\n",
    "            #new_prop = 10\n",
    "            #available_features = torch.where(torch.logical_and(result_sae_counts[sae_layer_to_i[layer],pos]>new_prop,result_sae_attrs[sae_layer_to_i[layer],pos] != 0.0))[0]\n",
    "            available_features = torch.where(torch.logical_and(result_sae_counts[sae_layer_to_i[layer],pos]>12,result_sae_attrs[sae_layer_to_i[layer],pos] != 0.0))[0]\n",
    "            print(f\"layer {layer} pos {pos} features {len(available_features)}\")\n",
    "            for sae_feature_i in available_features:\n",
    "                # multiply attr by count (idk why but I find this gives less edges)\n",
    "                attr = result_sae_attrs[sae_layer_to_i[layer],pos,sae_feature_i].flatten()[0].item()*result_sae_counts[sae_layer_to_i[layer],pos,sae_feature_i].flatten().item()\n",
    "                #attr = result_sae_attrs[sae_layer_to_i[layer],pos,sae_feature_i].flatten()[0].item()\n",
    "                copy_from_other = False\n",
    "                #if attr == 0.0:\n",
    "                #    copy_from_other = True\n",
    "                #    attr = ALWAYS_KEEP_WEIGHT\n",
    "                #    num_copy += 1\n",
    "                edges.append(Edge(\n",
    "                        label=(f'[{pos}:{sae_feature_i}]'.replace(\"None:\", \"\")),\n",
    "                        input_node=input(layer),\n",
    "                        input_hook=[\n",
    "                            (f'blocks.{layer}.hook_resid_pre', partial(sae_patching_storage_hook, position=pos, dummy=False, layer=layer, copy_from_other=copy_from_other, sae_feature_i=sae_feature_i))\n",
    "                        ],\n",
    "                        output_node=sae(layer),\n",
    "                        output_hook=(f'blocks.{layer}.hook_out_proj', partial(sae_patching_hook, position=pos, layer=layer)),\n",
    "                        score_diff_when_patched=attr,\n",
    "                ))\n",
    "                edges[-1].original_attr = result_sae_attrs[sae_layer_to_i[layer],pos,sae_feature_i].flatten()[0].item()\n",
    "                edges[-1].original_count = result_sae_counts[sae_layer_to_i[layer],pos,sae_feature_i].flatten().item()\n",
    "            if num_copy > 0:\n",
    "                print(f\"layer {layer} pos {pos} num copy {num_copy}\")\n",
    "            for i in range(100):\n",
    "                if not i in available_features:\n",
    "                    keep_i = i\n",
    "                    break\n",
    "            # we need one that always exists to ensure that saes are always applied\n",
    "            edges.append(Edge(\n",
    "                    label=(f'[{pos}:KEEP]'.replace(\"None:\", \"\")),\n",
    "                    input_node=input(layer),\n",
    "                    input_hook=[\n",
    "                        (f'blocks.{layer}.hook_resid_pre', partial(sae_patching_storage_hook, position=pos, layer=layer, dummy=True, sae_feature_i=-1))\n",
    "                    ],\n",
    "                    output_node=sae(layer),\n",
    "                    output_hook=(f'blocks.{layer}.hook_out_proj', partial(sae_patching_hook, position=pos, layer=layer)),\n",
    "                    score_diff_when_patched=ALWAYS_KEEP_WEIGHT,\n",
    "            ))\n",
    "        else:\n",
    "            # skip to output\n",
    "            edges.append(Edge(\n",
    "                    input_node=input(layer),\n",
    "                    output_node=sae(layer),\n",
    "                    score_diff_when_patched=ALWAYS_KEEP_WEIGHT\n",
    "            ))\n",
    "\n",
    "\n",
    "\n",
    "def normalized_logit_diff_acdc_metric(data: ACDCEvalData, printing=False):\n",
    "    B,V = data.patched.logits.size()\n",
    "\n",
    "    # [batch_size]\n",
    "    patched_correct = data.patched.correct[:,0]\n",
    "    #print(data.unpatched.logits.size(), data.patched.logits.size(), data.corrupted.logits.size())\n",
    "    A_logits_unpatched = data.unpatched.logits[torch.arange(B), patched_correct]\n",
    "    A_logits_patched = data.patched.logits[torch.arange(B), patched_correct]\n",
    "    A_logits_corrupted = data.corrupted.logits[torch.arange(B), patched_correct]\n",
    "\n",
    "    corrupted_correct = data.corrupted.correct[:,0]\n",
    "    B_logits_unpatched = data.unpatched.logits[torch.arange(B), corrupted_correct]\n",
    "    B_logits_patched = data.patched.logits[torch.arange(B), corrupted_correct]\n",
    "    B_logits_corrupted = data.corrupted.logits[torch.arange(B), corrupted_correct]\n",
    "\n",
    "    # A and B are two potential outputs\n",
    "    # if A patched > B patched, we are correct\n",
    "    # else we are incorrect\n",
    "\n",
    "    # thus we could just return A_logits_patched - B_logits_patched\n",
    "\n",
    "    # however it is useful to \"normalize\" these values\n",
    "\n",
    "    # in the worst case, our patching causes us to act like corrupted, and our diff will be\n",
    "    # A_logits_corrupted - B_logits_corrupted\n",
    "    # this will result in a small, negative value\n",
    "    \n",
    "    # in the best case, our patching will do nothing (cause us to act like unpatched), and our diff will be\n",
    "    # A_logits_unpatched - B_logits_unpatched\n",
    "    # this will result in a large, positive value\n",
    "\n",
    "    # thus we can treat those as the \"min\" and \"max\" and normalize accordingly\n",
    "    \n",
    "    min_diff = A_logits_corrupted - B_logits_corrupted\n",
    "    max_diff = A_logits_unpatched - B_logits_unpatched\n",
    "\n",
    "    possible_range = (max_diff-min_diff)\n",
    "    possible_range[possible_range == 0] = 1.0 # prevent divide by zero\n",
    "    \n",
    "    diff = A_logits_patched - B_logits_patched\n",
    "    normalized_diff = (diff-min_diff)/torch.abs(possible_range) # abs prevents incorrect data from wanting to be more incorrect\n",
    "\n",
    "    if printing:\n",
    "        print(f\"A corrupted {A_logits_corrupted}\")\n",
    "        print(f\"B corrupted {B_logits_corrupted}\")\n",
    "        print(f\"A unpatched {A_logits_unpatched}\")\n",
    "        print(f\"B unpatched {B_logits_unpatched}\")\n",
    "        print(f\"A patched {A_logits_patched}\")\n",
    "        print(f\"B patched {B_logits_patched}\")\n",
    "        print(f\"min diff {min_diff}\")\n",
    "        print(f\"max diff {max_diff}\")\n",
    "        print(f\"possible range {possible_range}\")\n",
    "        print(f\"diff {diff}\")\n",
    "        print(f\"normalized diff {normalized_diff}\")\n",
    "    # as described, 1.0 corresponds to acting like unpatched,\n",
    "    # and 0.0 corresponds to acting like corrupted\n",
    "\n",
    "    return torch.mean(normalized_diff)\n",
    "import acdc\n",
    "cfg = ACDCConfig(\n",
    "    ckpt_directory = \"blah\",\n",
    "    thresh = 0.0001,\n",
    "    rollback_thresh = 0.0001,\n",
    "    metric=acdc.accuracy_metric,\n",
    "    # extra inference args\n",
    "    model_kwargs=model_kwargs,\n",
    "    # these are needed for doing graph pruning\n",
    "    input_node=INPUT_NODE,\n",
    "    output_node=OUTPUT_NODE,\n",
    "    # batch size for evaluating data points\n",
    "    batch_size=1,\n",
    "    log_level=LOG_LEVEL_INFO,\n",
    "    # if False, will be equivalent to batch_size=1\n",
    "    batched = True,\n",
    "    # set these two to false to use traditional ACDC\n",
    "    # recursive will try patching multiple at a time (this is faster sometimes)\n",
    "    recursive = True,\n",
    "    # try_patching_multiple_at_same_time will evaluate many different patchings before commiting to any\n",
    "    # and includes a rollback scheme if after patching one, the others get worse\n",
    "    try_patching_multiple_at_same_time = True,\n",
    "    ## if true, you metric will also have the logits from a run with no patching available\n",
    "    # (useful for normalized logit diff)\n",
    "    store_unpatched_logits = True,\n",
    ")\n",
    "\n",
    "# accuracy assumes only one name\n",
    "\n",
    "clean_hooks(model)\n",
    "\n",
    "print(\"getting unpatched\")\n",
    "from acdc import get_currently_patched_edge_hooks, eval_acdc, wrap_run_with_hooks, get_logits_of_predicted_next_token\n",
    "unpatched_logits = get_logits_of_predicted_next_token(\n",
    "        model=model,\n",
    "        data=data.data,\n",
    "        last_token_position=data.last_token_position,\n",
    "        **model_kwargs\n",
    ")\n",
    "\n",
    "print(\"getting valid unpatched\")\n",
    "valid_unpatched_logits = get_logits_of_predicted_next_token(\n",
    "        model=model,\n",
    "        data=data.valid_data,\n",
    "        last_token_position=data.valid_last_token_position,\n",
    "        **model_kwargs\n",
    ")\n",
    "\n",
    "def eval_edges(edges_keeping, all_edges, valid=False):\n",
    "    for edge in all_edges:\n",
    "        edge.patching = False\n",
    "        edge.checked = True\n",
    "    for edge in edges_keeping:\n",
    "        edge.patching = True\n",
    "        edge.checked = True\n",
    "    currently_patched_edge_hooks = get_currently_patched_edge_hooks(cfg=cfg, edges=edges)\n",
    "    if valid:\n",
    "        return eval_acdc(\n",
    "                model=wrap_run_with_hooks(model=model, fwd_hooks=currently_patched_edge_hooks, **cfg.model_kwargs),\n",
    "                data=data.valid_data,\n",
    "                last_token_position=data.valid_last_token_position,\n",
    "                correct=data.valid_correct,\n",
    "                incorrect=data.valid_incorrect,\n",
    "                metric=cfg.metric,\n",
    "                num_edges=1,\n",
    "                constrain_to_answers=data.constrain_to_answers,\n",
    "                unpatched_logits=valid_unpatched_logits)[0].item()\n",
    "    else:\n",
    "        return eval_acdc(\n",
    "            model=wrap_run_with_hooks(model=model, fwd_hooks=currently_patched_edge_hooks, **cfg.model_kwargs),\n",
    "            data=data.data,\n",
    "            last_token_position=data.last_token_position,\n",
    "            correct=data.correct,\n",
    "            incorrect=data.incorrect,\n",
    "            metric=cfg.metric,\n",
    "            num_edges=1,\n",
    "            constrain_to_answers=data.constrain_to_answers,\n",
    "            unpatched_logits=unpatched_logits)[0].item()\n",
    "\n",
    "# can also do this instead to include negative contributions but I find the graph is larger, ymmv\n",
    "#edges.sort(key=lambda x: -abs(x[0]))\n",
    "print(\"sorting\")\n",
    "edges.sort(key=lambda edge: edge.score_diff_when_patched)\n",
    "\n",
    "import math\n",
    "def test_pos(pos):\n",
    "    #print(f\"testing pos {pos}\")\n",
    "    edges_to_keep = edges[:pos]\n",
    "    metric = eval_edges(edges_to_keep, edges)\n",
    "    #print(f\"testing pos {pos} got metric {metric}\")\n",
    "    return metric\n",
    "\n",
    "# from https://en.wikipedia.org/wiki/Binary_search_algorithm\n",
    "def binary_search(n, T):\n",
    "    L = 0\n",
    "    R = n - 1\n",
    "    while L != R:\n",
    "        m = math.ceil((L + R) / 2)\n",
    "        if test_pos(m) > T:\n",
    "            R = m - 1\n",
    "        else:\n",
    "            L = m\n",
    "    # go one further because this gives us below thresh\n",
    "    return min(n-1, L+1)\n",
    "\n",
    "print(\"check num always keep\")\n",
    "num_always_keep = 0\n",
    "for edge in edges:\n",
    "    if edge.score_diff_when_patched == ALWAYS_KEEP_WEIGHT:\n",
    "        num_always_keep += 1\n",
    "print(f\"always keep {num_always_keep}\")\n",
    "\n",
    "ACC_THRESH = 0.85\n",
    "torch.set_grad_enabled(False)\n",
    "#cutoff = binary_search(len(edges), T=ACC_THRESH)\n",
    "\n",
    "#edges_to_keep = edges[:cutoff]\n",
    "#scores = [edge.score_diff_when_patched for edge in edges[:cutoff]]\n",
    "#print(f\"keeping top {cutoff} edges\")\n",
    "#metric = eval_edges(edges_to_keep, edges)\n",
    "#print(f\"got metric {metric}\")    \n",
    "\n",
    "#metric = eval_edges(edges_to_keep, edges, valid=True)\n",
    "#print(f\"got valid metric {metric}\")    \n",
    "# .85 has 3061 edges for 5 ITERS\n",
    "# .85 has 2876 edges for 30 ITERS\n",
    "\n",
    "# todo: fix the \"only everything\" bug for convs too\n",
    "# patching format 0:\n",
    "# 2162 edges (2016 always keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## just basic logit diff, doesn't go to zero\n",
    "\n",
    "# occur more than 24 times, multiply by count\n",
    "# testing pos 2112 got metric 0.8524999618530273\n",
    "# keeping top 2112 edges\n",
    "\n",
    "# 12 times\n",
    "# testing pos 2178\n",
    "# testing pos 2178 got metric 0.8499999642372131\n",
    "\n",
    "# 30 times\n",
    "# testing pos 1765 got metric 0.8399999737739563\n",
    "# keeping top 1765 edges\n",
    "\n",
    "## 0 to 1 logit diff, by corrective thing\n",
    "# testing pos 5969 got metric 0.8474999666213989 for not multiply by count\n",
    "\n",
    "# occur more than 6 times, multiply by count\n",
    "# testing pos 2389 got metric 0.8524999618530273\n",
    "\n",
    "# occur more than 12 times, multiply by count\n",
    "# testing pos 2360 got metric 0.8499999642372131\n",
    "# keeping top 2361 edges\n",
    "\n",
    "# occur more than 24 times, multiply by count\n",
    "# testing pos 2356 got metric 0.8524999618530273\n",
    "# keeping top 2356 edges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 34 with 1215 unique features (0 duplicated)\n",
      "  pos pos17,..., num sae 1215 min attr scaled -1.961\n",
      "total num features 1215\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target val 0.81\n",
      "keeping 13 edges\n",
      "got valid metric 0.8100000023841858\n",
      "layer 34 with 13 unique features (0 duplicated)\n",
      "  pos pos17,..., num sae 13 min attr scaled -1.961\n",
      "total num features 13\n",
      "target val 0.8232499995231629\n",
      "keeping 257 edges\n",
      "got valid metric 0.824999988079071\n",
      "layer 34 with 257 unique features (0 duplicated)\n",
      "  pos pos17,..., num sae 257 min attr scaled -1.961\n",
      "total num features 257\n",
      "target val 0.8364999990463258\n",
      "keeping 661 edges\n",
      "got valid metric 0.8374999761581421\n",
      "layer 34 with 661 unique features (0 duplicated)\n",
      "  pos pos17,..., num sae 661 min attr scaled -1.961\n",
      "total num features 661\n",
      "target val 0.8497499985694885\n",
      "keeping 1122 edges\n",
      "got valid metric 0.8499999642372131\n",
      "layer 34 with 1122 unique features (0 duplicated)\n",
      "  pos pos17,..., num sae 1122 min attr scaled -1.961\n",
      "total num features 1122\n",
      "target val 0.8629999980926514\n",
      "keeping 1865 edges\n",
      "got valid metric 0.8650000095367432\n",
      "layer 34 with 1865 unique features (0 duplicated)\n",
      "  pos pos17,..., num sae 1865 min attr scaled -1.961\n",
      "total num features 1865\n",
      "target val 0.8762499976158142\n",
      "keeping 3407 edges\n",
      "got valid metric 0.8774999976158142\n",
      "layer 34 with 3407 unique features (0 duplicated)\n",
      "  pos pos12 proteins num sae 1 min attr scaled -0.333\n",
      "  pos pos17,..., num sae 3406 min attr scaled -1.961\n",
      "total num features 3407\n",
      "target val 0.8894999971389771\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m targetAdjusted \u001b[38;5;241m=\u001b[39m (target\u001b[38;5;241m*\u001b[39m(maxVal\u001b[38;5;241m-\u001b[39mminVal)\u001b[38;5;241m+\u001b[39mminVal)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget val \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtargetAdjusted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m cutoff \u001b[38;5;241m=\u001b[39m \u001b[43mbinary_search\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43medges\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargetAdjusted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeeping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcutoff\u001b[38;5;241m-\u001b[39mnum_always_keep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m edges\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m edges_to_keep \u001b[38;5;241m=\u001b[39m edges[:cutoff]\n",
      "Cell \u001b[0;32mIn[6], line 417\u001b[0m, in \u001b[0;36mbinary_search\u001b[0;34m(n, T)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m L \u001b[38;5;241m!=\u001b[39m R:\n\u001b[1;32m    416\u001b[0m     m \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil((L \u001b[38;5;241m+\u001b[39m R) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtest_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    418\u001b[0m         R \u001b[38;5;241m=\u001b[39m m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[6], line 407\u001b[0m, in \u001b[0;36mtest_pos\u001b[0;34m(pos)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_pos\u001b[39m(pos):\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m#print(f\"testing pos {pos}\")\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     edges_to_keep \u001b[38;5;241m=\u001b[39m edges[:pos]\n\u001b[0;32m--> 407\u001b[0m     metric \u001b[38;5;241m=\u001b[39m \u001b[43meval_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43medges_to_keep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;66;03m#print(f\"testing pos {pos} got metric {metric}\")\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metric\n",
      "Cell \u001b[0;32mIn[6], line 387\u001b[0m, in \u001b[0;36meval_edges\u001b[0;34m(edges_keeping, all_edges, valid)\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m eval_acdc(\n\u001b[1;32m    377\u001b[0m             model\u001b[38;5;241m=\u001b[39mwrap_run_with_hooks(model\u001b[38;5;241m=\u001b[39mmodel, fwd_hooks\u001b[38;5;241m=\u001b[39mcurrently_patched_edge_hooks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcfg\u001b[38;5;241m.\u001b[39mmodel_kwargs),\n\u001b[1;32m    378\u001b[0m             data\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mvalid_data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    384\u001b[0m             constrain_to_answers\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mconstrain_to_answers,\n\u001b[1;32m    385\u001b[0m             unpatched_logits\u001b[38;5;241m=\u001b[39mvalid_unpatched_logits)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meval_acdc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrap_run_with_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfwd_hooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrently_patched_edge_hooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_token_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_token_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorrect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincorrect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincorrect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_edges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconstrain_to_answers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstrain_to_answers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43munpatched_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munpatched_logits\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/acdc/acdc.py:269\u001b[0m, in \u001b[0;36meval_acdc\u001b[0;34m(model, data, last_token_position, correct, incorrect, metric, num_edges, constrain_to_answers, unpatched_logits, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03mInternal function,\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03mevaluates model on the data, using metric,\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03mreturns (metric_result, patched_logits) of sizes (num_edges, data.size()[0]//2)\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    267\u001b[0m num_examples \u001b[38;5;241m=\u001b[39m correct\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 269\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mget_logits_of_predicted_next_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_token_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_token_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m pad \u001b[38;5;241m=\u001b[39m get_pad_token(tokenizer\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtokenizer)\n\u001b[1;32m    272\u001b[0m logits[:,pad] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39minf \u001b[38;5;66;03m# manually set pad pr to -inf logit because sometimes we need to pad num correct or num incorrect\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Checks\n",
    "\n",
    "cutoffs = {}\n",
    "minVal = 0.81\n",
    "maxVal = 0.9424999952316284\n",
    "for i in range(0,10):\n",
    "    target = i/10.0\n",
    "    targetAdjusted = (target*(maxVal-minVal)+minVal)\n",
    "    print(f\"target val {targetAdjusted}\")\n",
    "    cutoff = binary_search(len(edges), T=targetAdjusted)\n",
    "    print(f\"keeping {cutoff-num_always_keep} edges\")\n",
    "    edges_to_keep = edges[:cutoff]\n",
    "    metric = eval_edges(edges_to_keep, edges, valid=True)\n",
    "    print(f\"got valid metric {metric}\")    \n",
    "    cutoffs[i] = cutoff\n",
    "    sae_edges = defaultdict(lambda: defaultdict(lambda: []))\n",
    "    counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    position_map = {}\n",
    "    toks = model.to_str_tokens(data.data[0])\n",
    "    for l in range(L):\n",
    "        position_map[l] = f'pos{l}{toks[l]}'\n",
    "        \n",
    "    for edge in edges_to_keep:\n",
    "        if '.sae' in edge.output_node and not edge.label is None:\n",
    "            # [pos:feature_i]\n",
    "            label = edge.label[1:-1]\n",
    "            pos, feature_i = label.split(\":\")\n",
    "            pos = int(pos)\n",
    "            if feature_i == 'KEEP': continue # dummy edge used to ensure sae always applied\n",
    "            feature_i = int(feature_i)\n",
    "            layer = int(edge.output_node.split(\".\")[0])\n",
    "            attr = edge.score_diff_when_patched\n",
    "            sae_edges[layer][pos].append((attr, feature_i))\n",
    "            counts[layer][feature_i] += 1\n",
    "\n",
    "    total_num_features = 0\n",
    "    for layer in sorted(list(sae_edges.keys())):\n",
    "        print(f\"layer {layer} with {len(counts[layer])} unique features ({len([x for x in counts[layer].values() if x > 1])} duplicated)\")\n",
    "        total_num_features += len(counts[layer])\n",
    "        values = sae_edges[layer]\n",
    "        for pos in sorted(list(values.keys())):\n",
    "            print(f\"  pos {position_map[pos]} num sae {len(values[pos])} min attr scaled {'{:.3f}'.format(1000*min([x[0] for x in values[pos]]))}\")\n",
    "    print(f\"total num features {total_num_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SAEFeature:\n",
    "    \"\"\"Class for keeping track of an item in inventory.\"\"\"\n",
    "    layer: int\n",
    "    pos: int\n",
    "    feature_i: int\n",
    "    attr: float\n",
    "    original_attr: float\n",
    "    original_count: float\n",
    "    records: list = field(default_factory=lambda: [])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.layer) + \" \" + str(self.pos) + \" \" + str(self.feature_i) + \" \" + str(self.attr)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "cutoff = 3407 + num_always_keep\n",
    "edges_to_keep = edges[:cutoff]\n",
    "\n",
    "sae_edges = defaultdict(lambda: defaultdict(lambda: []))\n",
    "counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "position_map = {}\n",
    "toks = model.to_str_tokens(data.data[0])\n",
    "for l in range(L):\n",
    "    position_map[l] = f'pos{l}{toks[l]}'\n",
    "    \n",
    "features = []\n",
    "for edge in edges_to_keep:\n",
    "    if '.sae' in edge.output_node and not edge.label is None:\n",
    "        # [pos:feature_i]\n",
    "        label = edge.label[1:-1]\n",
    "        pos, feature_i = label.split(\":\")\n",
    "        pos = int(pos)\n",
    "        if feature_i == 'KEEP': continue # dummy edge used to ensure sae always applied\n",
    "        feature_i = int(feature_i)\n",
    "        layer = int(edge.output_node.split(\".\")[0])\n",
    "        attr = edge.score_diff_when_patched\n",
    "        features.append(SAEFeature(layer=layer, pos=pos, feature_i=feature_i, attr=attr, original_attr=edge.original_attr, original_count=edge.original_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Max Activating Examples on a large dataset\n",
    "\n",
    "(takes ~12 hours, just leave this running and come back later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/5880 [00:06<10:11:35,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 100/5880 [11:26<9:36:49,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 200/5880 [23:43<30:49:02, 19.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 300/5880 [35:12<9:08:19,  5.90s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 400/5880 [47:32<29:52:04, 19.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 500/5880 [59:04<9:06:36,  6.10s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 600/5880 [1:11:20<28:52:29, 19.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 700/5880 [1:22:53<8:38:54,  6.01s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 800/5880 [1:35:13<28:06:50, 19.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 900/5880 [1:46:46<8:21:47,  6.05s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1000/5880 [1:59:05<26:22:08, 19.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 1100/5880 [2:10:27<7:57:13,  5.99s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1200/5880 [2:25:06<30:17:18, 23.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 1300/5880 [2:38:36<7:23:45,  5.81s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1400/5880 [2:53:32<28:55:23, 23.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 1500/5880 [3:09:02<10:30:32,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 1600/5880 [3:25:00<27:36:02, 23.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 1700/5880 [3:38:30<8:03:52,  6.95s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 1800/5880 [3:53:46<27:22:23, 24.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 1900/5880 [4:09:27<9:07:58,  8.26s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 2000/5880 [4:25:55<26:18:49, 24.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 2100/5880 [4:40:26<9:06:56,  8.68s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 2200/5880 [4:57:39<24:56:28, 24.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 2300/5880 [5:13:13<8:53:34,  8.94s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 2400/5880 [5:27:41<21:20:59, 22.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 2500/5880 [5:40:57<6:22:27,  6.79s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 2600/5880 [5:55:21<20:23:42, 22.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 2700/5880 [6:08:36<6:07:08,  6.93s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 2800/5880 [6:21:53<17:22:41, 20.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 2900/5880 [6:33:21<4:52:01,  5.88s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 3000/5880 [6:45:31<15:33:30, 19.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 3100/5880 [6:57:02<4:40:51,  6.06s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 3200/5880 [7:09:30<14:56:52, 20.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 3300/5880 [7:21:14<4:26:18,  6.19s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 3400/5880 [7:33:44<13:53:21, 20.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 3500/5880 [7:45:26<3:59:19,  6.03s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 3600/5880 [7:57:57<12:52:02, 20.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 3700/5880 [8:09:40<3:43:04,  6.14s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 3800/5880 [8:22:08<11:37:01, 20.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 3900/5880 [8:33:50<3:18:24,  6.01s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 4000/5880 [8:46:12<10:27:40, 20.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 4100/5880 [8:57:45<2:57:30,  5.98s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 4200/5880 [9:09:50<9:07:38, 19.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 4300/5880 [9:21:13<2:38:11,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 4400/5880 [9:33:21<8:06:03, 19.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 4500/5880 [9:44:43<2:14:11,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 4600/5880 [9:56:54<7:03:16, 19.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 4700/5880 [10:08:19<1:56:40,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 4800/5880 [10:20:26<5:49:31, 19.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 4900/5880 [10:31:49<1:37:06,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 5000/5880 [10:43:59<4:49:05, 19.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 5100/5880 [10:55:21<1:16:48,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 5200/5880 [11:07:27<3:44:09, 19.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 5300/5880 [11:18:51<57:35,  5.96s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 5400/5880 [11:30:55<2:35:37, 19.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 5500/5880 [11:42:19<36:54,  5.83s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 5600/5880 [11:54:28<1:32:09, 19.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 5700/5880 [12:05:52<17:37,  5.87s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 5800/5880 [12:17:59<25:44, 19.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 5879/5880 [12:26:34<00:07,  7.62s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (60) must match the existing size (100) at non-singleton dimension 0.  Target sizes: [60].  Tensor sizes: [100]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 155\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f,feats \u001b[38;5;129;01min\u001b[39;00m features_sorted_by_feat_i\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    153\u001b[0m     new_modified_feats \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m feats\n\u001b[0;32m--> 155\u001b[0m \u001b[43mforward_check_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_modified_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 117\u001b[0m, in \u001b[0;36mforward_check_features\u001b[0;34m(data, features, batch_size)\u001b[0m\n\u001b[1;32m    115\u001b[0m batch_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(DATA_LEN, batch_start\u001b[38;5;241m+\u001b[39mbatch_size)\n\u001b[1;32m    116\u001b[0m data_batch \u001b[38;5;241m=\u001b[39m data[batch_start:batch_end][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][:,:\u001b[38;5;241m128\u001b[39m]\n\u001b[0;32m--> 117\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfwd_hooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast_ssm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast_conv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ind \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaving\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/mamba_lens/input_dependent_hooks.py:268\u001b[0m, in \u001b[0;36mInputDependentHookedRootModule.run_with_hooks\u001b[0;34m(self, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# just call super, we need to override this function to ensure input_dependent hooks are setup\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dependent_hooks_context(\u001b[38;5;241m*\u001b[39mmodel_args, fwd_hooks\u001b[38;5;241m=\u001b[39mfwd_hooks, bwd_hooks\u001b[38;5;241m=\u001b[39mbwd_hooks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs):\n\u001b[0;32m--> 268\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_hooks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfwd_hooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfwd_hooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbwd_hooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbwd_hooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_hooks_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_hooks_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclear_contexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclear_contexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/transformer_lens/hook_points.py:454\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_hooks\u001b[0;34m(self, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING: Hooks will be reset at the end of run_with_hooks. This removes the backward hooks before a backward pass can occur.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m     )\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts) \u001b[38;5;28;01mas\u001b[39;00m hooked_model:\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhooked_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/mamba_lens/HookedMamba.py:522\u001b[0m, in \u001b[0;36mHookedMamba.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, stop_at_layer, only_use_these_layers, fast_conv, fast_ssm, warn_disabled_hooks)\u001b[0m\n\u001b[1;32m    518\u001b[0m    only_use_these_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(start_at_layer, stop_at_layer) \n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_i \u001b[38;5;129;01min\u001b[39;00m only_use_these_layers:\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;66;03m# [B,L,D]         [B,L,D]\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m     resid     \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_i\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast_conv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfast_conv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast_ssm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfast_ssm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn_disabled_hooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarn_disabled_hooks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# we stop early, just return the resid\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stopping:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/mamba_lens/HookedMamba.py:927\u001b[0m, in \u001b[0;36mHookedMambaBlock.forward\u001b[0;34m(self, resid, fast_conv, fast_ssm, warn_disabled_hooks)\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# [B,L,D]         [E->D]   [B,L,E]\u001b[39;00m\n\u001b[1;32m    926\u001b[0m y_out     \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj( y_after_skip ) \u001b[38;5;66;03m# no bias\u001b[39;00m\n\u001b[0;32m--> 927\u001b[0m y_out     \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_out_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_out\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [B,L,D]\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;66;03m# [B,L,D]   [B,L,D]   [B,L,D]\u001b[39;00m\n\u001b[1;32m    930\u001b[0m resid     \u001b[38;5;241m=\u001b[39m resid \u001b[38;5;241m+\u001b[39m  y_out\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1547\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1547\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1550\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/transformer_lens/hook_points.py:109\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbwd\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m ):  \u001b[38;5;66;03m# For a backwards hook, module_output is a tuple of (grad,) - I don't know why.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     module_output \u001b[38;5;241m=\u001b[39m module_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 73\u001b[0m, in \u001b[0;36msae_hook\u001b[0;34m(x, hook, layer)\u001b[0m\n\u001b[1;32m     71\u001b[0m buffer[get_batched_index_into(top_indices)] \u001b[38;5;241m=\u001b[39m top_acts\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features_by_layer[layer]:\n\u001b[0;32m---> 73\u001b[0m     feature\u001b[38;5;241m.\u001b[39mrecords[records_offset:records_offset\u001b[38;5;241m+\u001b[39mB] \u001b[38;5;241m=\u001b[39m buffer[:,feature\u001b[38;5;241m.\u001b[39mpos,feature\u001b[38;5;241m.\u001b[39mfeature_i]\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# kernel can't handle doing all token positions at same time by default\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# but if we make it think B*L is a single batch index it works fine\u001b[39;00m\n\u001b[1;32m     76\u001b[0m top_acts_flattened \u001b[38;5;241m=\u001b[39m top_acts\u001b[38;5;241m.\u001b[39mflatten(start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, end_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (60) must match the existing size (100) at non-singleton dimension 0.  Target sizes: [60].  Tensor sizes: [100]"
     ]
    }
   ],
   "source": [
    "\n",
    "LAYER_STUDYING = 34\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sae.data import chunk_and_tokenize\n",
    "dataset = load_dataset(\n",
    "    \"togethercomputer/RedPajama-Data-1T-Sample\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = model.tokenizer\n",
    "# too many processes crashes, probably memory issue\n",
    "tokenized = chunk_and_tokenize(dataset, tokenizer, num_proc=8)\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "\n",
    "def get_batched_index_into(indices):\n",
    "    '''\n",
    "    given data that is [B,N,V] and indicies that are [B,N,K] with each index being an index into the V space\n",
    "    this gives you indexes you can use to access your values\n",
    "    '''\n",
    "    first_axis = []\n",
    "    second_axis = []\n",
    "    third_axis = []\n",
    "    B, _, _ = indices.size()\n",
    "    for b in range(B):\n",
    "        second, third = get_index_into(indices[b])\n",
    "        first_axis.append(torch.full(second.size(), fill_value=b, device=model.cfg.device))\n",
    "        second_axis.append(second)\n",
    "        third_axis.append(third)\n",
    "\n",
    "    return torch.cat(first_axis), torch.cat(second_axis), torch.cat(third_axis)\n",
    "\n",
    "def get_index_into(indices):\n",
    "    '''\n",
    "    given data that is [N,V] and indicies that are [N,K] with each index being an index into the V space\n",
    "    this gives you indexes you can use to access your values\n",
    "    '''\n",
    "    num_data, num_per_data = indices.size()\n",
    "    # we want\n",
    "    # [0,0,0,...,] num per data of these\n",
    "    # [1,1,1,...,] num per data of these\n",
    "    # ...\n",
    "    # [num_data-1, num_data-1, ...]\n",
    "    first_axis_index = torch.arange(num_data, dtype=torch.long).view(num_data, 1)*torch.ones([num_data, num_per_data], dtype=torch.long)\n",
    "    # now we flatten it so it has an index for each term aligned with our indices\n",
    "    first_axis_index = first_axis_index.flatten()\n",
    "    second_axis_index = indices.flatten()\n",
    "    return first_axis_index, second_axis_index\n",
    "global buffer\n",
    "buffer = None\n",
    "global features_by_layer\n",
    "def sae_hook(\n",
    "    x,\n",
    "    hook,\n",
    "    layer,\n",
    "):\n",
    "    global records_offset\n",
    "    # s is [B,L,E]\n",
    "    K = saes[layer].cfg.k\n",
    "    sae = saes[layer]\n",
    "    B,L,D = x.size()\n",
    "    uncorrupted_features = sae.encode(x)\n",
    "    top_acts, top_indices = uncorrupted_features.topk(K, sorted=False)\n",
    "    global buffer\n",
    "    if buffer is None:\n",
    "        buffer = torch.zeros(uncorrupted_features.size(), device=model.cfg.device)\n",
    "    buffer[:] = 0\n",
    "    global features_by_layer\n",
    "    # zero everything except the top k\n",
    "    buffer[get_batched_index_into(top_indices)] = top_acts.flatten()\n",
    "    for feature in features_by_layer[layer]:\n",
    "        feature.records[records_offset:records_offset+B] = buffer[:,feature.pos,feature.feature_i]\n",
    "    # kernel can't handle doing all token positions at same time by default\n",
    "    # but if we make it think B*L is a single batch index it works fine\n",
    "    top_acts_flattened = top_acts.flatten(start_dim=0, end_dim=1)\n",
    "    top_indices_flattened = top_indices.flatten(start_dim=0, end_dim=1)\n",
    "    sae_out = sae.decode(top_acts_flattened, top_indices_flattened)\n",
    "    sae_out = sae_out.unflatten(dim=0, sizes=(B,L))\n",
    "    return sae_out\n",
    "from tqdm import tqdm\n",
    "\n",
    "def forward_check_features(data, features, batch_size):\n",
    "    global records_offset\n",
    "    \n",
    "    global features_by_layer\n",
    "    \n",
    "    feat_is = set()\n",
    "    for feature in features:\n",
    "        feat_is.add(feature.feature_i)\n",
    "\n",
    "    ACCUMULATE_FREQUENCY = 40\n",
    "\n",
    "    #with open(\"layer_15_features_on_large_data.pkl\", \"rb\") as f:\n",
    "    #    features = pickle.load(f)\n",
    "    features_by_layer = defaultdict(lambda: [])\n",
    "    for feature in features:\n",
    "        feature.records = torch.zeros([batch_size*ACCUMULATE_FREQUENCY], device=model.cfg.device)\n",
    "        features_by_layer[feature.layer].append(feature)\n",
    "\n",
    "    # only bother with SAE on the layers we are checking\n",
    "    layers_to_apply_sae = sorted(list(features_by_layer.keys()))\n",
    "    hooks = [(f'blocks.{layer}.hook_out_proj', partial(sae_hook, layer=layer)) for layer in layers_to_apply_sae]\n",
    "    DATA_LEN = len(data)\n",
    "\n",
    "    global records_offset\n",
    "    global feature_offset\n",
    "    records_offset = 0\n",
    "    feature_offset = 0\n",
    "    feature_i_top_k_indices = defaultdict(lambda: torch.tensor([], device=model.cfg.device))\n",
    "    feature_i_top_k_values = defaultdict(lambda: torch.tensor([], device=model.cfg.device))\n",
    "    K = 400\n",
    "    ind = 0\n",
    "    for batch_start in tqdm(list(range(0, DATA_LEN, batch_size))):\n",
    "        batch_end = min(DATA_LEN, batch_start+batch_size)\n",
    "        data_batch = data[batch_start:batch_end]['input_ids'][:,:128]\n",
    "        _ = model.run_with_hooks(input=data_batch, fwd_hooks=hooks, fast_ssm=True, fast_conv=True)\n",
    "        if ind % 100 == 0:\n",
    "            print(\"saving\")\n",
    "            torch.save((dict(feature_i_top_k_indices), dict(feature_i_top_k_values)), \"restopk.pkl\")\n",
    "            torch.save((dict(feature_i_top_k_indices), dict(feature_i_top_k_values)), \"restopkb.pkl\")\n",
    "        ind += 1\n",
    "\n",
    "        records_offset += batch_size\n",
    "\n",
    "        if records_offset == batch_size*ACCUMULATE_FREQUENCY:\n",
    "            records_offset = 0\n",
    "            for feature in features:\n",
    "                records = feature.records\n",
    "                top = torch.topk(records, min(K, batch_size))\n",
    "                # offset indices by total num seen so far\n",
    "                top_indices = top.indices + feature_offset\n",
    "                top_values = top.values\n",
    "                merged_top_indices = torch.concatenate([feature_i_top_k_indices[feature.feature_i], top_indices])\n",
    "                merged_top_values = torch.concatenate([feature_i_top_k_values[feature.feature_i], top_values])\n",
    "                merged_top = torch.topk(merged_top_values, min(K, merged_top_indices.size()[0]))\n",
    "                feature_i_top_k_indices[feature.feature_i] = merged_top_indices[merged_top.indices]\n",
    "                feature_i_top_k_values[feature.feature_i] = merged_top_values[merged_top.indices]\n",
    "                feature.records[:] = 0\n",
    "            feature_offset += batch_size*ACCUMULATE_FREQUENCY\n",
    "\n",
    "# we need to get their outputs on all positions, so make modified features\n",
    "features_sorted_by_feat_i = defaultdict(lambda: [])\n",
    "for feature in features:\n",
    "    new_feats = []\n",
    "    for pos in range(1,128):\n",
    "        feat1 = SAEFeature(layer=LAYER_STUDYING, pos=pos, feature_i=feature.feature_i, attr=feature.attr, original_attr=feature.original_attr, original_count=feature.original_count)\n",
    "        new_feats.append(feat1)\n",
    "    features_sorted_by_feat_i[feature.feature_i] = new_feats\n",
    "\n",
    "new_modified_feats = []\n",
    "for f,feats in features_sorted_by_feat_i.items():\n",
    "    new_modified_feats += feats\n",
    "\n",
    "forward_check_features(tokenized, new_modified_feats, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move into big torch array for performance reasons\n",
    "\n",
    "Also, gather feature activation values on max activating examples for positions other than the max activating position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 100/3407 [00:10<05:53,  9.34it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"restopk.pkl\", \"rb\") as f:\n",
    "    feature_i_top_k_indices, feature_i_top_k_values = torch.load(f)\n",
    "\n",
    "all_feature_i = sorted(list(feature_i_top_k_indices.keys()))\n",
    "K = len(feature_i_top_k_values[all_feature_i[0]])\n",
    "feature_to_storage_index = dict([(feat_i,index) for (index,feat_i) in enumerate(all_feature_i)])\n",
    "\n",
    "token_data = torch.zeros(len(all_feature_i), K, 128, device=torch.device(model.cfg.device), dtype=torch.long)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import tqdm\n",
    "for feature_i in tqdm.tqdm(all_feature_i):\n",
    "    storage_index = feature_to_storage_index[feature_i]\n",
    "    indices = feature_i_top_k_indices[feature_i]\n",
    "    for k, index in enumerate(indices):\n",
    "        token_data[storage_index, k, :] = tokenized[torch.round(index).long().item()]['input_ids'][:128]\n",
    "\n",
    "        \n",
    "global features_by_layer\n",
    "def sae_hook(\n",
    "    x,\n",
    "    hook,\n",
    "    layer,\n",
    "):\n",
    "    # s is [B,L,E]\n",
    "    K = saes[layer].cfg.k\n",
    "    sae = saes[layer]\n",
    "    B,L,D = x.size()\n",
    "    uncorrupted_features = sae.encode(x)\n",
    "    top_acts, top_indices = uncorrupted_features.topk(K, sorted=False)\n",
    "    buffer = torch.zeros(uncorrupted_features.size(), device=model.cfg.device)\n",
    "    global features_by_layer\n",
    "    global records_offset\n",
    "    # zero everything except the top k\n",
    "    buffer[get_batched_index_into(top_indices)] = top_acts.flatten()\n",
    "    for feature in features_by_layer[layer]:\n",
    "            feature.records[records_offset:records_offset+B] = buffer[:,feature.pos,feature.feature_i]\n",
    "    # kernel can't handle doing all token positions at same time by default\n",
    "    # but if we make it think B*L is a single batch index it works fine\n",
    "    top_acts_flattened = top_acts.flatten(start_dim=0, end_dim=1)\n",
    "    top_indices_flattened = top_indices.flatten(start_dim=0, end_dim=1)\n",
    "    sae_out = sae.decode(top_acts_flattened, top_indices_flattened)\n",
    "    sae_out = sae_out.unflatten(dim=0, sizes=(B,L))\n",
    "    return sae_out\n",
    "\n",
    "def forward_check_single_feature(data, feature, batch_size):\n",
    "    global records_offset\n",
    "    global features_by_layer\n",
    "\n",
    "    TOTAL_SIZE = data.size()[0]\n",
    "\n",
    "    features = []\n",
    "    for pos in range(128):\n",
    "        features.append(SAEFeature(layer=feature.layer, pos=pos, feature_i=feature.feature_i, attr=feature.attr, original_attr=feature.original_attr, original_count=feature.original_count))       \n",
    "\n",
    "    features_by_layer = defaultdict(lambda: [])\n",
    "    for feat in features:\n",
    "        feat.records = torch.zeros([TOTAL_SIZE], device=model.cfg.device)\n",
    "        features_by_layer[feat.layer].append(feat)\n",
    "\n",
    "    # only bother with SAE on the layers we are checking\n",
    "    layers_to_apply_sae = sorted(list(features_by_layer.keys()))\n",
    "    hooks = [(f'blocks.{layer}.hook_out_proj', partial(sae_hook, layer=layer)) for layer in layers_to_apply_sae]\n",
    "    DATA_LEN = len(data)\n",
    "\n",
    "    global records_offset\n",
    "    records_offset = 0\n",
    "    for batch_start in list(range(0, DATA_LEN, batch_size)):\n",
    "        batch_end = min(DATA_LEN, batch_start+batch_size)\n",
    "        data_batch = data[batch_start:batch_end]\n",
    "        _ = model.run_with_hooks(input=data_batch, fwd_hooks=hooks, fast_ssm=True, fast_conv=True)\n",
    "        records_offset += batch_size\n",
    "    return features\n",
    "\n",
    "feature_act_data = torch.zeros(len(all_feature_i), K, 128, device=torch.device(model.cfg.device))\n",
    "\n",
    "for feat_i in tqdm.tqdm(all_feature_i):\n",
    "    feature = [feat for feat in features if feat.feature_i == feat_i][0]\n",
    "    top_act_tokens = token_data[feature_to_storage_index[feat_i]]\n",
    "    feature_acts = forward_check_single_feature(top_act_tokens, feature, batch_size=100)\n",
    "    for i, f in enumerate(feature_acts):\n",
    "        feature_act_data[feature_to_storage_index[feat_i], :, i] = f.records\n",
    "\n",
    "torch.save((feature_act_data, token_data, feature_to_storage_index, features), \"out_topk_data2.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Processed Data (can be ran without running the above stuff once data is generated, though you do need to load SAEs first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SAEFeature:\n",
    "    \"\"\"Class for keeping track of an item in inventory.\"\"\"\n",
    "    layer: int\n",
    "    pos: int\n",
    "    feature_i: int\n",
    "    attr: float\n",
    "    original_attr: float\n",
    "    original_count: float\n",
    "    records: list = field(default_factory=lambda: [])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.layer) + \" \" + str(self.pos) + \" \" + str(self.feature_i) + \" \" + str(self.attr)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "with open(\"out_topk_data.pkl\", \"rb\") as f:\n",
    "    (feature_act_data, token_data, feature_to_storage_index, features) = torch.load(f)\n",
    "\n",
    "for feature in features:\n",
    "    feature.records = []\n",
    "\n",
    "features_sorted_by_feat_i = defaultdict(lambda: [])\n",
    "for feature in features:\n",
    "    features_sorted_by_feat_i[feature.feature_i].append(feature)\n",
    "\n",
    "del feature_act_data\n",
    "del token_data\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually test Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3751f474184ae9be8b6d319cec7490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', continuous_update=False, description='Test String', placeholder='Test String')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f87dbdb6e0348e2aa2a6e559a8f0578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', continuous_update=False, description='Feature Index', placeholder='Feature Index')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2bac867fc341af8582ac00498f6adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', continuous_update=False, description='Eval Tokens', placeholder='Eval Tokens')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5efb08b9fdd4eeabe93e8e3e08b832f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Test', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d36185073c5497193ab977cbcbbef00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "global features_by_layer\n",
    "def sae_hook(\n",
    "    x,\n",
    "    hook,\n",
    "    layer,\n",
    "):\n",
    "    # s is [B,L,E]\n",
    "    K = saes[layer].cfg.k\n",
    "    sae = saes[layer]\n",
    "    B,L,D = x.size()\n",
    "    uncorrupted_features = sae.encode(x)\n",
    "    top_acts, top_indices = uncorrupted_features.topk(K, sorted=False)\n",
    "    buffer = torch.zeros(uncorrupted_features.size(), device=model.cfg.device)\n",
    "    global features_by_layer\n",
    "    # zero everything except the top k\n",
    "    buffer[get_batched_index_into(top_indices)] = top_acts.flatten()\n",
    "    for feature in features_by_layer[layer]:\n",
    "        if feature.pos < L: # sometimes prompt is too small to consider this feature\n",
    "            feature.records += buffer[:,feature.pos,feature.feature_i].tolist()\n",
    "    # kernel can't handle doing all token positions at same time by default\n",
    "    # but if we make it think B*L is a single batch index it works fine\n",
    "    top_acts_flattened = top_acts.flatten(start_dim=0, end_dim=1)\n",
    "    top_indices_flattened = top_indices.flatten(start_dim=0, end_dim=1)\n",
    "    sae_out = sae.decode(top_acts_flattened, top_indices_flattened)\n",
    "    sae_out = sae_out.unflatten(dim=0, sizes=(B,L))\n",
    "    return sae_out\n",
    "\n",
    "def forward_check_featuresf(data, features):\n",
    "    \n",
    "    global features_by_layer\n",
    "    \n",
    "    #with open(\"layer_15_features_on_large_data.pkl\", \"rb\") as f:\n",
    "    #    features = pickle.load(f)\n",
    "    features_by_layer = defaultdict(lambda: [])\n",
    "    for feature in features:\n",
    "        feature.records = []\n",
    "        features_by_layer[feature.layer].append(feature)\n",
    "\n",
    "    # only bother with SAE on the layers we are checking\n",
    "    layers_to_apply_sae = sorted(list(features_by_layer.keys()))\n",
    "    hooks = [(f'blocks.{layer}.hook_out_proj', partial(sae_hook, layer=layer)) for layer in layers_to_apply_sae]\n",
    "    _ = model.run_with_hooks(input=data, fwd_hooks=hooks, fast_ssm=True, fast_conv=True)\n",
    "        \n",
    "def clicked(arg):\n",
    "    with outputTesting:\n",
    "        clear_output()\n",
    "        try:\n",
    "            if eval_data.value.strip() == \"\":\n",
    "                text = text_itemw.value\n",
    "                tokenized_input = torch.tensor([model.tokenizer.bos_token_id] + model.tokenizer.encode(text), device=model.cfg.device).reshape(1,-1)\n",
    "            else:\n",
    "                tokenized_input = eval(eval_data.value).reshape(1, -1)\n",
    "                print(\"eval to\", tokenized_input)\n",
    "            feature_i = int(feature_index.value)\n",
    "            L = tokenized_input.size()[1]\n",
    "            for feature in features:\n",
    "                if feature.feature_i == feature_i:\n",
    "                    feat = feature\n",
    "            features_with_i = []\n",
    "            for i in range(1, L):\n",
    "                features_with_i.append(SAEFeature(layer=feat.layer, pos=i, feature_i=feature_i, attr=feat.attr, original_attr=feat.original_attr, original_count=feat.original_count))\n",
    "            forward_check_featuresf(tokenized_input, features=features_with_i)\n",
    "            activations = torch.zeros(L)\n",
    "            for feature in features_with_i:\n",
    "                activations[feature.pos] = feature.records[0]\n",
    "            toks = model.to_str_tokens(tokenized_input[0])\n",
    "            print(toks)\n",
    "            token_pos = torch.argmax(activations).item()\n",
    "            \n",
    "            if activations[token_pos] == 0.0:\n",
    "                token_pos = L\n",
    "            out_toks = []\n",
    "            print(activations)\n",
    "            for j,tok in enumerate(toks):\n",
    "                if len(tok.strip()) == 0:\n",
    "                    tok = repr(tok)\n",
    "                tok = tok.replace(\"\\n\", \"\\\\n\")\n",
    "                colored = f\"<span id='ayy'><font color='white'>{tok}</font></span>\"\n",
    "                if j < 1: continue\n",
    "                if j == token_pos:\n",
    "                    colored = f\"<span id='ayy'><font color='red'>{tok}</font></span>\"\n",
    "                elif activations[j].item() > 0.01:\n",
    "                    colored = f\"<span id='ayy'><font color='pink'>{tok}</font></span>\"\n",
    "                if activations[j].item() > 0.01:\n",
    "                    out_toks.append(f\"{colored}{activations[j].item():.3f}\")\n",
    "                else:\n",
    "                    out_toks.append(colored)\n",
    "            simpler = model.tokenizer.decode(tokenized_input[0,1:token_pos+1])\n",
    "            print(simpler)\n",
    "            if token_pos == L:\n",
    "                print(\"all zero\")\n",
    "            else:\n",
    "                display(HTML(toks[token_pos] + \"\\t<br/><br/>\\t\" + \"<span id='ayy'>\" + simpler + \"</span>\\t<br/><br/>\\t\" + \"\".join(out_toks))) \n",
    "        except:\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "                \n",
    "text_itemw = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Test String',\n",
    "    description='Test String',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    ")\n",
    "\n",
    "feature_index = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Feature Index',\n",
    "    description='Feature Index',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    ")\n",
    "eval_data = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Eval Tokens',\n",
    "    description='Eval Tokens',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    ")\n",
    "\n",
    "test_button = widgets.Button(description = 'Test')   \n",
    "test_button.on_click(clicked)\n",
    "\n",
    "outputTesting = widgets.Output()\n",
    "\n",
    "display(text_itemw)\n",
    "display(feature_index)\n",
    "display(eval_data)\n",
    "display(test_button)\n",
    "display(outputTesting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(feature_act_data[feature_to_storage_index[2613]][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|> Electronrising inaccur invisible,..., proteins Electronrising inaccur invisible,..., proteins Electronrising inaccur invisible,...,'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(['<|endoftext|>', ' Electron', 'rising', ' inaccur', ' invisible', ',...,', ' proteins', ' Electron', 'rising', ' inaccur', ' invisible', ',...,', ' proteins', ' Electron', 'rising', ' inaccur', ' invisible', ',...,'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"restopk.pkl\", \"rb\") as f:\n",
    "    feature_i_top_k_indices, feature_i_top_k_values = torch.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# token_data[feature_to_storage_index[2613],0]\n",
    "# tokenized[int(feature_i_top_k_indices[2613][0].item())]['input_ids']\n",
    "from datasets import load_dataset\n",
    "from sae.data import chunk_and_tokenize\n",
    "dataset = load_dataset(\n",
    "    \"togethercomputer/RedPajama-Data-1T-Sample\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = model.tokenizer\n",
    "# too many processes crashes, probably memory issue\n",
    "tokenized = chunk_and_tokenize(dataset, tokenizer, num_proc=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  309,   858,   690,  ...,    13, 14635,  9137])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[int(feature_i_top_k_indices[2613][0].item())]['input_ids']\n",
    "tokenized[int(feature_i_top_k_indices[2613][0].item())]['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect TopK and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "/*overwrite hard coded write background by vscode for ipywidges */\n",
       ".cell-output-ipywidget-background {\n",
       "   background-color: transparent !important;\n",
       "}\n",
       "\n",
       "/*set widget foreground text and color of interactive widget to vs dark theme color */\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "/*overwrite hard coded write background by vscode for ipywidges */\n",
    ".cell-output-ipywidget-background {\n",
    "   background-color: transparent !important;\n",
    "}\n",
    "\n",
    "/*set widget foreground text and color of interactive widget to vs dark theme color */\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no feature labels saved atfeature_labels_copy.pkl starting, fresh\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<link href='https://fonts.googleapis.com/css?family=Noto Sans' rel='stylesheet'>\n",
       "<style>\n",
       "/* Base Noto Sans and Serif for Latin, Greek, and Cyrillic */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans:wght@400;700&family=Noto+Serif:wght@400;700&display=swap');\n",
       "\n",
       "/* East Asian scripts */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@400;700&family=Noto+Sans+KR:wght@400;700&family=Noto+Sans+SC:wght@400;700&family=Noto+Sans+TC:wght@400;700&display=swap');\n",
       "\n",
       "/* South Asian scripts */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Devanagari:wght@400;700&family=Noto+Sans+Bengali:wght@400;700&family=Noto+Sans+Tamil:wght@400;700&display=swap');\n",
       "\n",
       "/* Middle Eastern scripts */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Arabic:wght@400;700&family=Noto+Sans+Hebrew:wght@400;700&display=swap');\n",
       "\n",
       "/* Other scripts */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Thai:wght@400;700&family=Noto+Sans+Ethiopic:wght@400;700&display=swap');\n",
       "\n",
       "/* Specialty fonts */\n",
       "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Mono:wght@400;700&family=Noto+Color+Emoji&display=swap');\n",
       "\n",
       "#ayy {\n",
       "  font-family: 'Noto Sans', 'Noto Sans JP', 'Noto Sans KR', 'Noto Sans SC', 'Noto Sans TC', \n",
       "               'Noto Sans Devanagari', 'Noto Sans Bengali', 'Noto Sans Tamil', \n",
       "               'Noto Sans Arabic', 'Noto Sans Hebrew', 'Noto Sans Thai', 'Noto Sans Ethiopic',\n",
       "               sans-serif;\n",
       "}\n",
       "\n",
       "/* Language-specific rules */\n",
       ":lang(ja) { font-family: 'Noto Sans JP', sans-serif; }\n",
       ":lang(ko) { font-family: 'Noto Sans KR', sans-serif; }\n",
       ":lang(zh-CN) { font-family: 'Noto Sans SC', sans-serif; }\n",
       ":lang(zh-TW) { font-family: 'Noto Sans TC', sans-serif; }\n",
       ":lang(hi) { font-family: 'Noto Sans Devanagari', sans-serif; }\n",
       ":lang(bn) { font-family: 'Noto Sans Bengali', sans-serif; }\n",
       ":lang(ta) { font-family: 'Noto Sans Tamil', sans-serif; }\n",
       ":lang(ar) { font-family: 'Noto Sans Arabic', sans-serif; }\n",
       ":lang(he) { font-family: 'Noto Sans Hebrew', sans-serif; }\n",
       ":lang(th) { font-family: 'Noto Sans Thai', sans-serif; }\n",
       ":lang(am), :lang(ti) { font-family: 'Noto Sans Ethiopic', sans-serif; }\n",
       "\n",
       "/* Emoji support */\n",
       ".emoji {\n",
       "  font-family: 'Noto Color Emoji', sans-serif;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41ffa4ba67849d0b6c110aa9e17253f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='ffff', continuous_update=False, description='String:', placeholder='Type something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e6b75b138344c895e3540a8a87c769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "feature_labels_path = 'feature_labels_copy.pkl'\n",
    "if not os.path.exists(feature_labels_path):\n",
    "    print(\"no feature labels saved at\" + feature_labels_path + \" starting, fresh\")\n",
    "    feature_labels = {}\n",
    "else:\n",
    "    with open(\"feature_labels_copy.pkl\", \"rb\") as f:\n",
    "        feature_labels = pickle.load(f)\n",
    "\n",
    "top_k_data = feature_act_data\n",
    "global cur_feature_ind\n",
    "cur_feature_ind = None\n",
    "def display_unlabeled_feature():\n",
    "    global cur_feature_ind\n",
    "    global feature_labels\n",
    "    #cur_feature_ind = 15112\n",
    "    #text_item.value = feature_labels[cur_feature_ind]\n",
    "    #display_feat(cur_feature_ind)\n",
    "    available_features = features_sorted_by_feat_i.keys() - feature_labels.keys()\n",
    "    maybe = []\n",
    "    # sort by attr (they are negative, so this is largest first)\n",
    "    available_features = sorted(list(available_features), key=lambda feat_i: -max([feat.original_count for feat in features_sorted_by_feat_i[feat_i]]))\n",
    "    for f in available_features:\n",
    "        print(f\"feature {f}\")\n",
    "        for feat in features_sorted_by_feat_i[f]:\n",
    "            print(f\"pos {feat.pos} has attr {feat.original_attr} and count {feat.original_count}/{400*5}\")\n",
    "        cur_feature_ind = f\n",
    "        if f in feature_labels:\n",
    "            text_item.value = feature_labels[f]\n",
    "        else:\n",
    "            text_item.value = \"\"\n",
    "        display_feat(cur_feature_ind)\n",
    "        return\n",
    "\n",
    "# provide fonts for rendering characters in other languages\n",
    "display(HTML(\"\"\"<link href='https://fonts.googleapis.com/css?family=Noto Sans' rel='stylesheet'>\n",
    "<style>\n",
    "/* Base Noto Sans and Serif for Latin, Greek, and Cyrillic */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans:wght@400;700&family=Noto+Serif:wght@400;700&display=swap');\n",
    "\n",
    "/* East Asian scripts */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@400;700&family=Noto+Sans+KR:wght@400;700&family=Noto+Sans+SC:wght@400;700&family=Noto+Sans+TC:wght@400;700&display=swap');\n",
    "\n",
    "/* South Asian scripts */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Devanagari:wght@400;700&family=Noto+Sans+Bengali:wght@400;700&family=Noto+Sans+Tamil:wght@400;700&display=swap');\n",
    "\n",
    "/* Middle Eastern scripts */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Arabic:wght@400;700&family=Noto+Sans+Hebrew:wght@400;700&display=swap');\n",
    "\n",
    "/* Other scripts */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Thai:wght@400;700&family=Noto+Sans+Ethiopic:wght@400;700&display=swap');\n",
    "\n",
    "/* Specialty fonts */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Mono:wght@400;700&family=Noto+Color+Emoji&display=swap');\n",
    "\n",
    "#ayy {\n",
    "  font-family: 'Noto Sans', 'Noto Sans JP', 'Noto Sans KR', 'Noto Sans SC', 'Noto Sans TC', \n",
    "               'Noto Sans Devanagari', 'Noto Sans Bengali', 'Noto Sans Tamil', \n",
    "               'Noto Sans Arabic', 'Noto Sans Hebrew', 'Noto Sans Thai', 'Noto Sans Ethiopic',\n",
    "               sans-serif;\n",
    "}\n",
    "\n",
    "/* Language-specific rules */\n",
    ":lang(ja) { font-family: 'Noto Sans JP', sans-serif; }\n",
    ":lang(ko) { font-family: 'Noto Sans KR', sans-serif; }\n",
    ":lang(zh-CN) { font-family: 'Noto Sans SC', sans-serif; }\n",
    ":lang(zh-TW) { font-family: 'Noto Sans TC', sans-serif; }\n",
    ":lang(hi) { font-family: 'Noto Sans Devanagari', sans-serif; }\n",
    ":lang(bn) { font-family: 'Noto Sans Bengali', sans-serif; }\n",
    ":lang(ta) { font-family: 'Noto Sans Tamil', sans-serif; }\n",
    ":lang(ar) { font-family: 'Noto Sans Arabic', sans-serif; }\n",
    ":lang(he) { font-family: 'Noto Sans Hebrew', sans-serif; }\n",
    ":lang(th) { font-family: 'Noto Sans Thai', sans-serif; }\n",
    ":lang(am), :lang(ti) { font-family: 'Noto Sans Ethiopic', sans-serif; }\n",
    "\n",
    "/* Emoji support */\n",
    ".emoji {\n",
    "  font-family: 'Noto Color Emoji', sans-serif;\n",
    "}\n",
    "</style>\"\"\"))\n",
    "\n",
    "\n",
    "def display_feat(feature_i):\n",
    "    covered_already = set()\n",
    "    simpler_words = []\n",
    "    for k in range(100):\n",
    "        storage_index = feature_to_storage_index[feature_i]\n",
    "        activations = top_k_data[storage_index, k]\n",
    "        tokens = token_data[storage_index, k]\n",
    "        token_pos = torch.argmax(activations).item()\n",
    "        toks = model.to_str_tokens(tokens)\n",
    "        relevant_str = \"\".join(toks[:token_pos+1])\n",
    "        if relevant_str in covered_already:\n",
    "            continue\n",
    "        print(activations)\n",
    "        covered_already.add(relevant_str)\n",
    "        out_toks = []\n",
    "        colors = [''] + ['red', 'orange', 'yellow', 'green']*256\n",
    "        for j,tok in enumerate(toks):\n",
    "            if len(tok.strip()) == 0:\n",
    "                tok = repr(tok)\n",
    "            tok = tok.replace(\"\\n\", \"\\\\n\")\n",
    "            colored = f\"<span id='ayy'><font color='white'>{tok}</font></span>\"\n",
    "            if j < 1: continue\n",
    "            if j == token_pos:\n",
    "                colored = f\"<span id='ayy'><font color='red'>{tok}</font></span>\"\n",
    "            elif activations[j].item() > 0.01:\n",
    "                colored = f\"<span id='ayy'><font color='pink'>{tok}</font></span>\"\n",
    "            if activations[j].item() > 0.01:\n",
    "                out_toks.append(f\"{colored}{activations[j].item():.3f}\")\n",
    "            else:\n",
    "                out_toks.append(colored)\n",
    "        simpler = model.tokenizer.decode(tokens[1:token_pos+1])\n",
    "        simpler_words.append(simpler.strip())\n",
    "\n",
    "        display(HTML(toks[token_pos] + \"\\t||\\t\" + \"<span id='ayy'>\" + simpler + \"</span>\\t||\\t\" + \"\".join(out_toks)))\n",
    "        '''\n",
    "        if len(simpler_words) == 20:\n",
    "            out_s = \"<span id='ayy'>What do \"\n",
    "            for s in simpler_words:\n",
    "                out_s += f'\"{s.strip()}\", '\n",
    "            out_s += \"have in common? Take a deep breath and think step by step.\"\n",
    "            display(HTML(out_s + \"</span>\"))\n",
    "        '''\n",
    "\n",
    "def save_labels():\n",
    "    with open(\"feature_labels_copy.pkl\", \"wb\") as f:\n",
    "        global feature_labelsb\n",
    "        pickle.dump(feature_labelsb, f)\n",
    "        print(f\"done saving {len(feature_labelsb)}\")\n",
    "\n",
    "import traceback\n",
    "out = widgets.Output()\n",
    "\n",
    "def submitted(change):\n",
    "    global cur_feature_ind\n",
    "    global feature_labels\n",
    "    if len(text_item.value.strip()) > 0 and (text_item.value != feature_labels[cur_feature_ind]):\n",
    "        with out:\n",
    "            try:\n",
    "                times = 0\n",
    "                clear_output()\n",
    "                feature_labelsb[cur_feature_ind] = text_item.value\n",
    "                save_labels()\n",
    "                display_unlabeled_feature()\n",
    "            except:\n",
    "                print(traceback.format_exc())\n",
    "\n",
    "text_item = widgets.Text(\n",
    "    value='ffff',\n",
    "    placeholder='Type something',\n",
    "    description='String:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    ")\n",
    "\n",
    "display(text_item)\n",
    "display(out)\n",
    "text_item.observe(submitted, names='value')\n",
    "\n",
    "with out:\n",
    "    display_unlabeled_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Away \"no context\" features\n",
    "\n",
    "I found on IOI that many of the found features don't actually care about the context, and are just used to encode the current token. While useful, these aren't what we are looking for so we can discard them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "global buffer\n",
    "buffer = None\n",
    "def sae_hook(\n",
    "    x,\n",
    "    hook,\n",
    "    layer,\n",
    "):\n",
    "    # s is [B,L,E]\n",
    "    K = saes[layer].cfg.k\n",
    "    sae = saes[layer]\n",
    "    B,L,D = x.size()\n",
    "    uncorrupted_features = sae.encode(x)\n",
    "    top_acts, top_indices = uncorrupted_features.topk(K, sorted=False)\n",
    "    buffer = torch.zeros(uncorrupted_features.size(), device=model.cfg.device)\n",
    "    global features_by_layer\n",
    "    # zero everything except the top k\n",
    "    buffer[get_batched_index_into(top_indices)] = top_acts.flatten()\n",
    "    for feature in features_by_layer[layer]:\n",
    "        if feature.pos < L: # sometimes prompt is too small to consider this feature\n",
    "            feature.records += buffer[:,feature.pos,feature.feature_i].tolist()\n",
    "            feature.full_records += uncorrupted_features[:,feature.pos,feature.feature_i].tolist()\n",
    "    # kernel can't handle doing all token positions at same time by default\n",
    "    # but if we make it think B*L is a single batch index it works fine\n",
    "    top_acts_flattened = top_acts.flatten(start_dim=0, end_dim=1)\n",
    "    top_indices_flattened = top_indices.flatten(start_dim=0, end_dim=1)\n",
    "    sae_out = sae.decode(top_acts_flattened, top_indices_flattened)\n",
    "    sae_out = sae_out.unflatten(dim=0, sizes=(B,L))\n",
    "    return sae_out\n",
    "\n",
    "prompt = model.tokenizer.decode(data.data[0][1:])\n",
    "\n",
    "prompt_tokens = model.to_tokens(prompt)\n",
    "print(model.to_str_tokens(prompt_tokens))\n",
    "logits_dla, activations_dla = model.run_with_cache(prompt_tokens)\n",
    "\n",
    "features = \n",
    "\n",
    "features_sorted_by_feat_i = defaultdict(lambda: [])\n",
    "for feature in features:\n",
    "    features_sorted_by_feat_i[feature.feature_i].append(feature)\n",
    "\n",
    "\n",
    "global features_by_layer\n",
    "features_by_layer = defaultdict(lambda: [])\n",
    "layers_to_apply_sae = [15]\n",
    "feats = []\n",
    "L = prompt_tokens.size()[1]\n",
    "for layer in layers_to_apply_sae:\n",
    "    for f, feats in features_sorted_by_feat_i.items():\n",
    "        if len(feats) == 0:\n",
    "            print(\"huh zero\", f, feats)\n",
    "            continue\n",
    "        feat = feats[0]\n",
    "        if feat.layer == layer:\n",
    "            for i in range(1, L):\n",
    "                features_by_layer[layer].append(SAEFeature(layer=feat.layer, pos=i, feature_i=feat.feature_i, attr=feat.attr))\n",
    "                features_by_layer[layer][-1].records = []\n",
    "                features_by_layer[layer][-1].full_records = []\n",
    "hooks = [(f'blocks.{layer}.hook_out_proj', partial(sae_hook, layer=layer)) for layer in layers_to_apply_sae]\n",
    "_ = model.run_with_hooks(input=prompt_tokens_dla, fwd_hooks=hooks, fast_ssm=True, fast_conv=True)\n",
    "\n",
    "\n",
    "for layer in layers_to_apply_sae:\n",
    "    # [B,L,E]\n",
    "    x = activations_dla[f'blocks.{layer}.hook_ssm_input']\n",
    "    # [B,L,E]\n",
    "    y = activations_dla[f'blocks.{layer}.hook_y']\n",
    "    # [B,L,E,N]\n",
    "    A_bar = activations_dla[f'blocks.{layer}.hook_A_bar']\n",
    "    # [B,L,E,N]\n",
    "    B_bar = activations_dla[f'blocks.{layer}.hook_B_bar']\n",
    "    # [B,L,E]\n",
    "    skip = F.silu(activations_dla[f'blocks.{layer}.hook_skip'])\n",
    "    # [B,L,N]\n",
    "    C = activations_dla[f'blocks.{layer}.hook_C']\n",
    "    # [D,E]\n",
    "    W_out = model.blocks[layer].out_proj\n",
    "    # [E]\n",
    "    W_D = model.blocks[layer].W_D\n",
    "    \n",
    "    \n",
    "    # [B,L,E,N]\n",
    "    #h = torch.zeros([B,L,E,N], device=model.cfg.device)\n",
    "    ys = []\n",
    "    h = torch.zeros([B,E,N], device=model.cfg.device)\n",
    "    for l in range(L):\n",
    "        if l < 1:\n",
    "            # [B,E,N]   [B,E,N]     [B,E,N]          [B,E,N]          [B,E]\n",
    "            h        =    h    *  A_bar[:,l,:,:]  + B_bar[:,l,:,:] * x[:,l].view(B, E, 1) \n",
    "            h_0 = h\n",
    "        else:       \n",
    "            # [B,E,N]  [B,E,N]      [B,E,N] \n",
    "            h_0       =  h_0   *  A_bar[:,l,:,:] # do the A_bar multiply but ignore other x's\n",
    "            # [B,E,N]   [B,E,N]     [B,E,N]          [B,E,N]          [B,E]\n",
    "            h        =   h_0  + B_bar[:,l,:,:] * x[:,l].view(B, E, 1)\n",
    "        # [B,E]    [B,E,N]       [B,N,1]   # this is like [E,N] x [N,1] for each batch\n",
    "\n",
    "        y_l       =   h     @   C[:,l,:].view(B,N,1)\n",
    "        # [B,E]              [B,E,1]\n",
    "        y_l      =    y_l.view(B,E)\n",
    "        ys.append(y_l)\n",
    "    y = torch.stack(ys, dim=1)\n",
    "    y_out         =     y +   x    *  W_D\n",
    "    # [B,L,E]       [B,L,E]   [B,L,E]\n",
    "    y_out         = y_out   *   skip\n",
    "    # [B,L,D]       [B,L,E] x    [E,D]\n",
    "    y_out         = y_out   @ W_out.weight.T\n",
    "\n",
    "    sae = saes[layer]\n",
    "    # F is feature size\n",
    "    # [B,L,F]         [B,L,D]       [D]                [D,F]\n",
    "    K = sae.cfg.k\n",
    "    sae_vals = sae.encode(y_out)\n",
    "    top_acts, top_indices = sae_vals.topk(K, sorted=False)\n",
    "    buffer = torch.zeros(sae_vals.size(), device=model.cfg.device)\n",
    "    # zero everything except the top k\n",
    "    buffer[get_batched_index_into(top_indices)] = top_acts.flatten()\n",
    "    #sae_vals      = relu((y_out - sae.b_dec) @ sae.encoder.weight.T)\n",
    "    tests = 200\n",
    "    t = 0\n",
    "    for feat in sorted(list(features_by_layer[layer]), key=lambda f:  -abs(f.records[0] - buffer[0,f.pos,f.feature_i])):\n",
    "        diff = buffer[0,feat.pos,feat.feature_i] - feat.records[0]\n",
    "        if feat.pos == L-1:\n",
    "            if abs(diff) > 0.001:\n",
    "                print(diff.item(), feat.records[0], '->', buffer[0,l,feat.feature_i].item())\n",
    "                print(feat.feature_i, feat.attr, dict(labs)[feat.feature_i])\n",
    "                t += 1\n",
    "        if t >= 200: break\n",
    "    if t >= 200: break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
