{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook with Example Config for Different Models / Hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gelu-2L\n",
    "\n",
    "An example of a toy language model we're able to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gelu-2l\",\n",
    "    hook_point = \"blocks.0.hook_mlp_out\",\n",
    "    hook_point_layer = 0,\n",
    "    d_in = 512,\n",
    "    dataset_path = \"NeelNanda/c4-tokenized-2b\",\n",
    "    is_dataset_tokenized=True,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 32,\n",
    "    b_dec_init_method=\"mean\", # geometric median is better but slower to get started\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 0.0012,\n",
    "    lr_scheduler_name=\"constantwithwarmup\",\n",
    "    l1_coefficient = 0.00016,\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 128,\n",
    "    total_training_tokens = 1_000_000 * 100, \n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    use_ghost_grads=True,\n",
    "    feature_sampling_window = 5000,\n",
    "    dead_feature_window=5000,\n",
    "    dead_feature_threshold = 1e-4,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_language_models_gelu_2l_test\",\n",
    "    wandb_log_frequency=10,\n",
    "    \n",
    "    # Misc\n",
    "    device = device,\n",
    "    seed = 42,\n",
    "    n_checkpoints = 0,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 - Small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 24576-L1-8e-05-LR-0.0004-Tokens-3.000e+08\n",
      "n_tokens_per_buffer (millions): 0.524288\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.004096\n",
      "Total training steps: 73242\n",
      "Total wandb updates: 732\n",
      "n_tokens_per_feature_sampling_window (millions): 1310.72\n",
      "n_tokens_per_dead_feature_window (millions): 2621.44\n",
      "Using Ghost Grads.\n",
      "We will reset the sparsity calculation 29 times.\n",
      "Number tokens in sparsity calculation window: 1.02e+07\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "Dataset is not tokenized! Updating config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjbloom\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/mats_sae_training/scripts/wandb/run-20240208_045920-g3eank3s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/mats_sae_training_language_models_resid_pre_test/runs/g3eank3s' target=\"_blank\">24576-L1-8e-05-LR-0.0004-Tokens-3.000e+08</a></strong> to <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_resid_pre_test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_resid_pre_test' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_resid_pre_test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_resid_pre_test/runs/g3eank3s' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_resid_pre_test/runs/g3eank3s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitializing b_dec with mean of activations\n",
      "Previous distances: 60.95991516113281\n",
      "New distances: 45.077816009521484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7324| MSE Loss 0.005 | L1 0.008:  10%|▉         | 29999104/300000000 [14:15<1:13:03, 61600.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/ujwszwqz/30003200_sparse_autoencoder_gpt2-small_blocks.3.hook_resid_pre_24576.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14648| MSE Loss 0.004 | L1 0.008:  20%|█▉        | 59998208/300000000 [28:29<1:04:20, 62165.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/ujwszwqz/60002304_sparse_autoencoder_gpt2-small_blocks.3.hook_resid_pre_24576.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21972| MSE Loss 0.003 | L1 0.007:  30%|██▉       | 89997312/300000000 [42:57<1:11:09, 49187.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/ujwszwqz/90001408_sparse_autoencoder_gpt2-small_blocks.3.hook_resid_pre_24576.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29296| MSE Loss 0.004 | L1 0.007:  40%|███▉      | 119996416/300000000 [57:12<45:55, 65317.10it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/ujwszwqz/120000512_sparse_autoencoder_gpt2-small_blocks.3.hook_resid_pre_24576.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36621| MSE Loss 0.004 | L1 0.006:  50%|█████     | 150003712/300000000 [1:11:24<1:14:23, 33604.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/ujwszwqz/150003712_sparse_autoencoder_gpt2-small_blocks.3.hook_resid_pre_24576.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43945| MSE Loss 0.004 | L1 0.007:  60%|██████    | 180002816/300000000 [1:25:33<30:40, 65185.74it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/ujwszwqz/180002816_sparse_autoencoder_gpt2-small_blocks.3.hook_resid_pre_24576.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51269| MSE Loss 0.004 | L1 0.006:  70%|███████   | 210001920/300000000 [1:39:46<1:59:44, 12526.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/ujwszwqz/210001920_sparse_autoencoder_gpt2-small_blocks.3.hook_resid_pre_24576.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58593| MSE Loss 0.003 | L1 0.007:  80%|████████  | 240001024/300000000 [1:54:11<15:36, 64077.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/ujwszwqz/240001024_sparse_autoencoder_gpt2-small_blocks.3.hook_resid_pre_24576.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65917| MSE Loss 0.004 | L1 0.006:  90%|█████████ | 270000128/300000000 [2:08:34<07:46, 64296.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/ujwszwqz/270000128_sparse_autoencoder_gpt2-small_blocks.3.hook_resid_pre_24576.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73242| MSE Loss 0.003 | L1 0.006: 100%|█████████▉| 299999232/300000000 [2:22:58<00:00, 60141.22it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/ujwszwqz/final_sparse_autoencoder_gpt2-small_blocks.3.hook_resid_pre_24576.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73242| MSE Loss 0.003 | L1 0.006: : 300003328it [2:22:59, 34967.51it/s]                             \n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b10268ab1e543d99762fb4b43f81303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='1432.227 MB of 1441.999 MB uploaded\\r'), FloatProgress(value=0.9932229555125918, m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>▁▄▆█████████████████████████████████████</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>losses/ghost_grad_loss</td><td>▁▁▁▁██▁▁▇▁▁▇▁▁▇▁▇▇▁▁▇▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/mse_loss</td><td>█▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▂▂▂▂▂▁▁▂▁▁▂▁▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/CE_loss_score</td><td>▁▄▇█████████████████████████████████████</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>▂▆▆▆▇▇▆▅█▅▄▃▆▅▁▅▄▆█▅▆▅▅▆▇▄▆▅▃▄▁▆▇▅▅▇▅▄▄▃</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>█▆▃▃▁▃▃▃▃▃▂▂▂▃▃▂▃▃▄▂▃▃▃▃▃▂▄▂▂▂▂▃▃▃▅▄▃▃▁▂</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>▄▄▄▅▁▆▅▅▅▅▃▃▄▄▅▃▅▅▇▃▅▆▅▅▅▄▆▄▄▄▄▄▅▅█▆▅▄▁▂</td></tr><tr><td>metrics/explained_variance</td><td>▁▄▆▇▇▇▇▇▇▇████████████▇█████████████████</td></tr><tr><td>metrics/explained_variance_std</td><td>█▅▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l0</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l2_norm</td><td>▁▄▆▇█▇█▇█▇▇▇█▇▇█▇████▇██████▇█▇██▇▇█████</td></tr><tr><td>metrics/l2_ratio</td><td>▁▁▅▇▇▇████████▇███▇████████████▇▇████▇██</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>█▅▂▃▂▂▃▂▂▁▂▁▂▁▂▁▂▁▂▁▂▁▂▁▂▁▂▁▂</td></tr><tr><td>sparsity/below_1e-5</td><td>▁▁▅▅▇▇▅█▇█▆█▆█▇▇▇▇▇█▇█▇█▇█▇█▇</td></tr><tr><td>sparsity/below_1e-6</td><td>▁▁▅▃▇▆▃▆▄▇▅█▆▇▆▇▆█▆█▆█▆█▆█▆█▆</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▁▃▆▁▁█▁▁▃▁▁▆▁▃▃▁▁▆▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▁▁▂▆▂▃█▄▂▄▄▂▄▂▃▄▂▄▄▃▄▄▃▄▂▃▃▂▄▃▂▄▂▃▅▂▄▇▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>0.0004</td></tr><tr><td>details/n_training_tokens</td><td>299827200</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.0</td></tr><tr><td>losses/l1_loss</td><td>80.20925</td></tr><tr><td>losses/mse_loss</td><td>0.00356</td></tr><tr><td>losses/overall_loss</td><td>0.00997</td></tr><tr><td>metrics/CE_loss_score</td><td>0.98674</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>13.33905</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>3.44866</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>3.31572</td></tr><tr><td>metrics/explained_variance</td><td>0.94097</td></tr><tr><td>metrics/explained_variance_std</td><td>0.03523</td></tr><tr><td>metrics/l0</td><td>30.15112</td></tr><tr><td>metrics/l2_norm</td><td>78.60121</td></tr><tr><td>metrics/l2_ratio</td><td>0.96074</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-4.23483</td></tr><tr><td>sparsity/below_1e-5</td><td>7163</td></tr><tr><td>sparsity/below_1e-6</td><td>5444</td></tr><tr><td>sparsity/dead_features</td><td>0</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>190.36679</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">24576-L1-8e-05-LR-0.0004-Tokens-3.000e+08</strong> at: <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_resid_pre_test/runs/g3eank3s' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_resid_pre_test/runs/g3eank3s</a><br/>Synced 7 W&B file(s), 0 media file(s), 20 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240208_045920-g3eank3s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "layer = 3\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gpt2-small\",\n",
    "    hook_point = f\"blocks.{layer}.hook_resid_pre\",\n",
    "    hook_point_layer = layer,\n",
    "    d_in = 768,\n",
    "    dataset_path = \"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 32, # determines the dimension of the SAE.\n",
    "    b_dec_init_method = \"mean\", # geometric median is better but slower to get started\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 0.0004,\n",
    "    l1_coefficient = 0.00008,\n",
    "    lr_scheduler_name=\"constantwithwarmup\",\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    lr_warm_up_steps=5000,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 128,\n",
    "    total_training_tokens = 1_000_000 * 300, # 200M tokens seems doable overnight.\n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    use_ghost_grads=True,\n",
    "    feature_sampling_window = 2500,\n",
    "    dead_feature_window=5000,\n",
    "    dead_feature_threshold = 1e-8,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_language_models_resid_pre_test\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=100,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"cuda\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 10,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythia 70-M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "import sys \n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "import cProfile\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"pythia-70m-deduped\",\n",
    "    hook_point = \"blocks.0.hook_mlp_out\",\n",
    "    hook_point_layer = 0,\n",
    "    d_in = 512,\n",
    "    dataset_path = \"EleutherAI/the_pile_deduplicated\",\n",
    "    is_dataset_tokenized=False,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 64,\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 3e-4,\n",
    "    l1_coefficient = 4e-5,\n",
    "    train_batch_size = 8192,\n",
    "    context_size = 128,\n",
    "    lr_scheduler_name=\"constantwithwarmup\",\n",
    "    lr_warm_up_steps=10_000,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 64,\n",
    "    total_training_tokens = 1_000_000 * 800, \n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'anthropic',\n",
    "    feature_sampling_window = 2000, # Doesn't currently matter.\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=40000,\n",
    "    dead_feature_threshold = 1e-8,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_language_benchmark_tests\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=20,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"cuda\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 0,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythia 70M Hook Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"pythia-70m-deduped\",\n",
    "    hook_point = \"blocks.2.attn.hook_q\",\n",
    "    hook_point_layer = 2,\n",
    "    hook_point_head_index=7,\n",
    "    d_in = 64,\n",
    "    dataset_path = \"EleutherAI/the_pile_deduplicated\",\n",
    "    is_dataset_tokenized=False,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 16,\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 0.0012,\n",
    "    l1_coefficient = 0.003,\n",
    "    lr_scheduler_name=\"constantwithwarmup\",\n",
    "    lr_warm_up_steps=1000, # about 4 million tokens.\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 128,\n",
    "    total_training_tokens = 1_000_000 * 1500,\n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'anthropic',\n",
    "    feature_sampling_window = 1000,# doesn't do anything currently.\n",
    "    feature_reinit_scale = 0.2,\n",
    "    resample_batches=8,\n",
    "    dead_feature_window=60000,\n",
    "    dead_feature_threshold = 1e-5,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_pythia_70M_hook_q_L2H7\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=100,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 15,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"tiny-stories-2L-33M\",\n",
    "    hook_point = \"blocks.1.mlp.hook_post\",\n",
    "    hook_point_layer = 1,\n",
    "    d_in = 4096,\n",
    "    dataset_path = \"roneneldan/TinyStories\",\n",
    "    is_dataset_tokenized=False,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 4,\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 1e-4,\n",
    "    l1_coefficient = 3e-4,\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 128,\n",
    "    total_training_tokens = 1_000_000 * 10, # want 500M eventually.\n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'l2',\n",
    "    feature_sampling_window = 2500, # Doesn't currently matter.\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=1250,\n",
    "    dead_feature_threshold = 0.0005,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_language_benchmark_tests\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=10,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 0,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sae_training.toy_model_runner import SAEToyModelRunnerConfig, toy_model_sae_runner\n",
    "\n",
    "\n",
    "cfg = SAEToyModelRunnerConfig(\n",
    "    \n",
    "    # Model Details\n",
    "    n_features=200,\n",
    "    n_hidden=5,\n",
    "    n_correlated_pairs=0,\n",
    "    n_anticorrelated_pairs=0,\n",
    "    feature_probability=0.025,\n",
    "    model_training_steps=10_000,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    d_sae=240,\n",
    "    l1_coefficient=0.001,\n",
    "    \n",
    "    # SAE Train Config\n",
    "    train_batch_size=1028,\n",
    "    feature_sampling_window=3_000,\n",
    "    dead_feature_window=1_000,\n",
    "    feature_reinit_scale=0.5,\n",
    "    total_training_tokens=4096*300,\n",
    "    \n",
    "    # Other parameters\n",
    "    log_to_wandb=True,\n",
    "    wandb_project=\"sae-training-test\",\n",
    "    wandb_log_frequency=5,\n",
    "    device=\"mps\",\n",
    ")\n",
    "\n",
    "trained_sae = toy_model_sae_runner(cfg)\n",
    "\n",
    "assert trained_sae is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run caching of activations to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "\n",
    "from sae_training.config import CacheActivationsRunnerConfig\n",
    "from sae_training.cache_activations_runner import cache_activations_runner\n",
    "\n",
    "cfg = CacheActivationsRunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gpt2-small\",\n",
    "    hook_point = \"blocks.10.attn.hook_q\",\n",
    "    hook_point_layer = 10,\n",
    "    hook_point_head_index=7,\n",
    "    d_in = 64,\n",
    "    dataset_path = \"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    cached_activations_path=\"../activations/\",\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 16,\n",
    "    total_training_tokens = 500_000_000, \n",
    "    store_batch_size = 32,\n",
    "\n",
    "    # Activation caching shuffle parameters\n",
    "    n_shuffles_final = 16,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "cache_activations_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an SAE using the cached activations stored on disk\n",
    "Pass `use_cached_activations=True` into the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gpt2-small\",\n",
    "    hook_point = \"blocks.10.hook_resid_pre\",\n",
    "    hook_point_layer = 11,\n",
    "    d_in = 768,\n",
    "    dataset_path = \"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    use_cached_activations=True,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 64, # determines the dimension of the SAE.\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 1e-5,\n",
    "    l1_coefficient = 5e-4,\n",
    "    lr_scheduler_name=None,\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 64,\n",
    "    total_training_tokens = 200_000,\n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'l2',\n",
    "    feature_sampling_window = 1000,\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=5000,\n",
    "    dead_feature_threshold = 1e-7,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_gpt2_small\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=50,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 5,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "import sys \n",
    "sys.path.append(\"..\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "\n",
    "\n",
    "# for l1_coefficient in [9e-4,8e-4,7e-4]:\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gpt2-small\",\n",
    "    hook_point = \"blocks.10.attn.hook_q\",\n",
    "    hook_point_layer = 10,\n",
    "    hook_point_head_index=7,\n",
    "    d_in = 64,\n",
    "    dataset_path = \"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    use_cached_activations=True,\n",
    "    cached_activations_path=\"../activations/\",\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 64, # determines the dimension of the SAE. (64*64 = 4096, 64*4*64 = 32768)\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 1e-3,\n",
    "    l1_coefficient = 2e-4,\n",
    "    # lr_scheduler_name=\"LinearWarmupDecay\",\n",
    "    lr_warm_up_steps=2200,\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 512,\n",
    "    total_training_tokens = 3_000_000,\n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'l2',\n",
    "    feature_sampling_window = 1000,\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=200,\n",
    "    dead_feature_threshold = 5e-6,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_gpt2_small_hook_q_dev\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=5,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 0,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "# cfg.d_sae\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n",
    "# assert sparse_autoencoder is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
