{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c317973d-412e-4cc9-bdcc-a64d7f963e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "1\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry190.txtblocks.1.hook_out_proj/hook_blocks.1.hook_out_proj.pt\n",
      "2\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry191.txtblocks.2.hook_out_proj/hook_blocks.2.hook_out_proj.pt\n",
      "3\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry192.txtblocks.3.hook_out_proj/hook_blocks.3.hook_out_proj.pt\n",
      "4\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry193.txtblocks.4.hook_out_proj/hook_blocks.4.hook_out_proj.pt\n",
      "5\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry194.txtblocks.5.hook_out_proj/hook_blocks.5.hook_out_proj.pt\n",
      "6\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry195.txtblocks.6.hook_out_proj/hook_blocks.6.hook_out_proj.pt\n",
      "7\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry196.txtblocks.7.hook_out_proj/hook_blocks.7.hook_out_proj.pt\n",
      "8\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry200.txtblocks.8.hook_out_proj/hook_blocks.8.hook_out_proj.pt\n",
      "9\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry201.txtblocks.9.hook_out_proj/hook_blocks.9.hook_out_proj.pt\n",
      "10\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry202.txtblocks.10.hook_out_proj/hook_blocks.10.hook_out_proj.pt\n",
      "11\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry203.txtblocks.11.hook_out_proj/hook_blocks.11.hook_out_proj.pt\n",
      "12\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry204.txtblocks.12.hook_out_proj/hook_blocks.12.hook_out_proj.pt\n",
      "13\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry205.txtblocks.13.hook_out_proj/hook_blocks.13.hook_out_proj.pt\n",
      "14\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry206.txtblocks.14.hook_out_proj/hook_blocks.14.hook_out_proj.pt\n",
      "15\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry210.txtblocks.15.hook_out_proj/hook_blocks.15.hook_out_proj.pt\n",
      "16\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry211.txtblocks.16.hook_out_proj/hook_blocks.16.hook_out_proj.pt\n",
      "17\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry212.txtblocks.17.hook_out_proj/hook_blocks.17.hook_out_proj.pt\n",
      "18\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry213.txtblocks.18.hook_out_proj/hook_blocks.18.hook_out_proj.pt\n",
      "19\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry214.txtblocks.19.hook_out_proj/hook_blocks.19.hook_out_proj.pt\n",
      "20\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry215.txtblocks.20.hook_out_proj/hook_blocks.20.hook_out_proj.pt\n",
      "21\n",
      "/home/dev/sae-k-sparse-mamba/0.0001414213562373095 initialTry216.txtblocks.21.hook_out_proj/hook_blocks.21.hook_out_proj.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", font_size=None, show=True, color_continuous_midpoint=0.0, **kwargs):\n",
    "    import plotly.express as px\n",
    "    import transformer_lens.utils as utils\n",
    "    fig = px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=color_continuous_midpoint, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs)\n",
    "    if not font_size is None:\n",
    "        if 'x' in kwargs:\n",
    "            fig.update_layout(\n",
    "              xaxis = dict(\n",
    "                tickmode='array',\n",
    "                tickvals = kwargs['x'],\n",
    "                ticktext = kwargs['x'], \n",
    "                ),\n",
    "               font=dict(size=font_size, color=\"black\"))\n",
    "        if 'y' in kwargs:\n",
    "            fig.update_layout(\n",
    "              yaxis = dict(\n",
    "                tickmode='array',\n",
    "                tickvals = kwargs['y'],\n",
    "                ticktext = kwargs['y'], \n",
    "                ),\n",
    "               font=dict(size=font_size, color=\"black\"))\n",
    "    plot_args = {\n",
    "        'width': 800,\n",
    "        'height': 600,\n",
    "        \"autosize\": False,\n",
    "        'showlegend': True,\n",
    "        'margin': {\"l\":0,\"r\":0,\"t\":100,\"b\":0}\n",
    "    }\n",
    "    \n",
    "    fig.update_layout(**plot_args)\n",
    "    fig.update_layout(legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    ))\n",
    "    if show:\n",
    "        fig.show(renderer)\n",
    "    else:\n",
    "        return fig\n",
    "\n",
    "\n",
    "# requires\n",
    "# pip install git+https://github.com/Phylliida/MambaLens.git\n",
    "\n",
    "from mamba_lens import HookedMamba # this will take a little while to import\n",
    "import torch\n",
    "model_path = \"state-spaces/mamba-370m\"\n",
    "\n",
    "# NOTE! We need to monkeypatch transformer lens to use register_full_backward_hook\n",
    "model = HookedMamba.from_pretrained(model_path, device='cuda')\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "if not \"/home/dev/sae-k-sparse-mamba/sae\" in sys.path:\n",
    "    sys.path.append(\"/home/dev/sae-k-sparse-mamba\")\n",
    "import os\n",
    "os.chdir('/home/dev/sae-k-sparse-mamba')\n",
    "saes = [None]\n",
    "from importlib import reload\n",
    "from sae.sae import Sae\n",
    "\n",
    "ckpt_dir = \"/home/dev/sae-k-sparse-mamba/\"\n",
    "for i in range(1,22):\n",
    "    print(i)\n",
    "    hook = f'blocks.{i}.hook_out_proj'\n",
    "    path = [ckpt_dir + f for f in sorted(list(os.listdir(ckpt_dir))) if hook in f][0] + \"/\" + f'hook_{hook}.pt'\n",
    "    #path = f'/home/dev/sae-k-sparse-mamba/blocks.{i}.hook_resid_pre/hook_blocks.{i}.hook_resid_pre.pt'\n",
    "    print(path)\n",
    "    saes.append(Sae.load_from_disk(path, hook=f'blocks.{i}.hook_resid_pre', device=model.cfg.device))\n",
    "\n",
    "\n",
    "global PATCHING_FORMAT_I\n",
    "global patching_formats\n",
    "def make_data(num_patching_pairs, patching, template_i, seed, valid_seed):\n",
    "    constrain_to_answers = True\n",
    "    # this makes our data size 800, first 400 is each (a,b) pair, and then second 400 is each pair swapped to be (b,a)\n",
    "    has_symmetric_patching = True\n",
    "    \n",
    "    n1_patchings = [\"\"\"\n",
    "    ABC BC A\n",
    "    DBC BC D\"\"\",\n",
    "        \"\"\"\n",
    "    ABC CB A\n",
    "    DBC CB D\"\"\"]\n",
    "    \n",
    "    n2_patchings = [\"\"\"\n",
    "    ABC AC B\n",
    "    ADC AC D\"\"\",\n",
    "        \"\"\"\n",
    "    ABC CA B\n",
    "    ADC CA D\"\"\"]\n",
    "    \n",
    "    n3_patchings = [\"\"\"\n",
    "    ABC AB C\n",
    "    ABD AB D\"\"\",\n",
    "        \"\"\"\n",
    "    ABC BA C\n",
    "    ABD BA D\"\"\"]\n",
    "    \n",
    "    n4_patchings = [\"\"\"\n",
    "    ABC AC B\n",
    "    ABC BC A\"\"\",\n",
    "        \"\"\"\n",
    "    ABC AB C\n",
    "    ABC CB A\"\"\",\n",
    "        \"\"\"\n",
    "    ABC BA C\n",
    "    ABC CA B\"\"\"]\n",
    "    \n",
    "    n5_patchings = [\"\"\"\n",
    "    ABC CA B\n",
    "    ABC CB A\n",
    "    \"\"\",\n",
    "        \"\"\"\n",
    "    ABC BA C\n",
    "    ABC BC A\"\"\",\n",
    "        \"\"\"\n",
    "    ABC AB C\n",
    "    ABC AC B\"\"\"]\n",
    "    \n",
    "    patchings = {\n",
    "        'n1': n1_patchings,\n",
    "        'n2': n2_patchings,\n",
    "        'n3': n3_patchings,\n",
    "        'n4': n4_patchings,\n",
    "        'n5': n5_patchings\n",
    "    }\n",
    "    \n",
    "    all_patchings = []\n",
    "    for patching_set in patchings.values():\n",
    "        all_patchings += patching_set\n",
    "    all_patchings = sorted(all_patchings) # make deterministic \n",
    "    \n",
    "    patch_all_names = [\"\"\"\n",
    "    ABC AB C\n",
    "    DEF DE F\"\"\",\n",
    "        \"\"\"\n",
    "    ABC AC B\n",
    "    DEF DF E\"\"\",     \n",
    "        \"\"\"\n",
    "    ABC BA C\n",
    "    DEF ED F\"\"\",\n",
    "        \"\"\"\n",
    "    ABC BC A\n",
    "    DEF EF D\"\"\",\n",
    "        \"\"\"\n",
    "    ABC CA B\n",
    "    DEF FD E\"\"\",\n",
    "        \"\"\"\n",
    "    ABC CB A\n",
    "    DEF FE D\"\"\"]\n",
    "    \n",
    "    \n",
    "    patchings['all'] = all_patchings\n",
    "    patchings['allatonce'] = patch_all_names\n",
    "    from acdc.data.ioi import BABA_TEMPLATES, ABC_TEMPLATES\n",
    "    from acdc.data.ioi import ioi_data_generator, ABC_TEMPLATES, get_all_single_name_abc_patching_formats\n",
    "    from acdc.data.utils import generate_dataset\n",
    "    templates = ABC_TEMPLATES\n",
    "    #patching_formats = list(get_all_single_name_abc_patching_formats())\n",
    "    global PATCHING_FORMAT_I\n",
    "    global patching_formats\n",
    "    PATCHING_FORMAT_I = patching\n",
    "    patching_formats = [\"\\n\".join([line.strip() for line in x.split(\"\\n\")]).strip() for x in patchings[PATCHING_FORMAT_I]]\n",
    "    \n",
    "    print(\"using patching format\")\n",
    "    for patch in patching_formats:\n",
    "        print(patch)\n",
    "        print(\"\")\n",
    "    #print(patching_formats)\n",
    "    \n",
    "    \n",
    "    data = generate_dataset(model=model,\n",
    "                      data_generator=ioi_data_generator,\n",
    "                      num_patching_pairs=4,\n",
    "                      seed=seed,\n",
    "                      valid_seed=valid_seed,\n",
    "                      constrain_to_answers=constrain_to_answers,\n",
    "                      has_symmetric_patching=has_symmetric_patching, \n",
    "                      varying_data_lengths=True,\n",
    "                      templates=templates,\n",
    "                      patching_formats=patching_formats)\n",
    "    \n",
    "    \n",
    "    import acdc.data.ioi\n",
    "    from collections import defaultdict\n",
    "    name_positions_map = defaultdict(lambda: [])\n",
    "    for template in templates:\n",
    "        name = acdc.data.ioi.good_names[0]\n",
    "        template_filled_in = template.replace(\"[NAME]\", name)\n",
    "        template_filled_in = template_filled_in.replace(\"[PLACE]\", acdc.data.ioi.good_nouns['[PLACE]'][0])\n",
    "        template_filled_in = template_filled_in.replace(\"[OBJECT]\", acdc.data.ioi.good_nouns['[OBJECT]'][0])\n",
    "        # get the token positions of the [NAME] in the prompt\n",
    "        name_positions = tuple([(i) for (i,s) in enumerate(model.to_str_tokens(torch.tensor(model.tokenizer.encode(template_filled_in)))) if s == f' {name}'])\n",
    "        name_positions_map[name_positions].append(template)\n",
    "    sorted_by_frequency = sorted(list(name_positions_map.items()), key=lambda x: -len(x[1]))\n",
    "    most_frequent_name_positions, templates = sorted_by_frequency[0]\n",
    "    print(\"using templates\")\n",
    "    templates = [templates[0]]\n",
    "    for template in templates:\n",
    "        print(template)\n",
    "    print(f\"with name positions {most_frequent_name_positions}\")\n",
    "    import acdc.data.ioi\n",
    "    if 'Jesus' in acdc.data.ioi.good_names:\n",
    "        print(\"removed jesus\")\n",
    "        acdc.data.ioi.good_names.remove(\"Jesus\")\n",
    "    data = generate_dataset(model=model,\n",
    "                  data_generator=ioi_data_generator,\n",
    "                  num_patching_pairs=num_patching_pairs,\n",
    "                  seed=seed,\n",
    "                  valid_seed=valid_seed,\n",
    "                  constrain_to_answers=constrain_to_answers,\n",
    "                  has_symmetric_patching=has_symmetric_patching, \n",
    "                  varying_data_lengths=True,\n",
    "                  templates=templates,\n",
    "                  patching_formats=patching_formats)\n",
    "    \n",
    "    print(model.to_str_tokens(data.data[0]))\n",
    "    print(model.to_str_tokens(data.data[1]))\n",
    "    return data\n",
    "\n",
    "\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "# we do a hacky thing where this first hook clears the global storage\n",
    "# second hook stores all the hooks\n",
    "# then third hook computes the output (over all the hooks)\n",
    "# this avoids recomputing and so is much faster\n",
    "SAE_HOOKS = \"sae hooks\"\n",
    "SAE_BATCHES = \"sae batches\"\n",
    "SAE_OUTPUT = \"sae output\"\n",
    "def sae_patching_storage_hook(\n",
    "    x,\n",
    "    hook: HookPoint,\n",
    "    sae_feature_i: int,\n",
    "    dummy: bool,\n",
    "    position: int,\n",
    "    layer: int,\n",
    "    batch_start: int,\n",
    "    batch_end: int,\n",
    "    **kwargs,\n",
    "):\n",
    "    global sae_storage\n",
    "    if not SAE_HOOKS in sae_storage:\n",
    "        sae_storage[SAE_HOOKS] = [] # we can't do this above because it'll be emptied again on the next batch before this is called\n",
    "    sae_storage[SAE_OUTPUT] = None # clear output\n",
    "    sae_storage[SAE_HOOKS].append({\"position\": position, \"sae_feature_i\": sae_feature_i, \"dummy\": dummy})\n",
    "    #print(f\"sae feature i {sae_feature_i} position {position} layer {layer}\")\n",
    "    return x\n",
    "\n",
    "from jaxtyping import Float\n",
    "from einops import rearrange\n",
    "\n",
    "global sae_storage\n",
    "sae_storage = {}\n",
    "def sae_patching_hook(\n",
    "    x: Float[torch.Tensor, \"B L E\"],\n",
    "    hook: HookPoint,\n",
    "    input_hook_name: str,\n",
    "    layer: int,\n",
    "    **kwargs,\n",
    ") -> Float[torch.Tensor, \"B L E\"]:\n",
    "    global sae_storage\n",
    "    ### This is identical to what the conv is doing\n",
    "    # but we break it apart so we can patch on individual filters\n",
    "    # we have two input hooks, the second one is the one we want\n",
    "    input_hook_name = input_hook_name[1]\n",
    "    # don't recompute these if we don't need to\n",
    "    # because we stored all the hooks and batches in conv_storage, we can just do them all at once\n",
    "    \n",
    "    # they need to share an output because they write to the same output tensor\n",
    "    if sae_storage[SAE_OUTPUT] is None:\n",
    "        #print(f\"running for layer {layer}\")\n",
    "        K = saes[layer].cfg.k\n",
    "        sae = saes[layer]\n",
    "        #print(f\"layer {layer} storage {sae_storage}\")\n",
    "        sae_output = torch.zeros(x.size(), device=model.cfg.device)\n",
    "        #print(\"layer\", layer, \"keys\", conv_storage)\n",
    "        def get_filter_key(i):\n",
    "            return f'filter_{i}'\n",
    "        sae_input_uncorrupted = x[::2]\n",
    "        sae_input_corrupted = x[1::2]\n",
    "        B, L, D = sae_input_uncorrupted.size()\n",
    "        for l in range(L):\n",
    "            # [B, NFeatures]                             [B,D]\n",
    "            uncorrupted_features = sae.encode(sae_input_uncorrupted[:,l])\n",
    "            # [B, NFeatures]                             [B,D]\n",
    "            corrupted_features = sae.encode(sae_input_corrupted[:,l])\n",
    "            patched_features = corrupted_features.clone()\n",
    "            #patched_features = torch.zeros(corrupted_features.size(), device=model.cfg.device) # patch everything except the features we are keeping around\n",
    "            # apply hooks (one hook applies to a single feature)\n",
    "            #print(f\"{len(sae_storage[SAE_HOOKS])} hooks\")\n",
    "            for hook_data in sae_storage[SAE_HOOKS]:\n",
    "                position = hook_data['position']\n",
    "                sae_feature_i = hook_data['sae_feature_i']\n",
    "                dummy = hook_data['dummy']\n",
    "                if not dummy and (position == l or position is None): # position is None means all positions\n",
    "                    if copy_from_other:\n",
    "                        patched_features[:,sae_feature_i] = corrupted_features[:,sae_feature_i]\n",
    "                    else:\n",
    "                        patched_features[:,sae_feature_i] = uncorrupted_features[:,sae_feature_i]\n",
    "                    \n",
    "                    #print(f\"applying sae feature {sae_feature_i} to position {position} for layer {layer}\")\n",
    "                    #uncorrupted_features[:,sae_feature_i] = corrupted_features[:,sae_feature_i]\n",
    "            # compute sae outputs\n",
    "            patched_top_acts, patched_top_indices = patched_features.topk(K, sorted=False)\n",
    "            corrupted_top_acts, corrupted_top_indices = corrupted_features.topk(K, sorted=False)      \n",
    "            sae_output[::2,l] = sae.decode(patched_top_acts, patched_top_indices)     \n",
    "            sae_output[1::2,l] = sae.decode(corrupted_top_acts, corrupted_top_indices)\n",
    "        sae_storage = {} # clean up and prepare for next layer\n",
    "        sae_storage[SAE_OUTPUT] = sae_output # store the output\n",
    "    return sae_storage[SAE_OUTPUT]\n",
    "\n",
    "import pickle\n",
    "with open(\"cached_sae_feature_edges.pkl\", \"rb\") as f:\n",
    "    edges_to_keep = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b24f7e96-e4b1-4221-8963-ce3fdce69e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using patching format\n",
      "ABC AB C\n",
      "ABC AC B\n",
      "\n",
      "ABC AB C\n",
      "ABC CB A\n",
      "\n",
      "ABC AB C\n",
      "ABD AB D\n",
      "\n",
      "ABC AC B\n",
      "ABC BC A\n",
      "\n",
      "ABC AC B\n",
      "ADC AC D\n",
      "\n",
      "ABC BA C\n",
      "ABC BC A\n",
      "\n",
      "ABC BA C\n",
      "ABC CA B\n",
      "\n",
      "ABC BA C\n",
      "ABD BA D\n",
      "\n",
      "ABC BC A\n",
      "DBC BC D\n",
      "\n",
      "ABC CA B\n",
      "ABC CB A\n",
      "\n",
      "ABC CA B\n",
      "ADC CA D\n",
      "\n",
      "ABC CB A\n",
      "DBC CB D\n",
      "\n",
      "using templates\n",
      "Then, [NAME], [NAME] and [NAME] went to the [PLACE]. [NAME] and [NAME] gave a [OBJECT] to\n",
      "with name positions (2, 4, 6, 12, 14)\n",
      "['<|endoftext|>', 'Then', ',', ' Olivia', ',', ' Ian', ' and', ' Aaron', ' went', ' to', ' the', ' restaurant', '.', ' Aaron', ' and', ' Olivia', ' gave', ' a', ' computer', ' to']\n",
      "['<|endoftext|>', 'Then', ',', ' Olivia', ',', ' Ian', ' and', ' Aaron', ' went', ' to', ' the', ' restaurant', '.', ' Aaron', ' and', ' Ian', ' gave', ' a', ' computer', ' to']\n"
     ]
    }
   ],
   "source": [
    "data = make_data(num_patching_pairs=20000, patching=\"all\", template_i=0, seed=24, valid_seed=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c820181-cedc-41d9-90f0-9ea40dd47738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8487\n"
     ]
    }
   ],
   "source": [
    "\n",
    "toks = model.to_str_tokens(data.data[0])\n",
    "name_positions = [3,5,7,13,15]\n",
    "position_map = {}\n",
    "L = data.data.size()[1]\n",
    "for l in range(L):\n",
    "    position_map[l] = f'pos{l}{toks[l]}'\n",
    "position_map[3] = 'n1'\n",
    "position_map[5] = 'n2'\n",
    "position_map[7] = 'n3'\n",
    "position_map[13] = 'n4'\n",
    "position_map[15] = 'n5'\n",
    "position_map[19] = 'out'\n",
    "\n",
    "\n",
    "print(len(edges_to_keep))\n",
    "sae_edges = defaultdict(lambda: defaultdict(lambda: []))\n",
    "counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "num_to_show = 400\n",
    "iters = 0\n",
    "\n",
    "for edge in edges_to_keep:\n",
    "    if '.sae' in edge.output_node and not edge.label is None:\n",
    "        # [pos:feature_i]\n",
    "        label = edge.label[1:-1]\n",
    "        pos, feature_i = label.split(\":\")\n",
    "        pos = int(pos)\n",
    "        if feature_i == 'KEEP': continue # dummy edge used to ensure sae always applied\n",
    "        iters += 1\n",
    "        feature_i = int(feature_i)\n",
    "        layer = int(edge.output_node.split(\".\")[0])\n",
    "        attr = edge.score_diff_when_patched\n",
    "        sae_edges[layer][pos].append((attr, feature_i))\n",
    "        counts[layer][feature_i] += 1\n",
    "        #print(layer, pos, position_map[pos], feature_i, attr)\n",
    "        #print(layer, pos, feature_i, attr)\n",
    "        #if iters > num_to_show: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95a20ec1-1db3-4000-84eb-2bbb832acbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 with 144 unique features (144 duplicated)\n",
      "  pos n1 num sae 23 min attr scaled -16.656\n",
      "  pos n2 num sae 30 min attr scaled -10.017\n",
      "  pos n3 num sae 28 min attr scaled -14.836\n",
      "  pos n4 num sae 65 min attr scaled -84.558\n",
      "  pos n5 num sae 61 min attr scaled -152.670\n",
      "layer 2 with 17 unique features (17 duplicated)\n",
      "  pos n1 num sae 4 min attr scaled -18.957\n",
      "  pos n2 num sae 1 min attr scaled -19.427\n",
      "  pos n3 num sae 6 min attr scaled -11.387\n",
      "  pos n4 num sae 6 min attr scaled -29.547\n",
      "  pos n5 num sae 8 min attr scaled -28.615\n",
      "layer 3 with 213 unique features (213 duplicated)\n",
      "  pos n1 num sae 35 min attr scaled -16.394\n",
      "  pos n2 num sae 77 min attr scaled -46.406\n",
      "  pos pos6 and num sae 1 min attr scaled -6.787\n",
      "  pos n3 num sae 83 min attr scaled -27.689\n",
      "  pos pos11 garden num sae 13 min attr scaled -20.751\n",
      "  pos pos12. num sae 7 min attr scaled -12.354\n",
      "  pos n4 num sae 84 min attr scaled -44.882\n",
      "  pos n5 num sae 71 min attr scaled -77.533\n",
      "layer 4 with 22 unique features (22 duplicated)\n",
      "  pos n1 num sae 5 min attr scaled -17.063\n",
      "  pos n2 num sae 3 min attr scaled -10.081\n",
      "  pos n3 num sae 6 min attr scaled -13.332\n",
      "  pos n4 num sae 12 min attr scaled -18.175\n",
      "  pos n5 num sae 10 min attr scaled -19.352\n",
      "layer 5 with 27 unique features (27 duplicated)\n",
      "  pos n1 num sae 9 min attr scaled -16.312\n",
      "  pos n2 num sae 8 min attr scaled -21.393\n",
      "  pos n3 num sae 11 min attr scaled -20.099\n",
      "  pos pos11 garden num sae 1 min attr scaled -8.277\n",
      "  pos pos12. num sae 1 min attr scaled -6.815\n",
      "  pos n4 num sae 10 min attr scaled -13.761\n",
      "  pos n5 num sae 5 min attr scaled -15.067\n",
      "layer 6 with 106 unique features (106 duplicated)\n",
      "  pos n1 num sae 28 min attr scaled -96.565\n",
      "  pos n2 num sae 34 min attr scaled -33.768\n",
      "  pos n3 num sae 38 min attr scaled -61.748\n",
      "  pos n4 num sae 42 min attr scaled -74.310\n",
      "  pos pos14 and num sae 1 min attr scaled -8.445\n",
      "  pos n5 num sae 32 min attr scaled -46.432\n",
      "  pos pos16 gave num sae 1 min attr scaled -7.409\n",
      "layer 7 with 76 unique features (76 duplicated)\n",
      "  pos n1 num sae 17 min attr scaled -56.832\n",
      "  pos n2 num sae 34 min attr scaled -89.337\n",
      "  pos n3 num sae 28 min attr scaled -93.764\n",
      "  pos n4 num sae 36 min attr scaled -36.213\n",
      "  pos n5 num sae 30 min attr scaled -31.573\n",
      "layer 8 with 62 unique features (62 duplicated)\n",
      "  pos n1 num sae 12 min attr scaled -22.613\n",
      "  pos n2 num sae 34 min attr scaled -37.004\n",
      "  pos n3 num sae 29 min attr scaled -43.128\n",
      "  pos pos11 garden num sae 1 min attr scaled -6.443\n",
      "  pos n4 num sae 30 min attr scaled -37.179\n",
      "  pos n5 num sae 28 min attr scaled -56.839\n",
      "layer 9 with 51 unique features (51 duplicated)\n",
      "  pos n1 num sae 15 min attr scaled -83.370\n",
      "  pos n2 num sae 16 min attr scaled -74.662\n",
      "  pos n3 num sae 15 min attr scaled -49.574\n",
      "  pos n4 num sae 24 min attr scaled -69.208\n",
      "  pos pos14 and num sae 1 min attr scaled -9.820\n",
      "  pos n5 num sae 26 min attr scaled -34.164\n",
      "layer 10 with 240 unique features (240 duplicated)\n",
      "  pos n1 num sae 75 min attr scaled -59.283\n",
      "  pos n2 num sae 114 min attr scaled -68.241\n",
      "  pos n3 num sae 112 min attr scaled -115.886\n",
      "  pos n4 num sae 126 min attr scaled -85.361\n",
      "  pos n5 num sae 120 min attr scaled -81.880\n",
      "layer 11 with 476 unique features (476 duplicated)\n",
      "  pos n1 num sae 187 min attr scaled -73.841\n",
      "  pos n2 num sae 204 min attr scaled -147.710\n",
      "  pos n3 num sae 214 min attr scaled -143.825\n",
      "  pos n4 num sae 305 min attr scaled -463.253\n",
      "  pos pos14 and num sae 1 min attr scaled -6.627\n",
      "  pos n5 num sae 311 min attr scaled -214.225\n",
      "  pos pos16 gave num sae 1 min attr scaled -7.675\n",
      "layer 12 with 115 unique features (115 duplicated)\n",
      "  pos n1 num sae 23 min attr scaled -25.050\n",
      "  pos n2 num sae 43 min attr scaled -33.404\n",
      "  pos pos6 and num sae 1 min attr scaled -6.503\n",
      "  pos n3 num sae 31 min attr scaled -36.707\n",
      "  pos n4 num sae 62 min attr scaled -72.264\n",
      "  pos n5 num sae 46 min attr scaled -300.988\n",
      "layer 13 with 117 unique features (117 duplicated)\n",
      "  pos n1 num sae 35 min attr scaled -48.773\n",
      "  pos n2 num sae 43 min attr scaled -76.823\n",
      "  pos pos6 and num sae 1 min attr scaled -7.522\n",
      "  pos n3 num sae 27 min attr scaled -57.468\n",
      "  pos pos9 to num sae 1 min attr scaled -6.441\n",
      "  pos n4 num sae 29 min attr scaled -30.263\n",
      "  pos pos14 and num sae 2 min attr scaled -8.597\n",
      "  pos n5 num sae 66 min attr scaled -66.322\n",
      "layer 14 with 111 unique features (111 duplicated)\n",
      "  pos n1 num sae 21 min attr scaled -177.633\n",
      "  pos n2 num sae 33 min attr scaled -293.249\n",
      "  pos n3 num sae 29 min attr scaled -146.471\n",
      "  pos n4 num sae 63 min attr scaled -476.584\n",
      "  pos n5 num sae 43 min attr scaled -799.967\n",
      "layer 15 with 307 unique features (307 duplicated)\n",
      "  pos n1 num sae 81 min attr scaled -227.235\n",
      "  pos pos4, num sae 1 min attr scaled -12.824\n",
      "  pos n2 num sae 106 min attr scaled -174.778\n",
      "  pos pos6 and num sae 1 min attr scaled -7.157\n",
      "  pos n3 num sae 103 min attr scaled -240.607\n",
      "  pos pos12. num sae 1 min attr scaled -9.793\n",
      "  pos n4 num sae 189 min attr scaled -1451.181\n",
      "  pos pos14 and num sae 1 min attr scaled -6.769\n",
      "  pos n5 num sae 184 min attr scaled -892.802\n",
      "layer 16 with 158 unique features (158 duplicated)\n",
      "  pos n1 num sae 38 min attr scaled -50.164\n",
      "  pos pos4, num sae 1 min attr scaled -6.146\n",
      "  pos n2 num sae 46 min attr scaled -138.444\n",
      "  pos pos6 and num sae 2 min attr scaled -14.873\n",
      "  pos n3 num sae 42 min attr scaled -96.027\n",
      "  pos pos11 garden num sae 1 min attr scaled -8.679\n",
      "  pos n4 num sae 58 min attr scaled -92.183\n",
      "  pos pos14 and num sae 2 min attr scaled -7.226\n",
      "  pos n5 num sae 77 min attr scaled -213.139\n",
      "  pos out num sae 1 min attr scaled -6.823\n",
      "layer 17 with 326 unique features (326 duplicated)\n",
      "  pos n1 num sae 99 min attr scaled -80.235\n",
      "  pos n2 num sae 99 min attr scaled -81.565\n",
      "  pos n3 num sae 145 min attr scaled -87.856\n",
      "  pos n4 num sae 116 min attr scaled -87.923\n",
      "  pos n5 num sae 141 min attr scaled -111.574\n",
      "  pos pos18 apple num sae 1 min attr scaled -7.163\n",
      "layer 18 with 178 unique features (178 duplicated)\n",
      "  pos n1 num sae 29 min attr scaled -59.807\n",
      "  pos pos4, num sae 1 min attr scaled -12.021\n",
      "  pos n2 num sae 48 min attr scaled -51.506\n",
      "  pos n3 num sae 54 min attr scaled -34.326\n",
      "  pos pos9 to num sae 1 min attr scaled -9.297\n",
      "  pos pos11 garden num sae 1 min attr scaled -7.401\n",
      "  pos n4 num sae 61 min attr scaled -132.793\n",
      "  pos pos14 and num sae 1 min attr scaled -6.779\n",
      "  pos n5 num sae 88 min attr scaled -99.477\n",
      "  pos pos18 apple num sae 1 min attr scaled -6.201\n",
      "layer 19 with 490 unique features (490 duplicated)\n",
      "  pos n1 num sae 66 min attr scaled -388.410\n",
      "  pos n2 num sae 78 min attr scaled -519.979\n",
      "  pos pos6 and num sae 3 min attr scaled -42.154\n",
      "  pos n3 num sae 53 min attr scaled -271.056\n",
      "  pos pos8 went num sae 1 min attr scaled -25.634\n",
      "  pos n4 num sae 300 min attr scaled -260.125\n",
      "  pos n5 num sae 264 min attr scaled -439.111\n",
      "  pos out num sae 2 min attr scaled -8.284\n",
      "layer 20 with 636 unique features (636 duplicated)\n",
      "  pos n1 num sae 77 min attr scaled -183.778\n",
      "  pos n2 num sae 103 min attr scaled -381.476\n",
      "  pos n3 num sae 129 min attr scaled -177.679\n",
      "  pos n4 num sae 347 min attr scaled -989.013\n",
      "  pos n5 num sae 384 min attr scaled -1033.803\n",
      "  pos out num sae 1 min attr scaled -8.707\n",
      "layer 21 with 273 unique features (272 duplicated)\n",
      "  pos n1 num sae 33 min attr scaled -114.525\n",
      "  pos pos4, num sae 1 min attr scaled -8.348\n",
      "  pos n2 num sae 46 min attr scaled -144.918\n",
      "  pos pos6 and num sae 1 min attr scaled -7.095\n",
      "  pos n3 num sae 55 min attr scaled -158.659\n",
      "  pos n4 num sae 122 min attr scaled -347.354\n",
      "  pos pos14 and num sae 12 min attr scaled -18.921\n",
      "  pos n5 num sae 148 min attr scaled -348.150\n",
      "  pos pos16 gave num sae 2 min attr scaled -8.536\n",
      "  pos pos18 apple num sae 1 min attr scaled -7.370\n",
      "  pos out num sae 16 min attr scaled -14.859\n",
      "total num features 4145\n"
     ]
    }
   ],
   "source": [
    "total_num_features = 0\n",
    "for layer in sorted(list(sae_edges.keys())):\n",
    "    print(f\"layer {layer} with {len(counts[layer])} unique features ({len([x for x in counts[layer] if x > 1])} duplicated)\")\n",
    "    total_num_features += len(counts[layer])\n",
    "    values = sae_edges[layer]\n",
    "    for pos in sorted(list(values.keys())):\n",
    "        print(f\"  pos {position_map[pos]} num sae {len(values[pos])} min attr scaled {'{:.3f}'.format(1000*min([x[0] for x in values[pos]]))}\")\n",
    "print(f\"total num features {total_num_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "eb012272-a328-47ba-80af-deb7c7914024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "def get_batched_index_into(indices):\n",
    "    '''\n",
    "    given data that is [B,N,V] and indicies that are [B,N,K] with each index being an index into the V space\n",
    "    this gives you indexes you can use to access your values\n",
    "    '''\n",
    "    first_axis = []\n",
    "    second_axis = []\n",
    "    third_axis = []\n",
    "    B, _, _ = indices.size()\n",
    "    for b in range(B):\n",
    "        second, third = get_index_into(indices[b])\n",
    "        first_axis.append(torch.full(second.size(), fill_value=b, device=model.cfg.device))\n",
    "        second_axis.append(second)\n",
    "        third_axis.append(third)\n",
    "\n",
    "    return torch.cat(first_axis), torch.cat(second_axis), torch.cat(third_axis)\n",
    "\n",
    "def get_index_into(indices):\n",
    "    '''\n",
    "    given data that is [N,V] and indicies that are [N,K] with each index being an index into the V space\n",
    "    this gives you indexes you can use to access your values\n",
    "    '''\n",
    "    num_data, num_per_data = indices.size()\n",
    "    # we want\n",
    "    # [0,0,0,...,] num per data of these\n",
    "    # [1,1,1,...,] num per data of these\n",
    "    # ...\n",
    "    # [num_data-1, num_data-1, ...]\n",
    "    first_axis_index = torch.arange(num_data, dtype=torch.long).view(num_data, 1)*torch.ones([num_data, num_per_data], dtype=torch.long)\n",
    "    # now we flatten it so it has an index for each term aligned with our indices\n",
    "    first_axis_index = first_axis_index.flatten()\n",
    "    second_axis_index = indices.flatten()\n",
    "    return first_axis_index, second_axis_index\n",
    "global buffer\n",
    "buffer = None\n",
    "global features_by_layer\n",
    "def sae_hook(\n",
    "    x,\n",
    "    hook,\n",
    "    layer,\n",
    "):\n",
    "    # s is [B,L,E]\n",
    "    K = saes[layer].cfg.k\n",
    "    sae = saes[layer]\n",
    "    B,L,D = x.size()\n",
    "    uncorrupted_features = sae.encode(x)\n",
    "    top_acts, top_indices = uncorrupted_features.topk(K, sorted=False)\n",
    "    global buffer\n",
    "    if buffer is None:\n",
    "        buffer = torch.zeros(uncorrupted_features.size(), device=model.cfg.device)\n",
    "    buffer[:] = 0\n",
    "    \n",
    "    # zero everything except the top k\n",
    "    buffer[get_batched_index_into(top_indices)] = top_acts.flatten()\n",
    "    for feature in features_by_layer[layer]:\n",
    "        feature.records += [x.item() for x in buffer[:,feature.pos,feature.feature_i]]\n",
    "    # kernel can't handle doing all token positions at same time by default\n",
    "    # but if we make it think B*L is a single batch index it works fine\n",
    "    top_acts_flattened = top_acts.flatten(start_dim=0, end_dim=1)\n",
    "    top_indices_flattened = top_indices.flatten(start_dim=0, end_dim=1)\n",
    "    sae_out = sae.decode(top_acts_flattened, top_indices_flattened)\n",
    "    sae_out = sae_out.unflatten(dim=0, sizes=(B,L))\n",
    "    return sae_out\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SAEFeature:\n",
    "    \"\"\"Class for keeping track of an item in inventory.\"\"\"\n",
    "    layer: int\n",
    "    pos: int\n",
    "    feature_i: int\n",
    "    attr: float\n",
    "    records: list = field(default_factory=lambda: [])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.layer) + \" \" + str(self.pos) + \" \" + str(self.feature_i) + \" \" + str(self.attr)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "def parse_feature(feature_str):\n",
    "    layer, pos, pos_name, feature_i, attr = feature_str.split()\n",
    "    layer = int(layer)\n",
    "    pos = int(pos)\n",
    "    feature_i = int(feature_i)\n",
    "    attr = float(attr)\n",
    "    return SAEFeature(layer=layer, pos=pos, feature_i=feature_i, attr=attr)\n",
    "features = \"\"\"\n",
    "15 13 n4 15921 -1.4511811931733973\n",
    "15 13 n4 11839 -1.1555863783723908\n",
    "20 15 n5 27256 -1.0338027551188134\n",
    "20 13 n4 23228 -0.9890130006533582\n",
    "20 13 n4 2724 -0.9646386316744611\n",
    "15 13 n4 25771 -0.8934519871108932\n",
    "15 15 n5 26824 -0.8928024366832688\n",
    "20 13 n4 23731 -0.844180965796113\n",
    "15 15 n5 11839 -0.8429616270004772\n",
    "15 15 n5 27758 -0.8418691491242498\n",
    "14 15 n5 13971 -0.7999667014519218\n",
    "15 13 n4 17259 -0.7739449571818113\n",
    "20 15 n5 2724 -0.737606625945773\n",
    "15 15 n5 31021 -0.6951032060023863\n",
    "15 13 n4 7440 -0.6218199331851793\n",
    "15 15 n5 8113 -0.6123286176807596\n",
    "15 13 n4 8113 -0.6059157950221561\n",
    "20 15 n5 25369 -0.5720785861194599\n",
    "15 13 n4 6146 -0.5615684384829365\n",
    "20 13 n4 25369 -0.5487742855912074\n",
    "15 13 n4 26824 -0.5487118880992057\n",
    "15 13 n4 31021 -0.5430846622330137\n",
    "19 5 n2 30561 -0.5199790453916648\n",
    "20 15 n5 23228 -0.5164780604463886\n",
    "15 13 n4 28222 -0.5075037965434603\n",
    "14 13 n4 13971 -0.4765838644379983\n",
    "14 15 n5 32567 -0.4751656675944105\n",
    "11 13 n4 22965 -0.46325298480223864\n",
    "15 15 n5 6146 -0.45198706406517886\n",
    "19 15 n5 30740 -0.43911108660540776\n",
    "20 15 n5 29653 -0.43348417259403504\n",
    "15 15 n5 8935 -0.4290462978715368\n",
    "20 15 n5 8196 -0.40242351567940204\n",
    "15 15 n5 16138 -0.4004057846032083\n",
    "19 3 n1 30561 -0.3884095794055611\n",
    "20 15 n5 1899 -0.3876068123790901\n",
    "20 5 n2 27256 -0.3814757227519294\n",
    "15 13 n4 2344 -0.38085291545576183\n",
    "19 15 n5 30561 -0.37879287756368285\n",
    "15 15 n5 22790 -0.37808844797109487\n",
    "15 15 n5 2344 -0.3775725119630806\n",
    "15 13 n4 12167 -0.3703001577523537\n",
    "21 15 n5 7554 -0.34814969086801284\n",
    "21 13 n4 7554 -0.34735435343463905\n",
    "14 13 n4 32567 -0.3427205775587936\n",
    "15 15 n5 29892 -0.33304538365337066\n",
    "20 15 n5 23731 -0.33183777012163773\n",
    "15 15 n5 8649 -0.3311291775389691\n",
    "15 13 n4 2380 -0.32058422826230526\n",
    "15 15 n5 1349 -0.317370749762631\n",
    "15 15 n5 28979 -0.3154674572579097\n",
    "15 13 n4 30976 -0.3153745725285262\n",
    "20 13 n4 17612 -0.31191760893852916\n",
    "15 13 n4 22801 -0.3013547840891988\n",
    "12 15 n5 6008 -0.30098827754409285\n",
    "12 15 n5 4851 -0.2981748393503949\n",
    "20 13 n4 1899 -0.29404354887083173\n",
    "14 5 n2 13971 -0.29324889490089845\n",
    "15 13 n4 22790 -0.28884116747940425\n",
    "15 15 n5 17259 -0.2834519026146154\n",
    "19 5 n2 9076 -0.28338390760472976\n",
    "20 5 n2 23228 -0.2785688600561116\n",
    "15 13 n4 29892 -0.2779534184228396\n",
    "20 15 n5 24925 -0.27310544914143975\n",
    "15 13 n4 32240 -0.2724338702391833\n",
    "19 7 n3 30561 -0.27105611146544106\n",
    "20 13 n4 29653 -0.26252533006481826\n",
    "19 13 n4 30740 -0.26012522503879154\n",
    "20 5 n2 25369 -0.25853115250356495\n",
    "11 13 n4 19600 -0.2510134789190488\n",
    "20 15 n5 6758 -0.24530868871806888\n",
    "15 7 n3 8935 -0.24060688240570016\n",
    "11 13 n4 18719 -0.23991555671091191\n",
    "15 13 n4 28979 -0.23681864549871534\n",
    "20 15 n5 17612 -0.23483685902829166\n",
    "15 3 n1 25771 -0.22723528533242643\n",
    "15 15 n5 17920 -0.22721617101342417\n",
    "20 15 n5 6986 -0.22323725407477468\n",
    "14 15 n5 28831 -0.22306419239612296\n",
    "15 15 n5 22801 -0.22277752275113016\n",
    "20 13 n4 24925 -0.22063778418123547\n",
    "15 3 n1 8113 -0.21664783913001884\n",
    "11 15 n5 19600 -0.21422454587991524\n",
    "16 15 n5 19800 -0.21313862562237773\n",
    "20 13 n4 27256 -0.2103884415628272\n",
    "15 7 n3 26824 -0.208646113776922\n",
    "14 5 n2 32567 -0.20791079929767875\n",
    "20 15 n5 10083 -0.20755451168952277\n",
    "11 15 n5 18719 -0.20682069531903835\n",
    "15 15 n5 30976 -0.20519282953318907\n",
    "15 15 n5 10252 -0.19859311032632831\n",
    "20 5 n2 8455 -0.19858658533848939\n",
    "20 13 n4 3156 -0.18876934882428031\n",
    "20 5 n2 2724 -0.1882108402205631\n",
    "20 13 n4 6986 -0.18641833098081406\n",
    "20 3 n1 25369 -0.18377804876945447\n",
    "20 13 n4 1672 -0.18313410242262762\n",
    "20 7 n3 17612 -0.17767891606126796\n",
    "14 3 n1 13971 -0.177633166378655\n",
    "20 13 n4 31901 -0.17577282109414227\n",
    "15 5 n2 27758 -0.1747781486083113\n",
    "15 7 n3 2380 -0.174283399428532\n",
    "15 15 n5 9746 -0.1737501583957055\n",
    "20 3 n1 29653 -0.17337747645069612\n",
    "15 5 n2 26824 -0.16980820387470885\n",
    "15 13 n4 1349 -0.16958206321578473\n",
    "15 3 n1 2344 -0.16653591250360478\n",
    "20 13 n4 15013 -0.16628824583312962\n",
    "19 15 n5 27888 -0.16485086461761966\n",
    "14 13 n4 28831 -0.16269497356734064\n",
    "15 15 n5 25903 -0.15941790843498893\n",
    "20 15 n5 1672 -0.15898824847317883\n",
    "21 7 n3 6419 -0.15865897008188767\n",
    "15 15 n5 3888 -0.1581162586571736\n",
    "20 7 n3 8455 -0.1577744372516463\n",
    "19 13 n4 9076 -0.15575948629702907\n",
    "15 15 n5 25771 -0.15522614054498263\n",
    "1 15 n5 25764 -0.15266994523699395\n",
    "15 15 n5 12167 -0.15147657443594653\n",
    "15 15 n5 15921 -0.15116392161144176\n",
    "15 3 n1 31021 -0.1500888324371772\n",
    "20 3 n1 23228 -0.14868561428011162\n",
    "11 5 n2 19600 -0.147709660937835\n",
    "15 13 n4 25903 -0.1476323436727398\n",
    "15 15 n5 26556 -0.1475745009338425\n",
    "14 7 n3 13971 -0.1464709811261855\n",
    "20 13 n4 10083 -0.14522044641853427\n",
    "21 5 n2 7554 -0.14491799040115438\n",
    "11 7 n3 19600 -0.1438251133004087\n",
    "20 7 n3 27256 -0.14189712883853645\n",
    "15 3 n1 28222 -0.14156844822718995\n",
    "15 7 n3 27758 -0.13967746320849983\n",
    "20 13 n4 6758 -0.1395779878639587\n",
    "16 5 n2 8413 -0.1384436698135687\n",
    "15 15 n5 15762 -0.1374775571275677\n",
    "15 5 n2 25771 -0.1365963689131604\n",
    "14 7 n3 28831 -0.13502024069566687\n",
    "18 13 n4 24113 -0.13279320938272576\n",
    "20 3 n1 23731 -0.13276870549452724\n",
    "20 3 n1 24925 -0.12945301840591128\n",
    "15 5 n2 11839 -0.12674832851189421\n",
    "20 15 n5 1336 -0.12278529405011795\n",
    "20 3 n1 1899 -0.12241411584545858\n",
    "20 7 n3 6758 -0.12176360644843953\n",
    "20 15 n5 21539 -0.1217278007643472\n",
    "15 13 n4 9187 -0.11823179898783565\n",
    "20 15 n5 13117 -0.11767573590623215\n",
    "15 7 n3 31021 -0.11718380186539434\n",
    "11 7 n3 18719 -0.11692520580254495\n",
    "10 7 n3 11071 -0.11588574545021402\"\"\".strip()\n",
    "features = [parse_feature(line.strip()) for line in features.split(\"\\n\")]\n",
    "features = []\n",
    "\n",
    "for pos, feats in sae_edges[15].items():\n",
    "    features += [SAEFeature(layer=15, pos=pos, feature_i=feature_i, attr=attr) for (attr, feature_i) in feats]\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "def forward_check_features(data, features, batch_size):\n",
    "    \n",
    "    global features_by_layer\n",
    "\n",
    "    features_by_layer = defaultdict(lambda: [])\n",
    "    for feature in features:\n",
    "        feature.records = []\n",
    "        features_by_layer[feature.layer].append(feature)\n",
    "\n",
    "    # only bother with SAE on the layers we are checking\n",
    "    layers_to_apply_sae = sorted(list(features_by_layer.keys()))\n",
    "    hooks = [(f'blocks.{layer}.hook_out_proj', partial(sae_hook, layer=layer)) for layer in layers_to_apply_sae]\n",
    "\n",
    "    DATA_LEN = data.size()[0]\n",
    "    for batch_start in tqdm(list(range(0, DATA_LEN, batch_size))):\n",
    "        batch_end = min(DATA_LEN, batch_start+batch_size)\n",
    "        data_batch = data[batch_start:batch_end]\n",
    "        _ = model.run_with_hooks(input=data_batch, fwd_hooks=hooks, fast_ssm=True, fast_conv=True)\n",
    "    \n",
    "\n",
    "import acdc.data.ioi\n",
    "\n",
    "with open(\"/home/dev/mamba_interp/MoreNames.txt\", \"r\") as f:\n",
    "    all_names = [x.strip() for x in f.read().split(\"\\n\") if len(x.strip()) > 0]\n",
    "    # regenerate names, but more\n",
    "    acdc.data.ioi.NAMES = sorted(list(set(all_names)))\n",
    "    acdc.data.ioi.good_names = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356dbaff-a2a4-43ea-ad0d-627780e98e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = make_data(num_patching_pairs=50000, patching=\"all\", template_i=0, seed=24, valid_seed=23)\n",
    "forward_check_features(data=data.data, features=features, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07137acd-1cb3-4780-90b8-dfe5b516def2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"features_layer_15_with_collected_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e030af57-02a7-4520-947c-9dc4da9b0191",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'SAEFeature' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures_all_tokens_layer_15_with_collected_data.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m features \u001b[38;5;241m=\u001b[39m modifiedFeatures\n\u001b[1;32m      4\u001b[0m data\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data_for_all_tokens\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'SAEFeature' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "with open(\"features_all_tokens_layer_15_with_collected_data.pkl\", \"rb\") as f:\n",
    "    features = pickle.load(f)\n",
    "features = modifiedFeatures\n",
    "data.data = data_for_all_tokens\n",
    "global template_to_i\n",
    "template_to_i = {}\n",
    "\n",
    "global all_templates\n",
    "all_templates = []\n",
    "\n",
    "def extract_template(data_point):\n",
    "    a,b,c,d,e = data_point[3], data_point[5], data_point[7], data_point[13], data_point[15]\n",
    "    lookup = {}\n",
    "    template = \"\"\n",
    "    order = 'ABCDEF'\n",
    "    order_ind = 0\n",
    "    for name in [a,b,c,d,e]:\n",
    "        if not name in lookup:\n",
    "            lookup[name] = order[order_ind]\n",
    "            order_ind += 1\n",
    "        template += lookup[name]\n",
    "    global all_templates\n",
    "    \n",
    "    if not template in template_to_i.keys():\n",
    "        all_templates.append(template)\n",
    "        template_to_i[template] = len(all_templates)-1\n",
    "    return template_to_i[template]\n",
    "'''\n",
    "from tqdm import tqdm\n",
    "data_to_template = []\n",
    "for d in tqdm(range(data.data.size()[0])):\n",
    "    data_to_template.append(extract_template(data.data[d]))\n",
    "'''\n",
    "def get_name_counts(feature):\n",
    "    name_counts = {}\n",
    "    DATA_LEN = len(feature.records)\n",
    "    records_tensor = torch.tensor(feature.records)\n",
    "    non_zero_indices = torch.arange(DATA_LEN)[records_tensor!=0]\n",
    "    non_zero_tokens = data.data[non_zero_indices,feature.pos].cpu()\n",
    "    non_zero_records = records_tensor[non_zero_indices]\n",
    "    name_tokens = torch.unique(non_zero_tokens)\n",
    "    for name_token in name_tokens:\n",
    "        name_str = model.to_str_tokens(name_token.view(1,1))[0]\n",
    "        name_counts[name_str] = non_zero_records[non_zero_tokens==name_token.item()]\n",
    "    #for t,c in template_counts.items():\n",
    "    #    print(f\" template {t} with count {torch.mean(torch.tensor(c)).item()}\")\n",
    "    name_counts = sorted(list(name_counts.items()), key=lambda x: -torch.mean(x[1]).item())\n",
    "    return name_counts\n",
    "    #for n,c in name_counts[:100]:\n",
    "    #    print(f\" name {n} with avg {torch.mean(c).item()} min {torch.min(c).item()} max {torch.max(c).item()}\")\n",
    "\n",
    "#names = sorted(list(acdc.data.ioi.good_names))\n",
    "names = [x for (i,x) in spaceThings if len(x.strip()) > 0]\n",
    "NUM_NAMES = len(names)\n",
    "#name_to_i = dict([(\" \" + name, i) for (i, name) in enumerate(names)])\n",
    "name_to_i = dict([(name, i) for (i, name) in enumerate(names)])\n",
    "\n",
    "def get_name_vector(feature, feat_type):\n",
    "    name_vec = torch.zeros(NUM_NAMES)\n",
    "    for name,counts in get_name_counts(feature):\n",
    "        if feat_type == 'mean':\n",
    "            name_vec[name_to_i[name]] = torch.mean(counts)\n",
    "        elif feat_type == 'min':\n",
    "            name_vec[name_to_i[name]] = torch.min(counts)\n",
    "        elif feat_type == 'max':\n",
    "            name_vec[name_to_i[name]] = torch.max(counts)\n",
    "            \n",
    "    return name_vec\n",
    "\n",
    "#vecs = []\n",
    "#for feature in features:\n",
    "#    get_name_vector(feature)\n",
    "    \n",
    "\n",
    "features_sorted_by_feat_i = defaultdict(lambda: [])\n",
    "for feature in features:\n",
    "    features_sorted_by_feat_i[feature.feature_i].append(feature)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "90596b9e-d32f-478a-9c04-77d8b277e6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 307/307 [06:20<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "first_letter_to_name = defaultdict(lambda: [])\n",
    "for (name_i, name) in enumerate(names):\n",
    "    if len(name.strip()) > 0:\n",
    "        first_letter_to_name[name.strip()[0].lower()].append((name, name_i))\n",
    "#for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "#    first_letter_to_name[letter] = [(name, name_i) for (name_i, name) in enumerate(names) if len(name.strip().lower()) > 0 and name.strip().lower()[0] == letter]\n",
    "\n",
    "contains_letter_to_name = {}\n",
    "for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "    contains_letter_to_name[letter] = [(name, name_i) for (name_i, name) in enumerate(names) if letter in name.strip().lower()]\n",
    "\n",
    "def detect_single_letter(feature):\n",
    "    name_vec = get_name_vector(feature, 'mean')\n",
    "    top_inds = torch.argsort(-name_vec)\n",
    "    top_letter = names[top_inds[0]].strip()[0].lower()\n",
    "    num_matched = 0\n",
    "    for ind in top_inds[:200]:\n",
    "        if top_letter != names[ind].strip()[0].lower():\n",
    "            break\n",
    "        else:\n",
    "            num_matched += 1\n",
    "    #print(\"num matched\", num_matched, \"num names of that\", len(first_letter_to_name[top_letter]))\n",
    "    return top_letter, num_matched / float(len(first_letter_to_name[top_letter])), len(first_letter_to_name[top_letter])\n",
    "\n",
    "\n",
    "def detect_contains_letter(feature):\n",
    "    name_vec = get_name_vector(feature, 'mean')\n",
    "    top_inds = torch.argsort(-name_vec)\n",
    "    top_letter = names[top_inds[0]].strip()[0].lower()\n",
    "    num_matched = 0\n",
    "    for ind in top_inds[:200]:\n",
    "        if top_letter != names[ind].strip()[0].lower():\n",
    "            break\n",
    "        else:\n",
    "            num_matched += 1\n",
    "    #print(\"num matched\", num_matched, \"num names of that\", len(first_letter_to_name[top_letter]))\n",
    "    return top_letter, num_matched / float(len(first_letter_to_name[top_letter])), len(first_letter_to_name[top_letter])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if we are the only letter that is seen 0.85 of them after seeing two letters, that's good enough\n",
    "def improved_detect_first_letter(feature):\n",
    "    first_letter_info = list_first_letter_info(feature)\n",
    "    for i in range(1,4):\n",
    "        i_good_letters = []\n",
    "        for letter, freqdict in first_letter_info.items():\n",
    "            val = max([freqdict[k] for k in range(1,i+1)])\n",
    "            if val > 0.85:\n",
    "                i_good_letters.append((letter, val))\n",
    "        if len(i_good_letters) == 1:\n",
    "            letter, confidance = i_good_letters[0]\n",
    "            return letter, i, confidance, len(first_letter_to_name[letter])\n",
    "        elif len(i_good_letters) > 1:\n",
    "            return None, 0, 0.0, 0\n",
    "    return None, 0, 0.0, 0\n",
    "\n",
    "# gives a dict with [letter][ind]\n",
    "# where ind is the number of distinct letters seen so far\n",
    "# and the value of the dict[letter][ind] is the proportion of total\n",
    "# names seen of that letter so far (for ind or less)\n",
    "def list_first_letter_info(feature):\n",
    "    \n",
    "    name_vec = get_name_vector(feature, 'mean')\n",
    "    top_inds = torch.argsort(-name_vec)\n",
    "    top_letter = names[top_inds[0]].strip()[0].lower()\n",
    "    frequencies = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    for ind in top_inds[:500]:\n",
    "        letter = names[ind].strip()[0].lower()\n",
    "        num_letters = len(frequencies)\n",
    "        if not letter in frequencies:\n",
    "            num_letters += 1\n",
    "        frequencies[letter][num_letters] += 1\n",
    "        if num_letters > 3:\n",
    "            break\n",
    "    resultProportions = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    for letter, freqdict in frequencies.items():\n",
    "        nums = sorted(list(freqdict.keys()))\n",
    "        total = 0\n",
    "        for n in nums:\n",
    "            total += freqdict[n]\n",
    "            resultProportions[letter][n] = total / float(len(first_letter_to_name[letter]))\n",
    "    return resultProportions\n",
    "\n",
    "\n",
    "single_letter_feats = defaultdict(lambda: defaultdict(lambda: []))\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for feat_i, feats in tqdm(list(features_sorted_by_feat_i.items())):\n",
    "    is_single_letter = True\n",
    "    have_any = False\n",
    "    max_num_seen = 0\n",
    "    for feat in feats:\n",
    "        if position_map[feat.pos][0] == 'n':\n",
    "            letter, num_seen, rating, num_of_that_letter = improved_detect_first_letter(feat)\n",
    "            max_num_seen = max(num_seen, max_num_seen)\n",
    "            if rating < 0.85 or num_of_that_letter < 4:\n",
    "                is_single_letter = False\n",
    "            else:\n",
    "                have_any = True # needed incase they are all non name tokens\n",
    "        else:\n",
    "            print(f\"warning, feature {feat_i} has non name pos {position_map[feat.pos]} (all pos are {[position_map[f.pos] for f in feats]})\")\n",
    "    if is_single_letter and have_any:\n",
    "        single_letter_feats[max_num_seen][feat_i] = feats\n",
    "\n",
    "for num_seen in sorted(list(single_letter_feats.keys())):\n",
    "    print(f\"num seen {num_seen} single letter {len(single_letter_feats[num_seen])} num total {len(features_sorted_by_feat_i)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3859aa05-f317-4091-a873-cfdfea5774b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "letters = defaultdict(lambda: 0)\n",
    "for k in single_letter_feats.keys():\n",
    "    print(f\"k of {k}\")\n",
    "    for feat_i, feats in single_letter_feats[k].items():\n",
    "        print(f\"feature {feat_i}\")\n",
    "        letter = None\n",
    "        for feat in feats:\n",
    "            if position_map[feat.pos][0] == 'n':\n",
    "                letter = detect_single_letter(feat)\n",
    "                print(position_map[feat.pos], letter)\n",
    "                letter = letter\n",
    "            else:\n",
    "                print(f\"warning, feature {feat_i} has non name pos {position_map[feat.pos]} (all pos are {[position_map[f.pos] for f in feats]})\")\n",
    "        letters[letter[0]] += 1\n",
    "        continue\n",
    "        \n",
    "        feat_vecs = [get_name_vector(feat, 'mean') for feat in feats]\n",
    "        diffs = torch.zeros(len(feats), len(feats))\n",
    "        for i,featv1 in enumerate(feat_vecs):\n",
    "            for j,featv2 in enumerate(feat_vecs):\n",
    "                diffs[i,j] = torch.mean(torch.abs(featv1-featv2))\n",
    "        labels = [position_map[feat.pos] for feat in feats]\n",
    "        imshow(diffs, x=labels, y=labels, font_size=9)\n",
    "        \n",
    "        avg_vec = torch.stack(feat_vecs).mean(dim=0)\n",
    "        min_vec = torch.stack([get_name_vector(feat, 'min') for feat in feats]).min(dim=0).values\n",
    "        max_vec = torch.stack([get_name_vector(feat, 'max') for feat in feats]).max(dim=0).values\n",
    "        sorted_names = torch.argsort(-avg_vec)\n",
    "        #print(avg_vec, min_vec, max_vec, sorted_names)\n",
    "        for name_i in sorted_names[:50]:\n",
    "            print(name_i)\n",
    "            print(f\" name {names[name_i]} with avg {avg_vec[name_i]} min {min_vec[name_i]} max {max_vec[name_i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7d6b992b-9ab0-443e-8ecd-996cf76cda69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'p': 1,\n",
       "             's': 2,\n",
       "             't': 3,\n",
       "             'l': 3,\n",
       "             'v': 3,\n",
       "             'n': 2,\n",
       "             'a': 1,\n",
       "             'b': 1,\n",
       "             'i': 2,\n",
       "             'r': 4,\n",
       "             'w': 1,\n",
       "             'z': 2})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac3976b-bdcc-45a9-b65a-2a57f5185bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d1181a-c802-4681-a425-095056404857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b3ca7-84d6-4583-8ef7-1d39243cdaaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def pretty_print_list_first_letter_info(info):\n",
    "    for k,v in info.items():\n",
    "        print(k, sorted(list(v.items())))\n",
    "\n",
    "for feat_i, feats in features_sorted_by_feat_i.items():\n",
    "    if any([position_map[feat.pos][0] != 'n' for feat in feats]):\n",
    "        print(f\"warning, feature {feat_i} has non name poses, all pos are {[position_map[f.pos] for f in feats]})\")\n",
    "        continue\n",
    "    found = False\n",
    "    for k in single_letter_feats.keys():\n",
    "        if feat_i in single_letter_feats[k]:\n",
    "            print(feat_i)\n",
    "            print(position_map[feats[0].pos])\n",
    "            print(f\"feature {feat_i} is {k} seen, single letter feat \", detect_single_letter(feats[0]))\n",
    "            found = True\n",
    "            break\n",
    "    if found:\n",
    "        continue\n",
    "        \n",
    "    for feat in feats:\n",
    "        if position_map[feat.pos][0] == 'n':\n",
    "            print(position_map[feat.pos], detect_single_letter(feat))\n",
    "            pretty_print_list_first_letter_info(list_first_letter_info(feat))\n",
    "            print(improved_first_letter(feat))\n",
    "    \n",
    "    feat_vecs = [get_name_vector(feat, 'mean') for feat in feats]\n",
    "    diffs = torch.zeros(len(feats), len(feats))\n",
    "    for i,featv1 in enumerate(feat_vecs):\n",
    "        for j,featv2 in enumerate(feat_vecs):\n",
    "            diffs[i,j] = torch.mean(torch.abs(featv1-featv2))\n",
    "    labels = [position_map[feat.pos] for feat in feats]\n",
    "    imshow(diffs, x=labels, y=labels, font_size=9)\n",
    "    \n",
    "    avg_vec = torch.stack(feat_vecs).mean(dim=0)\n",
    "    min_vec = torch.stack([get_name_vector(feat, 'min') for feat in feats]).min(dim=0).values\n",
    "    max_vec = torch.stack([get_name_vector(feat, 'max') for feat in feats]).max(dim=0).values\n",
    "    sorted_names = torch.argsort(-avg_vec)\n",
    "    #print(avg_vec, min_vec, max_vec, sorted_names)\n",
    "    for name_i in sorted_names[:50]:\n",
    "        print(name_i)\n",
    "        print(f\" name {names[name_i]} with avg {avg_vec[name_i]} min {min_vec[name_i]} max {max_vec[name_i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cb690050-9172-472f-98ae-276c80078f22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4ee061ca-f81b-4bef-b512-f89ff4c145df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "92d249cf-8819-442a-bcaa-9cf6f4a904fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9337a227-05ab-40f3-a4e2-f8dbcacec903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2416, ' writ')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "baf2351f-7d6c-4423-ab4a-a8f7ba3b8209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' writ'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(torch.tensor([2416]).view(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85326f0c-caed-42d0-964c-20b7ef07dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model.to_str_tokens(torch.arange(model.tokenizer.vocab_size))\n",
    "spaceThings = [(i, x) for (i, x) in enumerate(h) if x[0] == ' ' and len(x.strip()) > 0]\n",
    "modifiedFeatures = []\n",
    "for feature in features:\n",
    "    modifiedFeatures.append(SAEFeature(layer=feature.layer, pos=3, feature_i=feature.feature_i, attr=feature.attr))\n",
    "prefix = data.data[0][:3].view(1,-1)\n",
    "new_data_toks = torch.tensor([tok for (tok,s) in spaceThings], device=model.cfg.device)\n",
    "data_for_all_tokens = torch.cat([prefix.repeat((len(new_data_toks),1)), new_data_toks.view(-1,1)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fed02952-2e7a-436b-a759-3d8a0bcc63db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 144/144 [03:53<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "['<|endoftext|>',\n",
    " 'Then',\n",
    " ',',\n",
    "'''\n",
    "\n",
    "forward_check_features(data=data_for_all_tokens, features=modifiedFeatures, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96586914-aa8b-4ed5-a4f7-53e21256f1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using patching format\n",
      "ABC AB C\n",
      "ABC AC B\n",
      "\n",
      "ABC AB C\n",
      "ABC CB A\n",
      "\n",
      "ABC AB C\n",
      "ABD AB D\n",
      "\n",
      "ABC AC B\n",
      "ABC BC A\n",
      "\n",
      "ABC AC B\n",
      "ADC AC D\n",
      "\n",
      "ABC BA C\n",
      "ABC BC A\n",
      "\n",
      "ABC BA C\n",
      "ABC CA B\n",
      "\n",
      "ABC BA C\n",
      "ABD BA D\n",
      "\n",
      "ABC BC A\n",
      "DBC BC D\n",
      "\n",
      "ABC CA B\n",
      "ABC CB A\n",
      "\n",
      "ABC CA B\n",
      "ADC CA D\n",
      "\n",
      "ABC CB A\n",
      "DBC CB D\n",
      "\n",
      "using templates\n",
      "Then, [NAME], [NAME] and [NAME] went to the [PLACE]. [NAME] and [NAME] gave a [OBJECT] to\n",
      "with name positions (2, 4, 6, 12, 14)\n",
      "['<|endoftext|>', 'Then', ',', ' Olivia', ',', ' Ian', ' and', ' Aaron', ' went', ' to', ' the', ' restaurant', '.', ' Aaron', ' and', ' Olivia', ' gave', ' a', ' computer', ' to']\n",
      "['<|endoftext|>', 'Then', ',', ' Olivia', ',', ' Ian', ' and', ' Aaron', ' went', ' to', ' the', ' restaurant', '.', ' Aaron', ' and', ' Ian', ' gave', ' a', ' computer', ' to']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 250/250 [03:05<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "@dataclass\n",
    "class SAEFeature:\n",
    "    \"\"\"Class for keeping track of an item in inventory.\"\"\"\n",
    "    layer: int\n",
    "    pos: int\n",
    "    feature_i: int\n",
    "    attr: float\n",
    "    records: list = field(default_factory=lambda: [])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.layer) + \" \" + str(self.pos) + \" \" + str(self.feature_i) + \" \" + str(self.attr)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "from tqdm import tqdm\n",
    "data = make_data(num_patching_pairs=2, patching=\"all\", template_i=0, seed=24, valid_seed=23)\n",
    "h = model.to_str_tokens(torch.arange(model.tokenizer.vocab_size))\n",
    "#spaceThings = [(i, x) for (i, x) in enumerate(h) if x[0] == ' ' and len(x.strip()) > 0]\n",
    "spaceThings = [(i, x) for (i, x) in enumerate(h) if len(x.strip()) > 0]\n",
    "prefix = data.data[0][:1].view(1,-1)\n",
    "new_data_toks = torch.tensor([tok for (tok,s) in spaceThings], device=model.cfg.device)\n",
    "data_for_all_tokens = torch.cat([prefix.repeat((len(new_data_toks),1)), new_data_toks.view(-1,1)], dim=1)\n",
    "data.data = data_for_all_tokens\n",
    "with open(\"features_all_tokens_layer_15_with_collected_data.pkl\", \"rb\") as f:\n",
    "    features = pickle.load(f)\n",
    "\n",
    "DATA_LEN = data.data.size()[0]\n",
    "\n",
    "TOP_K = 40\n",
    "batch_size = 200\n",
    "new_data = []\n",
    "for batch_start in tqdm(list(range(0, DATA_LEN, batch_size))):\n",
    "    batch_end = min(DATA_LEN, batch_start+batch_size)\n",
    "    data_batch = data.data[batch_start:batch_end]\n",
    "    # [B,L,V]\n",
    "    logits = model(input=data_batch, fast_ssm=True, fast_conv=True)\n",
    "    inds = torch.argsort(-logits[:,-1,:], dim=1)\n",
    "    prs = torch.softmax(logits[:,-1,:], dim=1)\n",
    "    for b in range(batch_end-batch_start):\n",
    "        for t in range(TOP_K):\n",
    "            pr = prs[b,inds[b,t]]\n",
    "            if pr > 0.05:\n",
    "                new_data.append(torch.concatenate([data_batch[b],inds[b,t:t+1]]))\n",
    "    # [B,20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38933cd5-e86f-4196-b6a0-89a50f008336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([112552, 3])\n",
      "['<|endoftext|>', ' antagon', 'ism']\n",
      "['<|endoftext|>', ' citiz', 'in']\n",
      "['<|endoftext|>', ' stalled', '\\n']\n",
      "['<|endoftext|>', ' ammonium', ' nitrate']\n",
      "['<|endoftext|>', 'ouble', 'ts']\n",
      "['<|endoftext|>', ' Waste', 'water']\n",
      "['<|endoftext|>', ' appeal', '\\n']\n",
      "['<|endoftext|>', 'Paper', 'back']\n",
      "['<|endoftext|>', 'ividual', ' and']\n",
      "['<|endoftext|>', 'ulfide', '\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 563/563 [05:47<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([202620, 4])\n",
      "['<|endoftext|>', '', '', '']\n",
      "['<|endoftext|>', ' ', '', '']\n",
      "['<|endoftext|>', ' file', '1', '.']\n",
      "['<|endoftext|>', ' 297', '*', 'w']\n",
      "['<|endoftext|>', ' spo', 'ilt', ' by']\n",
      "['<|endoftext|>', '}}}(', '1', ')']\n",
      "['<|endoftext|>', 'Browser', '-', 'based']\n",
      "['<|endoftext|>', ' recurring', '?', '  ']\n",
      "['<|endoftext|>', ' ', '', '']\n",
      "['<|endoftext|>', ' Lots', ' of', ' people']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1014/1014 [10:34<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([389372, 5])\n",
      "['<|endoftext|>', 'Were', ' there', ' any']\n",
      "['<|endoftext|>', ' in', ' -', '4']\n",
      "['<|endoftext|>', ' decor', '.', 'css']\n",
      "['<|endoftext|>', 'zag', 're', 'ba']\n",
      "['<|endoftext|>', ' BUSINESS', ' RE', 'PORT']\n",
      "['<|endoftext|>', ' Feld', 'k', 'irc']\n",
      "['<|endoftext|>', ' Eigen', 'values', ' and']\n",
      "['<|endoftext|>', ' van', ' der', ' Wa']\n",
      "['<|endoftext|>', 'Tg', 'A', 'i']\n",
      "['<|endoftext|>', ' Ref', 'erral', ' of']\n",
      "['<|endoftext|>', ' semiconductor', ' devices', ' such']\n",
      "['<|endoftext|>', ' assignment', ':', '\\n']\n",
      "['<|endoftext|>', ' Mend', 'elian', ' inheritance']\n",
      "['<|endoftext|>', 'bf', 'c', '_']\n",
      "['<|endoftext|>', '---------------------------------------------------', '\\n', '--']\n",
      "['<|endoftext|>', ' Derby', ' (', 'film']\n",
      "['<|endoftext|>', 'EV', 'ERY', 'ONE']\n",
      "['<|endoftext|>', ' affinity', ':', ' {']\n",
      "['<|endoftext|>', '#', 'include', ' <']\n",
      "['<|endoftext|>', 'pmb', '\\n', '\\n']\n",
      "['<|endoftext|>', ' attribut', 'es', ' to']\n",
      "['<|endoftext|>', ' MAKE', ' IT', ' RIGHT']\n",
      "['<|endoftext|>', ' check', ':', '\\n']\n",
      "['<|endoftext|>', ' soundtrack', ' of', ' the']\n",
      "['<|endoftext|>', ' Leigh', '-', 'on']\n",
      "['<|endoftext|>', ' Conduct', 'ors', ' in']\n",
      "['<|endoftext|>', '', '', '']\n",
      "['<|endoftext|>', '', '', '']\n",
      "['<|endoftext|>', '', '', '']\n",
      "['<|endoftext|>', ' know', ' -', '4']\n",
      "['<|endoftext|>', ' gent', 'ile', '\\n']\n",
      "['<|endoftext|>', ' Method', '\\n', '=======']\n",
      "['<|endoftext|>', ' <', 'html', '><']\n",
      "['<|endoftext|>', ' 146', '\\n', 'What']\n",
      "['<|endoftext|>', '', '', '']\n",
      "['<|endoftext|>', ' immunoprecip', '.', '\\n']\n",
      "['<|endoftext|>', 'aurus', 'aurus', '.']\n",
      "['<|endoftext|>', 'Contract', 'ile', ' function']\n",
      "['<|endoftext|>', ' invoked', '\\n', '<|endoftext|>']\n",
      "['<|endoftext|>', ' y', '.', '\\n']\n",
      "['<|endoftext|>', 'sched', 'ul', 'ers']\n",
      "['<|endoftext|>', ' 32', ' -', ' (']\n",
      "['<|endoftext|>', 'fac', 'el', 'ift']\n",
      "['<|endoftext|>', ' gy', 'res', '.']\n",
      "['<|endoftext|>', ' licensed', ' to', ' the']\n",
      "['<|endoftext|>', '0038', '\\n', '\\n']\n",
      "['<|endoftext|>', '.\"\"\"', '\\n\\n', '\\n']\n",
      "['<|endoftext|>', '', '', '']\n",
      "['<|endoftext|>', 'reen', 'act', '.']\n",
      "['<|endoftext|>', '2019', '-', '06']\n",
      "['<|endoftext|>', ' 104', '.', ' What']\n",
      "['<|endoftext|>', ' infring', 'ing', '\\n']\n",
      "['<|endoftext|>', 'rive', ' g', ' to']\n",
      "['<|endoftext|>', 'chron', 'ological', ' order']\n",
      "['<|endoftext|>', '010', '\\n', 'Let']\n",
      "['<|endoftext|>', 'Thanks', ' to', ' all']\n",
      "['<|endoftext|>', ' BET', 'S', '\\n']\n",
      "['<|endoftext|>', 'Analy', 'sts', ' say']\n",
      "['<|endoftext|>', ' Eq', ':', ' \\\\']\n",
      "['<|endoftext|>', ' 433', ',', ' -']\n",
      "['<|endoftext|>', ' shouting', ',', ' \"']\n",
      "['<|endoftext|>', 'doesn', \"'t\", ' have']\n",
      "['<|endoftext|>', 'Ty', 'rosine', ' phosphorylation']\n",
      "['<|endoftext|>', ' ', '', 'e']\n",
      "['<|endoftext|>', '', '', '']\n",
      "['<|endoftext|>', ' Thanksgiving', ' Day', ' (']\n",
      "['<|endoftext|>', ' treacher', 'y', '.']\n",
      "['<|endoftext|>', ' dubbed', ' \"', 'The']\n",
      "['<|endoftext|>', 'rounds', ' of', ' the']\n",
      "['<|endoftext|>', ' obstructive', ' airway', ' symptoms']\n",
      "['<|endoftext|>', ' postnatal', ' day', ' 7']\n",
      "['<|endoftext|>', ' Resp', 'ite', ',']\n",
      "['<|endoftext|>', ' stop', '\\n', '\\n']\n",
      "['<|endoftext|>', ' cooperation', ' in', ' the']\n",
      "['<|endoftext|>', ' Kai', '-', 'Ch']\n",
      "['<|endoftext|>', 'Development', ' and', ' validation']\n",
      "['<|endoftext|>', ' expand', ' (-', '1']\n",
      "['<|endoftext|>', '', 'd', '\\n']\n",
      "['<|endoftext|>', ' v', ' v', 's']\n",
      "['<|endoftext|>', '', '', '']\n",
      "['<|endoftext|>', ' usual', ' =', ' (']\n",
      "['<|endoftext|>', 'mittee', ' for', ' the']\n",
      "['<|endoftext|>', 'erably', ' you', ' would']\n",
      "['<|endoftext|>', 'rystall', 'ization', '\\n']\n",
      "['<|endoftext|>', '', '', '']\n",
      "['<|endoftext|>', ' stop', '\\n', '\\n']\n",
      "['<|endoftext|>', 'hour', ':', '\\n']\n",
      "['<|endoftext|>', ' hors', 'erad', 'ish']\n",
      "['<|endoftext|>', ' 477', '.', ' What']\n",
      "['<|endoftext|>', ' Advent', 'um', ' (']\n",
      "['<|endoftext|>', ' appropriately', '.', '\\n']\n",
      "['<|endoftext|>', ' contraception', '\\n', '\\n']\n",
      "['<|endoftext|>', 'ANT', 'ON', 'IO']\n",
      "['<|endoftext|>', ' subtracting', ' 1', '?']\n",
      "['<|endoftext|>', ' Indiana', ' State', ' University']\n",
      "['<|endoftext|>', 'flu', 'ct', 'uating']\n",
      "['<|endoftext|>', ' ', '', '']\n",
      "['<|endoftext|>', 'WIDTH', ' =', ' 64']\n",
      "['<|endoftext|>', ' Athlet', 'es', ' who']\n",
      "['<|endoftext|>', ' cardiomy', 'ocyte', ' apoptosis']\n"
     ]
    }
   ],
   "source": [
    "data_for_all_tokens = torch.stack(new_data)\n",
    "print(data_for_all_tokens.size())\n",
    "\n",
    "'''\n",
    "data_for_all_tokens = torch.cat([prefix.repeat((len(new_data_toks),1)), new_data_toks.view(-1,1)], dim=1)\n",
    "data_for_all_tokens = data_for_all_tokens.repeat((TOP_K, 1))\n",
    "next_tok = torch.concatenate(next_tokens).view(-1,1)\n",
    "print(next_tok[:20])\n",
    "print(next_tokens[0][0])\n",
    "data_for_top_k_tokens = torch.cat([data_for_all_tokens, next_tok], dim=1)\n",
    "print(data_for_top_k_tokens.size())\n",
    "'''\n",
    "for i in range(10):\n",
    "    import random\n",
    "    ind = random.randint(0,data_for_all_tokens.size()[0])\n",
    "    print(model.to_str_tokens(data_for_all_tokens[ind]))\n",
    "\n",
    "\n",
    "\n",
    "DATA_LEN = data_for_all_tokens.size()[0]\n",
    "TOP_K = 20\n",
    "batch_size = 200\n",
    "new_data = []\n",
    "for batch_start in tqdm(list(range(0, DATA_LEN, batch_size))):\n",
    "    batch_end = min(DATA_LEN, batch_start+batch_size)\n",
    "    data_batch = data_for_all_tokens[batch_start:batch_end]\n",
    "    # [B,L,V]\n",
    "    logits = model(input=data_batch, fast_ssm=True, fast_conv=True)\n",
    "    inds = torch.argsort(-logits[:,-1,:], dim=1)\n",
    "    prs = torch.softmax(logits[:,-1,:], dim=1)\n",
    "    for b in range(batch_end-batch_start):\n",
    "        for t in range(TOP_K):\n",
    "            pr = prs[b,inds[b,t]]\n",
    "            if pr > 0.05:\n",
    "                new_data.append(torch.concatenate([data_batch[b],inds[b,t:t+1]]))\n",
    "\n",
    "data_for_all_tokens2 = torch.stack(new_data)\n",
    "print(data_for_all_tokens2.size())\n",
    "\n",
    "for i in range(10):\n",
    "    import random\n",
    "    ind = random.randint(0,data_for_all_tokens2.size()[0])\n",
    "    print(model.to_str_tokens(data_for_all_tokens2[ind]))\n",
    "\n",
    "DATA_LEN = data_for_all_tokens2.size()[0]\n",
    "TOP_K = 20\n",
    "batch_size = 200\n",
    "new_data = []\n",
    "for batch_start in tqdm(list(range(0, DATA_LEN, batch_size))):\n",
    "    batch_end = min(DATA_LEN, batch_start+batch_size)\n",
    "    data_batch = data_for_all_tokens2[batch_start:batch_end]\n",
    "    # [B,L,V]\n",
    "    logits = model(input=data_batch, fast_ssm=True, fast_conv=True)\n",
    "    inds = torch.argsort(-logits[:,-1,:], dim=1)\n",
    "    prs = torch.softmax(logits[:,-1,:], dim=1)\n",
    "    for b in range(batch_end-batch_start):\n",
    "        for t in range(TOP_K):\n",
    "            pr = prs[b,inds[b,t]]\n",
    "            if pr > 0.05:\n",
    "                new_data.append(torch.concatenate([data_batch[b],inds[b,t:t+1]]))\n",
    "\n",
    "data_for_all_tokens3 = torch.stack(new_data)\n",
    "print(data_for_all_tokens3.size())\n",
    "\n",
    "for i in range(100):\n",
    "    import random\n",
    "    ind = random.randint(0,data_for_all_tokens2.size()[0])\n",
    "    print(model.to_str_tokens(data_for_all_tokens2[ind]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f5e1c2e-d2fe-4e10-b084-15a8ce09a6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1947/1947 [1:22:09<00:00,  2.53s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features_sorted_by_feat_i = defaultdict(lambda: [])\n",
    "for feature in features:\n",
    "    new_feats = []\n",
    "    for pos in range(1,5):\n",
    "        feat1 = SAEFeature(layer=15, pos=pos, feature_i=feature.feature_i, attr=feature.attr)\n",
    "        new_feats.append(feat1)\n",
    "    features_sorted_by_feat_i[feature.feature_i] = new_feats\n",
    "\n",
    "new_modified_feats = []\n",
    "for f,feats in features_sorted_by_feat_i.items():\n",
    "    new_modified_feats += feats\n",
    "from functools import partial\n",
    "\n",
    "def get_batched_index_into(indices):\n",
    "    '''\n",
    "    given data that is [B,N,V] and indicies that are [B,N,K] with each index being an index into the V space\n",
    "    this gives you indexes you can use to access your values\n",
    "    '''\n",
    "    first_axis = []\n",
    "    second_axis = []\n",
    "    third_axis = []\n",
    "    B, _, _ = indices.size()\n",
    "    for b in range(B):\n",
    "        second, third = get_index_into(indices[b])\n",
    "        first_axis.append(torch.full(second.size(), fill_value=b, device=model.cfg.device))\n",
    "        second_axis.append(second)\n",
    "        third_axis.append(third)\n",
    "\n",
    "    return torch.cat(first_axis), torch.cat(second_axis), torch.cat(third_axis)\n",
    "\n",
    "def get_index_into(indices):\n",
    "    '''\n",
    "    given data that is [N,V] and indicies that are [N,K] with each index being an index into the V space\n",
    "    this gives you indexes you can use to access your values\n",
    "    '''\n",
    "    num_data, num_per_data = indices.size()\n",
    "    # we want\n",
    "    # [0,0,0,...,] num per data of these\n",
    "    # [1,1,1,...,] num per data of these\n",
    "    # ...\n",
    "    # [num_data-1, num_data-1, ...]\n",
    "    first_axis_index = torch.arange(num_data, dtype=torch.long).view(num_data, 1)*torch.ones([num_data, num_per_data], dtype=torch.long)\n",
    "    # now we flatten it so it has an index for each term aligned with our indices\n",
    "    first_axis_index = first_axis_index.flatten()\n",
    "    second_axis_index = indices.flatten()\n",
    "    return first_axis_index, second_axis_index\n",
    "global buffer\n",
    "buffer = None\n",
    "global features_by_layer\n",
    "def sae_hook(\n",
    "    x,\n",
    "    hook,\n",
    "    layer,\n",
    "):\n",
    "    # s is [B,L,E]\n",
    "    K = saes[layer].cfg.k\n",
    "    sae = saes[layer]\n",
    "    B,L,D = x.size()\n",
    "    uncorrupted_features = sae.encode(x)\n",
    "    top_acts, top_indices = uncorrupted_features.topk(K, sorted=False)\n",
    "    global buffer\n",
    "    if buffer is None:\n",
    "        buffer = torch.zeros(uncorrupted_features.size(), device=model.cfg.device)\n",
    "    buffer[:] = 0\n",
    "    \n",
    "    # zero everything except the top k\n",
    "    buffer[get_batched_index_into(top_indices)] = top_acts.flatten()\n",
    "    for feature in features_by_layer[layer]:\n",
    "        feature.records += [x.item() for x in buffer[:,feature.pos,feature.feature_i]]\n",
    "    # kernel can't handle doing all token positions at same time by default\n",
    "    # but if we make it think B*L is a single batch index it works fine\n",
    "    top_acts_flattened = top_acts.flatten(start_dim=0, end_dim=1)\n",
    "    top_indices_flattened = top_indices.flatten(start_dim=0, end_dim=1)\n",
    "    sae_out = sae.decode(top_acts_flattened, top_indices_flattened)\n",
    "    sae_out = sae_out.unflatten(dim=0, sizes=(B,L))\n",
    "    return sae_out\n",
    "\n",
    "\n",
    "def forward_check_features(data, features, batch_size):\n",
    "    \n",
    "    global features_by_layer\n",
    "\n",
    "    features_by_layer = defaultdict(lambda: [])\n",
    "    for feature in features:\n",
    "        feature.records = []\n",
    "        features_by_layer[feature.layer].append(feature)\n",
    "\n",
    "    # only bother with SAE on the layers we are checking\n",
    "    layers_to_apply_sae = sorted(list(features_by_layer.keys()))\n",
    "    hooks = [(f'blocks.{layer}.hook_out_proj', partial(sae_hook, layer=layer)) for layer in layers_to_apply_sae]\n",
    "    DATA_LEN = data.size()[0]\n",
    "    for batch_start in tqdm(list(range(0, DATA_LEN, batch_size))):\n",
    "        batch_end = min(DATA_LEN, batch_start+batch_size)\n",
    "        data_batch = data[batch_start:batch_end]\n",
    "        _ = model.run_with_hooks(input=data_batch, fwd_hooks=hooks, fast_ssm=True, fast_conv=True)\n",
    "    \n",
    "\n",
    "\n",
    "forward_check_features(data=data_for_all_tokens3, features=new_modified_feats, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d3db508-2207-4d55-8446-641e90b0b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"layer_15_features_more_more.pkl\", \"wb\") as f:\n",
    "    pickle.dump((data_for_all_tokens3, new_modified_feats), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96fefab0-4deb-48e5-ae56-7ac707abdb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sae.data import chunk_and_tokenize\n",
    "dataset = load_dataset(\n",
    "    \"togethercomputer/RedPajama-Data-1T-Sample\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = model.tokenizer\n",
    "# too many processes crashes, probably memory issue\n",
    "tokenized = chunk_and_tokenize(dataset, tokenizer, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ff9b7-a97f-4413-ac86-f73ed0b9bced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|                                                                             | 99/5880 [06:02<6:47:19,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|                                                                           | 199/5880 [12:40<6:23:19,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|                                                                          | 299/5880 [19:18<8:49:10,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|                                                                        | 399/5880 [25:46<6:25:53,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|                                                                       | 499/5880 [32:16<6:05:27,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|                                                                      | 599/5880 [38:50<8:20:11,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|                                                                    | 699/5880 [45:26<5:59:13,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|                                                                   | 799/5880 [52:04<5:49:03,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|                                                                  | 899/5880 [58:42<7:45:09,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|                                                               | 999/5880 [1:05:17<5:35:41,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|                                                             | 1099/5880 [1:11:52<5:24:21,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|                                                           | 1199/5880 [1:18:29<7:23:45,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|                                                          | 1299/5880 [1:25:42<5:19:20,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|                                                          | 1320/5880 [1:27:41<3:49:06,  3.01s/it]"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SAEFeature:\n",
    "    \"\"\"Class for keeping track of an item in inventory.\"\"\"\n",
    "    layer: int\n",
    "    pos: int\n",
    "    feature_i: int\n",
    "    attr: float\n",
    "    records: list = field(default_factory=lambda: [])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.layer) + \" \" + str(self.pos) + \" \" + str(self.feature_i) + \" \" + str(self.attr)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(\"features_all_tokens_layer_15_with_collected_data.pkl\", \"rb\") as f:\n",
    "    features = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_batched_index_into(indices):\n",
    "    '''\n",
    "    given data that is [B,N,V] and indicies that are [B,N,K] with each index being an index into the V space\n",
    "    this gives you indexes you can use to access your values\n",
    "    '''\n",
    "    first_axis = []\n",
    "    second_axis = []\n",
    "    third_axis = []\n",
    "    B, _, _ = indices.size()\n",
    "    for b in range(B):\n",
    "        second, third = get_index_into(indices[b])\n",
    "        first_axis.append(torch.full(second.size(), fill_value=b, device=model.cfg.device))\n",
    "        second_axis.append(second)\n",
    "        third_axis.append(third)\n",
    "\n",
    "    return torch.cat(first_axis), torch.cat(second_axis), torch.cat(third_axis)\n",
    "\n",
    "def get_index_into(indices):\n",
    "    '''\n",
    "    given data that is [N,V] and indicies that are [N,K] with each index being an index into the V space\n",
    "    this gives you indexes you can use to access your values\n",
    "    '''\n",
    "    num_data, num_per_data = indices.size()\n",
    "    # we want\n",
    "    # [0,0,0,...,] num per data of these\n",
    "    # [1,1,1,...,] num per data of these\n",
    "    # ...\n",
    "    # [num_data-1, num_data-1, ...]\n",
    "    first_axis_index = torch.arange(num_data, dtype=torch.long).view(num_data, 1)*torch.ones([num_data, num_per_data], dtype=torch.long)\n",
    "    # now we flatten it so it has an index for each term aligned with our indices\n",
    "    first_axis_index = first_axis_index.flatten()\n",
    "    second_axis_index = indices.flatten()\n",
    "    return first_axis_index, second_axis_index\n",
    "global buffer\n",
    "buffer = None\n",
    "global features_by_layer\n",
    "def sae_hook(\n",
    "    x,\n",
    "    hook,\n",
    "    layer,\n",
    "):\n",
    "    # s is [B,L,E]\n",
    "    K = saes[layer].cfg.k\n",
    "    sae = saes[layer]\n",
    "    B,L,D = x.size()\n",
    "    uncorrupted_features = sae.encode(x)\n",
    "    top_acts, top_indices = uncorrupted_features.topk(K, sorted=False)\n",
    "    global buffer\n",
    "    if buffer is None:\n",
    "        buffer = torch.zeros(uncorrupted_features.size(), device=model.cfg.device)\n",
    "    buffer[:] = 0\n",
    "    global features_by_layer\n",
    "    # zero everything except the top k\n",
    "    buffer[get_batched_index_into(top_indices)] = top_acts.flatten()\n",
    "    for feature in features_by_layer[layer]:\n",
    "        feature.records += buffer[:,feature.pos,feature.feature_i].tolist()\n",
    "    # kernel can't handle doing all token positions at same time by default\n",
    "    # but if we make it think B*L is a single batch index it works fine\n",
    "    top_acts_flattened = top_acts.flatten(start_dim=0, end_dim=1)\n",
    "    top_indices_flattened = top_indices.flatten(start_dim=0, end_dim=1)\n",
    "    sae_out = sae.decode(top_acts_flattened, top_indices_flattened)\n",
    "    sae_out = sae_out.unflatten(dim=0, sizes=(B,L))\n",
    "    return sae_out\n",
    "from tqdm import tqdm\n",
    "\n",
    "def forward_check_features(data, features, batch_size):\n",
    "    \n",
    "    global features_by_layer\n",
    "    \n",
    "    #with open(\"layer_15_features_on_large_data.pkl\", \"rb\") as f:\n",
    "    #    features = pickle.load(f)\n",
    "    features_by_layer = defaultdict(lambda: [])\n",
    "    for feature in features:\n",
    "        feature.records = []\n",
    "        features_by_layer[feature.layer].append(feature)\n",
    "\n",
    "    # only bother with SAE on the layers we are checking\n",
    "    layers_to_apply_sae = sorted(list(features_by_layer.keys()))\n",
    "    hooks = [(f'blocks.{layer}.hook_out_proj', partial(sae_hook, layer=layer)) for layer in layers_to_apply_sae]\n",
    "    DATA_LEN = len(data)\n",
    "    indd = 0\n",
    "    num_already_processed = len(features[0].records)\n",
    "    for batch_start in tqdm(list(range(0, DATA_LEN, batch_size))):\n",
    "        indd += 1\n",
    "        if indd % 100 == 0:\n",
    "            with open(f\"layer_15_features_on_large_data{indd}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(features, f)\n",
    "                print(\"saving\")\n",
    "            for feature in features:\n",
    "                del feature.records\n",
    "                feature.records = []\n",
    "        batch_end = min(DATA_LEN, batch_start+batch_size)\n",
    "        if batch_end <= num_already_processed: continue\n",
    "        data_batch = data[batch_start:batch_end]['input_ids'][:,:128]\n",
    "        _ = model.run_with_hooks(input=data_batch, fwd_hooks=hooks, fast_ssm=True, fast_conv=True)\n",
    "\n",
    "features_sorted_by_feat_i = defaultdict(lambda: [])\n",
    "for feature in features:\n",
    "    new_feats = []\n",
    "    for pos in range(1,128):\n",
    "        feat1 = SAEFeature(layer=15, pos=pos, feature_i=feature.feature_i, attr=feature.attr)\n",
    "        new_feats.append(feat1)\n",
    "    features_sorted_by_feat_i[feature.feature_i] = new_feats\n",
    "\n",
    "new_modified_feats = []\n",
    "for f,feats in features_sorted_by_feat_i.items():\n",
    "    new_modified_feats += feats\n",
    "\n",
    "forward_check_features(tokenized, new_modified_feats, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d8eb43-8a8c-42b3-928c-2c0df2433482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f83adaa8-719a-4c90-9e16-40bd7ceb44c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spaceThings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 20\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m name_counts\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#for n,c in name_counts[:100]:\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m#    print(f\" name {n} with avg {torch.mean(c).item()} min {torch.min(c).item()} max {torch.max(c).item()}\")\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#names = sorted(list(acdc.data.ioi.good_names))\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m names \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m (i,x) \u001b[38;5;129;01min\u001b[39;00m \u001b[43mspaceThings\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     21\u001b[0m NUM_NAMES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(names)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#name_to_i = dict([(\" \" + name, i) for (i, name) in enumerate(names)])\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spaceThings' is not defined"
     ]
    }
   ],
   "source": [
    "def get_name_counts(feature):\n",
    "    name_counts = {}\n",
    "    DATA_LEN = len(feature.records)\n",
    "    records_tensor = torch.tensor(feature.records)\n",
    "    non_zero_indices = torch.arange(DATA_LEN)[records_tensor!=0]\n",
    "    non_zero_tokens = data.data[non_zero_indices,feature.pos].cpu()\n",
    "    non_zero_records = records_tensor[non_zero_indices]\n",
    "    name_tokens = torch.unique(non_zero_tokens)\n",
    "    for name_token in name_tokens:\n",
    "        name_str = model.to_str_tokens(name_token.view(1,1))[0]\n",
    "        name_counts[name_str] = non_zero_records[non_zero_tokens==name_token.item()]\n",
    "    #for t,c in template_counts.items():\n",
    "    #    print(f\" template {t} with count {torch.mean(torch.tensor(c)).item()}\")\n",
    "    name_counts = sorted(list(name_counts.items()), key=lambda x: -torch.mean(x[1]).item())\n",
    "    return name_counts\n",
    "    #for n,c in name_counts[:100]:\n",
    "    #    print(f\" name {n} with avg {torch.mean(c).item()} min {torch.min(c).item()} max {torch.max(c).item()}\")\n",
    "\n",
    "#names = sorted(list(acdc.data.ioi.good_names))\n",
    "names = [x for (i,x) in spaceThings if len(x.strip()) > 0]\n",
    "NUM_NAMES = len(names)\n",
    "#name_to_i = dict([(\" \" + name, i) for (i, name) in enumerate(names)])\n",
    "name_to_i = dict([(name, i) for (i, name) in enumerate(names)])\n",
    "\n",
    "def get_name_vector(feature, feat_type):\n",
    "    name_vec = torch.zeros(NUM_NAMES)\n",
    "    for name,counts in get_name_counts(feature):\n",
    "        if feat_type == 'mean':\n",
    "            name_vec[name_to_i[name]] = torch.mean(counts)\n",
    "        elif feat_type == 'min':\n",
    "            name_vec[name_to_i[name]] = torch.min(counts)\n",
    "        elif feat_type == 'max':\n",
    "            name_vec[name_to_i[name]] = torch.max(counts)\n",
    "            \n",
    "    return name_vec\n",
    "\n",
    "toks = model.to_str_tokens(data.data[0])\n",
    "name_positions = [3,5,7,13,15]\n",
    "position_map = {}\n",
    "L = data.data.size()[1]\n",
    "for l in range(L):\n",
    "    position_map[l] = f'pos{l}{toks[l]}'\n",
    "position_map[3] = 'n1'\n",
    "position_map[5] = 'n2'\n",
    "position_map[7] = 'n3'\n",
    "position_map[13] = 'n4'\n",
    "position_map[15] = 'n5'\n",
    "position_map[19] = 'out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cd2738c-9e8c-45cb-9cb2-070c8996cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "global feature_labels\n",
    "feature_labels = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b49bf76-95fd-409f-b51f-3cae04ada2a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_sorted_by_feat_i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mipywidgets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m widgets\n\u001b[1;32m      4\u001b[0m out \u001b[38;5;241m=\u001b[39m Output()\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mfeatures_sorted_by_feat_i\u001b[49m))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m cur_feature_ind\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdisplay_unlabeled_feature\u001b[39m():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_sorted_by_feat_i' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets.widgets import Output\n",
    "from ipywidgets import widgets\n",
    "out = Output()\n",
    "print(len(features_sorted_by_feat_i))\n",
    "\n",
    "global cur_feature_ind\n",
    "\n",
    "def display_unlabeled_feature():\n",
    "    global feature_labels\n",
    "    available_features = features_sorted_by_feat_i.keys() - feature_labels.keys()\n",
    "    for f in available_features:\n",
    "        print(f\"feature {f}\")\n",
    "        feats = features_sorted_by_feat_i[f]\n",
    "        #if any([position_map[feat.pos][0] != 'n' for feat in feats]):\n",
    "            #print(f\"warning, feature {feat_i} has non name poses, all pos are {[position_map[f.pos] for f in feats]})\")\n",
    "        #    continue\n",
    "        global cur_feature_ind\n",
    "        cur_feature_ind = f\n",
    "        display_feats(feats)\n",
    "        return\n",
    "    \n",
    "\n",
    "def display_feats(feats):\n",
    "    \n",
    "    feat_vecs = [get_name_vector(feat, 'mean') for feat in feats]\n",
    "    avg_vec = torch.stack(feat_vecs).mean(dim=0)\n",
    "    min_vec = torch.stack([get_name_vector(feat, 'min') for feat in feats]).min(dim=0).values\n",
    "    max_vec = torch.stack([get_name_vector(feat, 'max') for feat in feats]).max(dim=0).values\n",
    "    sorted_names = torch.argsort(-avg_vec)\n",
    "    #print(avg_vec, min_vec, max_vec, sorted_names)\n",
    "    for name_i in sorted_names[:50]:\n",
    "        #print(name_i)\n",
    "        print(f\" name {names[name_i]} with avg {avg_vec[name_i]} min {min_vec[name_i]} max {max_vec[name_i]}\")\n",
    "    \n",
    "    for feat in feats:\n",
    "        if position_map[feat.pos][0] == 'n':\n",
    "            print(position_map[feat.pos], detect_single_letter(feat))\n",
    "            pretty_print_list_first_letter_info(list_first_letter_info(feat))\n",
    "            print(improved_first_letter(feat))\n",
    "    \n",
    "    diffs = torch.zeros(len(feats), len(feats))\n",
    "    for i,featv1 in enumerate(feat_vecs):\n",
    "        for j,featv2 in enumerate(feat_vecs):\n",
    "            diffs[i,j] = torch.mean(torch.abs(featv1-featv2))\n",
    "    labels = [position_map[feat.pos] for feat in feats]\n",
    "    imshow(diffs, x=labels, y=labels, font_size=9)\n",
    "    \n",
    "\n",
    "def save_labels():\n",
    "    with open(\"layer_15_features.pkl\", \"wb\") as f:\n",
    "        global feature_labels\n",
    "        pickle.dump(feature_labels, f)\n",
    "        print(f\"done saving {len(feature_labels)}\")\n",
    "\n",
    "def submitted(arg):\n",
    "    if len(text_item.value.strip()) > 0:\n",
    "        with out:\n",
    "            clear_output()\n",
    "            global cur_feature_ind\n",
    "            global feature_labels\n",
    "            feature_labels[cur_feature_ind] = text_item.value\n",
    "            save_labels()\n",
    "            display_unlabeled_feature()\n",
    "        \n",
    "        text_item.value = ''\n",
    "text_item = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type something',\n",
    "    description='String:',\n",
    "    disabled=False\n",
    ")\n",
    "text_item.on_submit(submitted, names=\"value\")\n",
    "display(text_item)\n",
    "display(out)\n",
    "\n",
    "with out:\n",
    "    clear_output()\n",
    "    display_unlabeled_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "3e9ff81b-0ea0-4157-94c4-78c19347875a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(feature_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691b6084-5232-4b59-8bef-9bd18a91318f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
