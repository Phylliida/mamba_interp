{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "996a35f3-1bb7-4245-8e34-549b1f7fb097",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6efb8f18-92cd-490b-982e-17d528bd77fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(120000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 120 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 120\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import torch\n",
    "from einops import rearrange\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from jaxtyping import Float\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from dataclasses import dataclass\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets\n",
    "import plotly.subplots as sp\n",
    "\n",
    "\n",
    "# Import stuff\n",
    "import torch\n",
    "from functools import partial\n",
    "import transformer_lens.utils as utils\n",
    "import plotly.express as px\n",
    "torch.set_grad_enabled(False)\n",
    "device = utils.get_device()\n",
    "# modified from neel nanda's examples\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", font_size=None, show=True, color_continuous_midpoint=0.0, **kwargs):\n",
    "    fig = px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=color_continuous_midpoint, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs)\n",
    "    if not font_size is None:\n",
    "        if 'x' in kwargs:\n",
    "            fig.update_layout(\n",
    "              xaxis = dict(\n",
    "                tickmode='array',\n",
    "                tickvals = kwargs['x'],\n",
    "                ticktext = kwargs['x'], \n",
    "                ),\n",
    "               font=dict(size=font_size, color=\"black\"))\n",
    "        if 'y' in kwargs:\n",
    "            fig.update_layout(\n",
    "              yaxis = dict(\n",
    "                tickmode='array',\n",
    "                tickvals = kwargs['y'],\n",
    "                ticktext = kwargs['y'], \n",
    "                ),\n",
    "               font=dict(size=font_size, color=\"black\"))\n",
    "    if show:\n",
    "        fig.show(renderer)\n",
    "    else:\n",
    "        return fig\n",
    "\n",
    "def show(data, x, xaxis, yaxis, title, clipped):\n",
    "    if clipped:\n",
    "        imshow(data[:,2:], x=x[2:], xaxis=xaxis, yaxis=yaxis, title=title)\n",
    "    else:\n",
    "        imshow(data, x=x, xaxis=xaxis, yaxis=yaxis, title=title)\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)\n",
    "\n",
    "\n",
    "def bar_chart(data, x_labels, y_label, title, font_size=None):\n",
    "    # it requires a pandas dict with the columns and rows named, annoying\n",
    "    # by default rows and columns are named with ints so we relabel them accordingly\n",
    "    renames = dict([(i, x_labels[i]) for i in range(len(x_labels))])\n",
    "    ps = pd.DataFrame(data.cpu().numpy()).rename(renames, axis='rows').rename({0: y_label}, axis='columns')\n",
    "    fig = px.bar(ps, y=y_label, x=x_labels, title=title)\n",
    "    if not font_size is None:\n",
    "        fig.update_layout(\n",
    "          xaxis = dict(\n",
    "            tickmode='array',\n",
    "            tickvals = x_labels,\n",
    "            ticktext = x_labels, \n",
    "            ),\n",
    "           font=dict(size=font_size, color=\"black\"))\n",
    "        \n",
    "        #fig.update_xaxes(title_font=dict(size=font_size))\n",
    "    \n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbc699d-1d47-44da-9532-af58b3b85eb5",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e321013-128c-44e9-9061-ed534cf2538d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f04b3091d20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mamba_lens import HookedMamba\n",
    "model = HookedMamba.from_pretrained(\"state-spaces/mamba-370m\", device='cuda')\n",
    "#from transformer_lens import HookedTransformer\n",
    "#model = HookedTransformer.from_pretrained(\"gpt2-large\")\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2229a82-1322-4580-8db9-ac712439453a",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c46f00ab-f54d-40cb-bc38-48bd591c72d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_token(tokenizer):\n",
    "    return tokenizer.pad_token_id\n",
    "\n",
    "# given data that is [N,V] and indicies that are [N,K] with each index being an index into the V space\n",
    "# this does what you'd want, it indexes them\n",
    "# idk, see the test\n",
    "def index_into(data, indices):\n",
    "    num_data, num_per_data = indices.size()\n",
    "    # we want\n",
    "    # [0,0,0,...,] num per data of these\n",
    "    # [1,1,1,...,] num per data of these\n",
    "    # ...\n",
    "    # [num_data-1, num_data-1, ...]\n",
    "    first_axis_index = torch.arange(num_data, dtype=torch.long).view(num_data, 1)*torch.ones([num_data, num_per_data], dtype=torch.long)\n",
    "    # now we flatten it so it has an index for each term aligned with our indices\n",
    "    first_axis_index = first_axis_index.flatten()\n",
    "\n",
    "    second_axis_index = indices.flatten()\n",
    "    # now we can just index, and then view back to our original shape\n",
    "    return data[first_axis_index, second_axis_index].view(num_data, num_per_data)\n",
    "\n",
    "def eval_debug(model, data, correct, incorrect, constrain_to_answers, **kwargs):\n",
    "    for data_i in range(correct.size()[0]):\n",
    "        input = model.tokenizer.decode(data[data_i])\n",
    "        top_k = 2\n",
    "        logits = model.forward(input, **kwargs)[0,-1]\n",
    "        correct_answers = correct[data_i]\n",
    "        incorrect_answers = incorrect[data_i]\n",
    "        if constrain_to_answers:\n",
    "            only_consider_answer_logits = torch.full(logits.size(), fill_value=-torch.inf, device=model.cfg.device)\n",
    "            only_consider_answer_logits[correct_answers] = logits[correct_answers]\n",
    "            only_consider_answer_logits[incorrect_answers] = logits[incorrect_answers]\n",
    "            logits = only_consider_answer_logits\n",
    "        prs = torch.nn.functional.softmax(logits, dim=0)\n",
    "        top = torch.argsort(-logits)[:top_k]\n",
    "        printed = False\n",
    "        printed_prompt = False\n",
    "        for i, tok in enumerate(top):\n",
    "            if not i == 0 and tok in correct_answers:\n",
    "                if not printed_prompt:\n",
    "                    print(\"prompt\", input)\n",
    "                    printed_prompt = True\n",
    "                print(f\"  correct top {i} token {repr(model.tokenizer.decode([tok]))} logit {logits[tok]} prs {prs[tok]}\")\n",
    "                printed = True\n",
    "                #print(\"correct\")\n",
    "                #print(f\"token {model.tokenizer.decode([tok])} logit {logits[tok]} prs {prs[tok]}\")\n",
    "            elif i == 0 and not tok in correct_answers:\n",
    "                if not printed_prompt:\n",
    "                    print(\"prompt\", input)\n",
    "                    printed_prompt = True\n",
    "                print(f\"  incorrect top {i} token {repr(model.tokenizer.decode([tok]))} logit {logits[tok]} prs {prs[tok]}\")\n",
    "                printed = True\n",
    "            if tok in correct_answers:\n",
    "                break\n",
    "        if printed:\n",
    "            print()\n",
    "\n",
    "def eval(model, data, correct, incorrect, constrain_to_answers, **kwargs):\n",
    "        num_examples = correct.size()[0]\n",
    "        logits = model(data, **kwargs)[:,-1]\n",
    "        pad = get_pad_token(tokenizer=model.tokenizer)\n",
    "        logits[:,pad] = -torch.inf # manually set pad pr to -inf logit because sometimes we need to pad num correct or num incorrect\n",
    "\n",
    "        n_data, n_correct = correct.size()\n",
    "        n_data, n_incorrect = incorrect.size()\n",
    "\n",
    "        if constrain_to_answers:\n",
    "            # [n_data, n_correct]\n",
    "            correct_logits = index_into(logits, correct)\n",
    "            # [n_data, n_incorrect]\n",
    "            incorrect_logits = index_into(logits, incorrect)\n",
    "            # [n_data, n_correct + n_incorrect]\n",
    "            combined_logits = torch.concatenate([correct_logits, incorrect_logits], dim=1)\n",
    "            combined_prs = torch.softmax(combined_logits, dim=1)\n",
    "            biggest = torch.argsort(-combined_prs, dim=1)\n",
    "            # if biggest pr is in the correct, we are correct, otherwise, we are not\n",
    "            num_correct = torch.sum(biggest[:,0] < n_correct)\n",
    "    \n",
    "            correct_prs, incorrect_prs = combined_prs.split([n_correct, n_incorrect], dim=1)\n",
    "        else:\n",
    "            prs = torch.nn.functional.softmax(logits, dim=1)\n",
    "            # [n_data, n_correct]\n",
    "            correct_prs = index_into(prs, correct)\n",
    "            # [n_data, n_incorrect]\n",
    "            incorrect_prs = index_into(prs, incorrect)\n",
    "            # [n_data, 1]\n",
    "            top_tokens = torch.topk(logits, 1, dim=1).indices\n",
    "            # [n_data, n_correct]\n",
    "            in_correct = top_tokens == correct\n",
    "            # [n_data]\n",
    "            has_any_correct = torch.any(in_correct, dim=1)\n",
    "            # [1]\n",
    "            num_correct = torch.sum(has_any_correct)\n",
    "    \n",
    "        # the sum(dim=1) is because we or of all the different possible probabilities by summing\n",
    "        # then we'll just report the average\n",
    "        return torch.mean(correct_prs.sum(dim=1)).item(), torch.mean(incorrect_prs.sum(dim=1)).item(), num_correct.item()/float(n_data)\n",
    "\n",
    "\n",
    "def add_padding_answers(tokenizer, answers):\n",
    "    longest_len = len(max(answers, key=lambda x: len(x)))\n",
    "    padded_answers = []\n",
    "    pad_token = get_pad_token(tokenizer=tokenizer)\n",
    "    for answer in answers:\n",
    "        padded_answers.append(answer + [pad_token]*(longest_len-len(answer)))\n",
    "    return padded_answers\n",
    "\n",
    "def get_batched_data(data):\n",
    "    batched_data = []\n",
    "    batched_correct = []\n",
    "    batched_incorrect = []\n",
    "\n",
    "    bos = [model.tokenizer.bos_token_id]\n",
    "    \n",
    "    for i, (prompt, corrects, incorrects) in enumerate(data):\n",
    "        if i < 7:\n",
    "            print(prompt, corrects, incorrects)\n",
    "        batched_data.append(torch.tensor(bos + model.tokenizer.encode(prompt), device=model.cfg.device))\n",
    "        batched_correct.append([model.tokenizer.encode(correct)[0] for correct in corrects])\n",
    "        batched_incorrect.append([model.tokenizer.encode(incorrect)[0] for incorrect in incorrects])\n",
    "    try:\n",
    "        batched_data = torch.stack(batched_data)\n",
    "        batched_correct = torch.tensor(add_padding_answers(tokenizer=model.tokenizer, answers=batched_correct), device=model.cfg.device)\n",
    "        batched_incorrect = torch.tensor(add_padding_answers(tokenizer=model.tokenizer, answers=batched_incorrect), device=model.cfg.device)\n",
    "    except RuntimeError:\n",
    "        typical_len = len(batched_data[0])\n",
    "        for s in batched_data:\n",
    "            if not len(s) == typical_len:\n",
    "                print(len(s), \"is len of this, typical len is\", typical_len, \"for sequence\", model.to_str_tokens(s))\n",
    "        raise\n",
    "    return batched_data, batched_correct, batched_incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1704b-a0e3-433a-92b3-8e94ec3515f5",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd811756-3a9a-435d-b71f-6872ce8e6a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "Lately, Dale and Jerry had fun at the restaurant. Dale gave a ring to [' Jerry'] [' Dale']\n",
      "Lately, Dale and Jerry had fun at the restaurant. Jerry gave a ring to [' Dale'] [' Jerry']\n",
      "Lately, Raf and Judy had fun at the house. Raf gave a drink to [' Judy'] [' Raf']\n",
      "Lately, Raf and Judy had fun at the house. Judy gave a drink to [' Raf'] [' Judy']\n",
      "Lately, Lindsey and Sandra had fun at the store. Lindsey gave a snack to [' Sandra'] [' Lindsey']\n",
      "Lately, Lindsey and Sandra had fun at the store. Sandra gave a snack to [' Lindsey'] [' Sandra']\n",
      "Lately, Teresa and Lib had fun at the hospital. Teresa gave a drink to [' Lib'] [' Teresa']\n",
      "\n",
      "\n",
      "valid\n",
      "\n",
      "Lately, Florida and Max had fun at the office. Florida gave a ring to [' Max'] [' Florida']\n",
      "Lately, Florida and Max had fun at the office. Max gave a ring to [' Florida'] [' Max']\n",
      "Lately, Theresa and Heather had fun at the garden. Theresa gave a necklace to [' Heather'] [' Theresa']\n",
      "Lately, Theresa and Heather had fun at the garden. Heather gave a necklace to [' Theresa'] [' Heather']\n",
      "Lately, Emma and Dix had fun at the station. Emma gave a ring to [' Dix'] [' Emma']\n",
      "Lately, Emma and Dix had fun at the station. Dix gave a ring to [' Emma'] [' Dix']\n",
      "Lately, Bert and Jean had fun at the office. Bert gave a basketball to [' Jean'] [' Bert']\n",
      "\n",
      "\n",
      "test\n",
      "Lately, Eden and Theo had fun at the store. Eden gave a bone to [' Theo'] [' Eden']\n",
      "Lately, Eden and Theo had fun at the store. Theo gave a bone to [' Eden'] [' Theo']\n",
      "Lately, Hillary and Faith had fun at the station. Hillary gave a ring to [' Faith'] [' Hillary']\n",
      "Lately, Hillary and Faith had fun at the station. Faith gave a ring to [' Hillary'] [' Faith']\n",
      "Lately, Teddy and Adelaide had fun at the restaurant. Teddy gave a basketball to [' Adelaide'] [' Teddy']\n",
      "Lately, Teddy and Adelaide had fun at the restaurant. Adelaide gave a basketball to [' Teddy'] [' Adelaide']\n",
      "Lately, Elizabeth and Marian had fun at the station. Elizabeth gave a computer to [' Marian'] [' Elizabeth']\n"
     ]
    }
   ],
   "source": [
    "from docstring import docstring_prompt_generator_function\n",
    "from importlib import reload\n",
    "import test_data\n",
    "reload(test_data)\n",
    "from test_data import IOI_generator, BABA_TEMPLATES, greater_than_data_generator, ABC_TEMPLATES, copy_generator, IOI_custom_generator\n",
    "\n",
    "num_examples = 60\n",
    "\n",
    "seed = 27\n",
    "valid_seed = 37\n",
    "test_seed = 47\n",
    "\n",
    "# todo: move name tokens further apart so they can't use conv\n",
    "data_type = 'ioi custom'\n",
    "\n",
    "constrain_to_answers = True\n",
    "\n",
    "if data_type == 'ioi':\n",
    "    data = IOI_generator(templates=[BABA_TEMPLATES[1]], tokenizer=model.tokenizer, num_examples=num_examples, symmetric=True, seed=seed)\n",
    "    valid_data = IOI_generator(templates=[BABA_TEMPLATES[1]], tokenizer=model.tokenizer, num_examples=num_examples, symmetric=True, seed=valid_seed)\n",
    "    test_data = IOI_generator(templates=[BABA_TEMPLATES[1]], tokenizer=model.tokenizer, num_examples=num_examples, symmetric=True, seed=test_seed)\n",
    "elif data_type == 'ioi custom':\n",
    "    ioi_format = \"\"\"\n",
    "AB A B\n",
    "AB B A\"\"\".strip()\n",
    "    data = IOI_custom_generator(ioi_format=ioi_format, tokenizer=model.tokenizer, num_examples=num_examples, seed=seed)\n",
    "    valid_data = IOI_custom_generator(ioi_format=ioi_format, tokenizer=model.tokenizer, num_examples=num_examples, seed=valid_seed)\n",
    "    test_data = IOI_custom_generator(ioi_format=ioi_format, tokenizer=model.tokenizer, num_examples=num_examples, seed=test_seed)\n",
    "elif data_type == 'docstring':\n",
    "    data = docstring_prompt_generator_function(tokenizer=model.tokenizer, num_examples=num_examples, corrupt='random_answer', seed=seed)\n",
    "    valid_data = docstring_prompt_generator_function(tokenizer=model.tokenizer, num_examples=num_examples, corrupt='random_answer', seed=valid_seed)\n",
    "    test_data = docstring_prompt_generator_function(tokenizer=model.tokenizer, num_examples=num_examples, corrupt='random_answer', seed=test_seed)\n",
    "elif data_type == 'greater than':\n",
    "    constrain_to_answers = False\n",
    "    data = greater_than_data_generator(tokenizer=model.tokenizer, num_examples=num_examples, seed=seed)\n",
    "    valid_data = greater_than_data_generator(tokenizer=model.tokenizer, num_examples=num_examples, seed=valid_seed)\n",
    "    test_data = greater_than_data_generator(tokenizer=model.tokenizer, num_examples=num_examples, seed=test_seed)\n",
    "elif data_type == 'copy':\n",
    "    constrain_to_answers = False\n",
    "    copy_seq_len = 4\n",
    "    data = copy_generator(tokenizer=model.tokenizer, num_examples=num_examples, copy_seq_len=copy_seq_len, seed=seed)\n",
    "    valid_data = copy_generator(tokenizer=model.tokenizer, num_examples=num_examples, copy_seq_len=copy_seq_len, seed=valid_seed)\n",
    "    test_data = copy_generator(tokenizer=model.tokenizer, num_examples=num_examples, copy_seq_len=copy_seq_len, seed=test_seed)\n",
    "\n",
    "print(\"data\")\n",
    "batched_data, batched_correct, batched_incorrect = get_batched_data(data)\n",
    "print(\"\\n\\nvalid\\n\")\n",
    "vbatched_data, vbatched_correct, vbatched_incorrect = get_batched_data(valid_data)\n",
    "print(\"\\n\\ntest\")\n",
    "tbatched_data, tbatched_correct, tbatched_incorrect = get_batched_data(test_data)\n",
    "\n",
    "relative_str = ' relative ' if constrain_to_answers else ' '\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de9aa60-c92f-453e-bfc4-88e4b5c7188d",
   "metadata": {},
   "source": [
    "## Patching\n",
    "\n",
    "See [MambaLens](https://github.com/Phylliida/MambaLens) for a list of all the hooks available.\n",
    "\n",
    "set `fast_conv=True` unless you are patching on conv\n",
    "\n",
    "set `fast_ssm=True` unless you are patching on `h`, `delta`, `A_bar`, `B_bar`, or `y`\n",
    "\n",
    "(you can set them both to False always, it'll just be slower)\n",
    "\n",
    "There are a few special ones:\n",
    "\n",
    "## `skip h`\n",
    "\n",
    "This sets the hidden state contribution at the target position and layer to 0.\n",
    "\n",
    "In other words, where normally the hidden state at that position would be\n",
    "\n",
    "$$h_{pos+1} = Ah_{pos} + Bx$$\n",
    "\n",
    "Now it is just\n",
    "\n",
    "$$h_{pos+1} = Ah_{pos}$$\n",
    "\n",
    "## `h_n`\n",
    "\n",
    "Patches on the N different E-sized hidden states\n",
    "\n",
    "Note that patching on `h_n` is hardcoded to layer 39 via\n",
    "\n",
    "```\n",
    "H_N_PATCHING_LAYER = 39\n",
    "```\n",
    "\n",
    "You can change this to pick a different layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e54af2e7-9115-481b-aa8e-0c8ae0237453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline correct relative pr 0.9470134973526001 incorrect relative pr 0.05298648774623871 accuracy 0.9666666666666667\n",
      "uncorrupted prompt\n",
      "Lately, Dale and Jerry had fun at the restaurant. Dale gave a ring to\n",
      "' Jerry' logit 11.421454429626465\n",
      "' Jerry' pr 0.061610378324985504\n",
      "' Dale' logit 7.610736846923828\n",
      "' Dale' pr 0.0013635788345709443\n",
      "\n",
      "corrupted prompt\n",
      "Lately, Dale and Jerry had fun at the restaurant. Jerry gave a ring to\n",
      "' Jerry' logit 8.538025856018066\n",
      "' Jerry' pr 0.002474979031831026\n",
      "' Dale' logit 12.287115097045898\n",
      "' Dale' pr 0.1051429882645607\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf9e75669884f3e9fae4d5758a19dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Patching Type', options=('normalized input', 'layer input', 'skip', 'in proj', 'conv', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa285bf01704a39ab20dc78f7022bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='fast conv', options=('True', 'False'), value='True')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934db95a5e2f4d9daae3c3a4b233cbb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='fast ssm', options=('False', 'True'), value='False')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f040cf16512c48cf978cd822a48c6b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='logits or pr', options=('Logits', 'Pr', 'Both'), value='Logits')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09bc909f75e84ed99fb5406c8ad4aa74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run Patching', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63e4778dd8040669456c2021fdabe27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# modified from neel nanda's examples\n",
    "\n",
    "H_N_PATCHING_LAYER = 39\n",
    "\n",
    "# default settings aren't very good, these are better\n",
    "plot_args = {\n",
    "    'width': 800,\n",
    "    'height': 600,\n",
    "    \"autosize\": False,\n",
    "    'showlegend': True,\n",
    "    'margin': {\"l\":0,\"r\":0,\"t\":100,\"b\":0}\n",
    "}\n",
    "\n",
    "#limited_layers = [0, 3, 10, 14, 15, 17, 18, 21, 22, 25, 26, 27, 28, 33, 36, 39, 40, 45, 46]\n",
    "limited_layers = list(range(model.cfg.n_layers))\n",
    "\n",
    "# grab first two data points as patching inputs\n",
    "# the data generators will pair things nicely\n",
    "# you can also just replace these with two strings\n",
    "prompt_uncorrupted = model.tokenizer.decode(batched_data[0][1:])\n",
    "prompt_corrupted = model.tokenizer.decode(batched_data[1][1:])\n",
    "\n",
    "answer_tokens = sorted(list(set([x.item() for x in batched_correct[0]] + [x.item() for x in batched_correct[1]] + [x.item() for x in batched_incorrect[0]] + [x.item() for x in batched_incorrect[1]])))\n",
    "uncorrupted_answer = model.tokenizer.decode([batched_correct[0,0]])\n",
    "corrupted_answer = model.tokenizer.decode([batched_correct[1,0]])\n",
    "\n",
    "def wrap_run_with_hooks(model, fwd_hooks, **kwargs):\n",
    "    def wrapper(input):\n",
    "        return model.run_with_hooks(input, fwd_hooks=fwd_hooks, **kwargs)\n",
    "    wrapper.tokenizer = model.tokenizer\n",
    "    return wrapper\n",
    "\n",
    "# constrain_to_answers will constrain generation to only outputting those answers\n",
    "correct, incorrect, acc = eval(\n",
    "                            wrap_run_with_hooks(model=model, fwd_hooks=[], only_use_these_layers=limited_layers),\n",
    "                            vbatched_data, vbatched_correct, vbatched_incorrect,\n",
    "                            constrain_to_answers=constrain_to_answers)\n",
    "\n",
    "# if you are doing constrain_to_answers, the prs will be relative (so within the given answers the pr will sum to 1)\n",
    "# otherwise they will just be the prs given by softmax of the logits\n",
    "print(f\"baseline correct{relative_str}pr {correct} incorrect{relative_str}pr {incorrect} accuracy {acc}\")\n",
    "\n",
    "prompt_uncorrupted_tokens = model.to_tokens(prompt_uncorrupted)\n",
    "prompt_corrupted_tokens = model.to_tokens(prompt_corrupted)\n",
    "\n",
    "# logits should be [B,L,V] \n",
    "def uncorrupted_logit_minus_corrupted_logit(logits, uncorrupted_answer, corrupted_answer):\n",
    "    uncorrupted_index = model.to_single_token(uncorrupted_answer)\n",
    "    corrupted_index = model.to_single_token(corrupted_answer)\n",
    "    return logits[0, -1, uncorrupted_index] - logits[0, -1, corrupted_index]\n",
    "\n",
    "# prs should be [B,L,V] \n",
    "def uncorrupted_pr_minus_corrupted_pr(prs, uncorrupted_answer, corrupted_answer):\n",
    "    uncorrupted_index = model.to_single_token(uncorrupted_answer)\n",
    "    corrupted_index = model.to_single_token(corrupted_answer)\n",
    "    return prs[0, -1, uncorrupted_index] - prs[0, -1, corrupted_index]\n",
    "\n",
    "# [B,L,V]\n",
    "corrupted_logits, corrupted_activations = model.run_with_cache(prompt_corrupted_tokens, only_use_these_layers=limited_layers)\n",
    "corrupted_logit_diff = uncorrupted_logit_minus_corrupted_logit(logits=corrupted_logits, uncorrupted_answer=uncorrupted_answer, corrupted_answer=corrupted_answer)\n",
    "corrupted_prs = torch.softmax(corrupted_logits, dim=2)\n",
    "corrupted_pr_diff = uncorrupted_pr_minus_corrupted_pr(prs=corrupted_prs, uncorrupted_answer=uncorrupted_answer, corrupted_answer=corrupted_answer)\n",
    "\n",
    "# [B,L,V]\n",
    "uncorrupted_logits = model(prompt_uncorrupted_tokens, only_use_these_layers=limited_layers)\n",
    "uncorrupted_logit_diff = uncorrupted_logit_minus_corrupted_logit(logits=uncorrupted_logits, uncorrupted_answer=uncorrupted_answer, corrupted_answer=corrupted_answer)\n",
    "uncorrupted_prs = torch.softmax(uncorrupted_logits, dim=2)\n",
    "uncorrupted_pr_diff = uncorrupted_pr_minus_corrupted_pr(prs=uncorrupted_prs, uncorrupted_answer=uncorrupted_answer, corrupted_answer=corrupted_answer)\n",
    "\n",
    "uncorrupted_index = model.to_single_token(uncorrupted_answer)\n",
    "corrupted_index = model.to_single_token(corrupted_answer)\n",
    "print(f'uncorrupted prompt\\n{prompt_uncorrupted}')\n",
    "print(f\"{repr(uncorrupted_answer)} logit {uncorrupted_logits[0,-1,uncorrupted_index]}\")\n",
    "print(f\"{repr(uncorrupted_answer)} pr {uncorrupted_prs[0,-1,uncorrupted_index]}\")\n",
    "print(f\"{repr(corrupted_answer)} logit {uncorrupted_logits[0,-1,corrupted_index]}\")\n",
    "print(f\"{repr(corrupted_answer)} pr {uncorrupted_prs[0,-1,corrupted_index]}\")\n",
    "print(f'\\ncorrupted prompt\\n{prompt_corrupted}')\n",
    "print(f\"{repr(uncorrupted_answer)} logit {corrupted_logits[0,-1,uncorrupted_index]}\")\n",
    "print(f\"{repr(uncorrupted_answer)} pr {corrupted_prs[0,-1,uncorrupted_index]}\")\n",
    "print(f\"{repr(corrupted_answer)} logit {corrupted_logits[0,-1,corrupted_index]}\")\n",
    "print(f\"{repr(corrupted_answer)} pr {corrupted_prs[0,-1,corrupted_index]}\")\n",
    "\n",
    "# We make a tensor to store the results for each patching run. We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.\n",
    "L = len(prompt_uncorrupted_tokens[0])\n",
    "if len(prompt_corrupted_tokens[0]) != len(prompt_uncorrupted_tokens[0]):\n",
    "    raise Exception(\"Prompts are not the same length\") # feel free to comment this out, you can patch for different sized prompts its just a lil sus\n",
    "\n",
    "# diff is logit of uncorrupted_answer - logit of corrupted_answer\n",
    "# we expect corrupted_diff to have a negative value (as corrupted should put high pr on corrupted_answer)\n",
    "# we expect uncorrupted to have a positive value (as uncorrupted should put high pr on uncorrupted_answer)\n",
    "# thus we can treat these as (rough) min and max possible values\n",
    "min_logit_diff = corrupted_logit_diff\n",
    "max_logit_diff = uncorrupted_logit_diff\n",
    "\n",
    "min_pr_diff = corrupted_pr_diff\n",
    "max_pr_diff = uncorrupted_pr_diff\n",
    "\n",
    "# make token labels that describe the patch\n",
    "corrupted_str_tokens = model.to_str_tokens(prompt_corrupted_tokens)\n",
    "uncorrupted_str_tokens = model.to_str_tokens(prompt_uncorrupted_tokens)\n",
    "token_labels = []\n",
    "for index, (corrupted_token, uncorrupted_token) in enumerate(zip(corrupted_str_tokens, uncorrupted_str_tokens)):\n",
    "    if corrupted_token == uncorrupted_token:\n",
    "        token_labels.append(f\"{corrupted_token}_{index}\")\n",
    "    else:\n",
    "        token_labels.append(f\"{uncorrupted_token}->{corrupted_token}_{index}\")\n",
    "\n",
    "def run_patching(patching_type, patching_hook_name_func, patching_hook_func, show_options, show_plot=True, **kwargs):\n",
    "    hook_title = patching_hook_name_func(layer='{layer}', position='{position}')\n",
    "    print(f\"running patching {patching_type}, using hook {hook_title}\")\n",
    "    global patching_result_logits, patching_result_prs # if you want to access it once this is done running\n",
    "    n_layers = len(limited_layers)\n",
    "\n",
    "    if patching_type == H_N_PATCHING:\n",
    "        print(f\"on layer H_N_PATCHING_LAYER={H_N_PATCHING_LAYER}\")\n",
    "        N = model.cfg.N\n",
    "        patching_result_normalized_logits = torch.zeros((N, L), device=model.cfg.device)\n",
    "        patching_result_normalized_prs = torch.zeros((N, L), device=model.cfg.device)\n",
    "    \n",
    "        num_answers = len(answer_tokens)\n",
    "        patching_result_logits = torch.zeros((N, L, num_answers), device=model.cfg.device)\n",
    "        patching_result_prs = torch.zeros((N, L, num_answers), device=model.cfg.device)\n",
    "    else:\n",
    "        patching_result_normalized_logits = torch.zeros((n_layers, L), device=model.cfg.device)\n",
    "        patching_result_normalized_prs = torch.zeros((n_layers, L), device=model.cfg.device)\n",
    "    \n",
    "        num_answers = len(answer_tokens)\n",
    "        patching_result_logits = torch.zeros((n_layers, L, num_answers), device=model.cfg.device)\n",
    "        patching_result_prs = torch.zeros((n_layers, L, num_answers), device=model.cfg.device)\n",
    "\n",
    "    hooks = []\n",
    "    # skipping h needs A_bar stored, so also add that hook\n",
    "    if patching_type == SKIPPING_H_PATCHING:\n",
    "        for i, layer in list(enumerate(limited_layers)):\n",
    "            hooks.append((f'blocks.{layer}.hook_A_bar', partial(A_bar_storage_hook_for_skipping_h, layer=layer)))\n",
    "\n",
    "    # skipping layer needs layer_input (resid_pre) stored, so also add that hook\n",
    "    if patching_type == LAYER_SKIPPING:\n",
    "        for i, layer in list(enumerate(limited_layers)):\n",
    "            hooks.append((f'blocks.{layer}.hook_resid_pre', partial(layer_input_storage_hook, layer=layer)))\n",
    "    \n",
    "    if patching_type == H_N_PATCHING:\n",
    "        batch = 0\n",
    "        indices = []\n",
    "        for n in range(N):\n",
    "            for position in range(L):\n",
    "                patching_hook_name = patching_hook_name_func(layer=H_N_PATCHING_LAYER, position=position)\n",
    "                patching_hook = partial(patching_hook_func, layer=H_N_PATCHING_LAYER, position=position, n=n, batch=batch)\n",
    "                batch += 1\n",
    "                indices.append((n,position))\n",
    "                hooks.append((patching_hook_name, patching_hook))\n",
    "    else:\n",
    "        batch = 0\n",
    "        indices = []\n",
    "        for i, layer in list(enumerate(limited_layers)):\n",
    "            for position in range(L):\n",
    "                patching_hook_name = patching_hook_name_func(layer=layer, position=position)\n",
    "                patching_hook = partial(patching_hook_func, layer=layer, position=position, batch=batch)\n",
    "                batch += 1\n",
    "                indices.append((i,position))\n",
    "                hooks.append((patching_hook_name, patching_hook))\n",
    "    \n",
    "    # [B,L,V]\n",
    "    patched_logits = model.run_with_hooks(prompt_uncorrupted_tokens.expand(batch,L), fwd_hooks=hooks, only_use_these_layers=limited_layers, **kwargs)\n",
    "    # [B,L,V]\n",
    "    patched_prs = torch.softmax(patched_logits, dim=2)\n",
    "\n",
    "    for b, (i,position) in enumerate(indices):\n",
    "        if corrupted_answer != uncorrupted_answer:\n",
    "            patched_logit_diff = uncorrupted_logit_minus_corrupted_logit(logits=patched_logits[b:b+1],\n",
    "                                                                         uncorrupted_answer=uncorrupted_answer,\n",
    "                                                                         corrupted_answer=corrupted_answer)\n",
    "            # normalize it so\n",
    "            # 0 means min_logit_diff (so 0 means that it is acting like the corrupted model)\n",
    "            # 1 means max_logit_diff (so 1 means that it is acting like the uncorrupted model)\n",
    "            normalized_patched_logit_diff = (patched_logit_diff-min_logit_diff)/(max_logit_diff - min_logit_diff)\n",
    "            # now flip them, since most interventions will do nothing and thus act like uncorrupted model, visually its better to have that at 0\n",
    "            # so now\n",
    "            # 0 means that it is acting like the uncorrupted model\n",
    "            # 1 means that it is acting like the corrupted model\n",
    "            normalized_patched_logit_diff = 1.0 - normalized_patched_logit_diff\n",
    "            patching_result_normalized_logits[i, position] = normalized_patched_logit_diff\n",
    "            \n",
    "            # same for pr\n",
    "            patched_pr_diff = uncorrupted_pr_minus_corrupted_pr(prs=patched_prs[b:b+1],\n",
    "                                                                uncorrupted_answer=uncorrupted_answer,\n",
    "                                                                corrupted_answer=corrupted_answer)\n",
    "            normalized_patched_pr_diff = 1.0-(patched_pr_diff-min_pr_diff)/(max_pr_diff - min_pr_diff)\n",
    "            patching_result_normalized_prs[i, position] = normalized_patched_pr_diff\n",
    "\n",
    "        for k, answer_token in enumerate(answer_tokens):\n",
    "            patching_result_logits[i, position, k] = patched_logits[b,-1,answer_token]\n",
    "            patching_result_prs[i, position, k] = patched_prs[b,-1,answer_token]\n",
    "    \n",
    "        \n",
    "    if patching_type == H_N_PATCHING:\n",
    "        layer_labels = [str(n) for n in range(N)]\n",
    "    else:\n",
    "        layer_labels = [str(layer) for layer in limited_layers]\n",
    "    figs = []\n",
    "    if corrupted_answer != uncorrupted_answer:\n",
    "        if show_options in [SHOW_LOGITS, SHOW_BOTH]:\n",
    "            figs.append(imshow(patching_result_normalized_logits, show=False, x=token_labels, y=layer_labels, xaxis=\"Position\", yaxis=\"Layer\", title=f\"Normalized logit difference after patching {patching_type} using hook {hook_title}\", font_size=8))\n",
    "        if show_options in [SHOW_PR, SHOW_BOTH]:\n",
    "            figs.append(imshow(patching_result_normalized_prs, show=False, x=token_labels, y=layer_labels, xaxis=\"Position\", yaxis=\"Layer\", title=f\"Normalized pr difference after patching {patching_type} using hook {hook_title}\", font_size=8))\n",
    "    \n",
    "    for k, answer_token in enumerate(answer_tokens):\n",
    "        if show_options in [SHOW_LOGITS, SHOW_BOTH]:\n",
    "            figs.append(imshow(patching_result_logits[:,:,k], color_continuous_midpoint=None, show=False, x=token_labels, y=layer_labels, xaxis=\"Position\", yaxis=\"Layer\", title=f\"Logit of uncorrupted answer {repr(model.tokenizer.decode([answer_token]))} after patching {patching_type} using hook {hook_title}\", font_size=8))\n",
    "        if show_options in [SHOW_PR, SHOW_BOTH]:\n",
    "            figs.append(imshow(patching_result_prs[:,:,k], show=False, x=token_labels, y=layer_labels, xaxis=\"Position\", yaxis=\"Layer\", title=f\"Pr of uncorrupted answer {repr(model.tokenizer.decode([answer_token]))} after patching {patching_type} using hook {hook_title}\", font_size=8)) \n",
    "\n",
    "    for fig in figs:\n",
    "        fig.update_layout(**plot_args)\n",
    "        fig.update_layout(legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=0.01\n",
    "        ))\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "\n",
    "## hooks for layer skipping\n",
    "def layer_input_storage_hook(\n",
    "    layer_input: Float[torch.Tensor, \"B L D\"],\n",
    "    hook: HookPoint,\n",
    "    layer: int,\n",
    ") -> Float[torch.Tensor, \"B L D\"]:\n",
    "    global storage\n",
    "    storage = {}\n",
    "    storage['layer_input'] = layer_input\n",
    "    return layer_input\n",
    "\n",
    "def layer_output_skipping_hook(\n",
    "    layer_output: Float[torch.Tensor, \"B L D\"],\n",
    "    hook: HookPoint,\n",
    "    position: int,\n",
    "    layer: int,\n",
    "    batch: int,\n",
    ") -> Float[torch.Tensor, \"B L D\"]:\n",
    "    global storage\n",
    "    layer_input = storage['layer_input']\n",
    "    # intervene on the batch at the position\n",
    "    layer_output[batch,position,:] = layer_input[batch,position,:]\n",
    "    return layer_output\n",
    "\n",
    "\n",
    "## hooks for h skipping\n",
    "def A_bar_storage_hook_for_skipping_h(\n",
    "    A_bar: Float[torch.Tensor, \"B L E N\"],\n",
    "    hook: HookPoint,\n",
    "    layer: int,\n",
    ") -> Float[torch.Tensor, \"B L E N\"]:\n",
    "    global storage\n",
    "    storage = {}\n",
    "    storage['A_bar'] = A_bar\n",
    "    return A_bar\n",
    "\n",
    "def skipping_h_hook(\n",
    "    h: Float[torch.Tensor, \"B E N\"],\n",
    "    hook: HookPoint,\n",
    "    position: int,\n",
    "    layer: int,\n",
    "    batch: int,\n",
    ") -> Float[torch.Tensor, \"B E N\"]:\n",
    "    #print(\"fetching\", storage[grab_pos][0,0,0:5], \"from position\", grab_pos)\n",
    "    #print(\"my value (being ignore) is\", h[0,0,0:5])\n",
    "    #print(f\"skipping ahead h at position {position}\")\n",
    "    global storage\n",
    "    B,E,N = h.size()\n",
    "    grab_pos = position-1\n",
    "    if grab_pos < 0:\n",
    "        h[batch,:,:] = torch.zeros((E,N), device=model.cfg.device)\n",
    "    else:\n",
    "        B,E,N = h.size()\n",
    "        A_contribution = torch.ones((E,N), device=model.cfg.device)\n",
    "        for missed_pos in range(grab_pos+1, position+1):\n",
    "            A_contribution *= storage['A_bar'][batch,missed_pos,:,:]\n",
    "        h_stored = storage[grab_pos][batch,:,:]\n",
    "        h[batch,:,:] = A_contribution*h_stored\n",
    "        #return A_contribution*storage[grab_pos]\n",
    "    storage[position] = h\n",
    "    return h\n",
    "\n",
    "\n",
    "## Regular patching hooks\n",
    "def position_patching_hook( # also works for B L E, B L E N, and B L N sized things\n",
    "    x: Float[torch.Tensor, \"B L D\"],\n",
    "    hook: HookPoint,\n",
    "    position: int,\n",
    "    layer: int, # we don't care about this\n",
    "    batch: int,\n",
    ") -> Float[torch.Tensor, \"B L D\"]:\n",
    "    # only intervene on the specific pos\n",
    "    corrupted_x = corrupted_activations[hook.name]\n",
    "    x[batch, position, :] = corrupted_x[0, position, :]\n",
    "    return x\n",
    "\n",
    "def h_patching_hook(\n",
    "    h: Float[torch.Tensor, \"B E N\"],\n",
    "    hook: HookPoint,\n",
    "    position: int,\n",
    "    layer: int,\n",
    "    batch: int,\n",
    ") -> Float[torch.Tensor, \"B E N\"]:\n",
    "    corrupted_h = corrupted_activations[hook.name]\n",
    "    h[batch] = corrupted_h[0]\n",
    "    return h\n",
    "\n",
    "def h_n_patching_hook(\n",
    "    h: Float[torch.Tensor, \"B E N\"],\n",
    "    hook: HookPoint,\n",
    "    position: int,\n",
    "    layer: int,\n",
    "    n: int,\n",
    "    batch: int,\n",
    ") -> Float[torch.Tensor, \"B E N\"]:\n",
    "    corrupted_h = corrupted_activations[hook.name]\n",
    "    h[batch,:,n] = corrupted_h[0,:,n]\n",
    "    return h\n",
    "\n",
    "SKIPPING_H_PATCHING = 'skipping h'\n",
    "H_N_PATCHING = 'h_n'\n",
    "LAYER_SKIPPING = 'skipping layer'\n",
    "\n",
    "patching_types = {\n",
    "    'normalized input': (lambda layer, position: f'blocks.{layer}.hook_normalized_input', position_patching_hook),\n",
    "    'layer input': (lambda layer, position: f'blocks.{layer}.hook_resid_pre', position_patching_hook),\n",
    "    'skip': (lambda layer, position: f'blocks.{layer}.hook_skip', position_patching_hook), \n",
    "    'in proj': (lambda layer, position: f'blocks.{layer}.hook_in_proj', position_patching_hook), \n",
    "    'conv': (lambda layer, position: f'blocks.{layer}.hook_conv', position_patching_hook), \n",
    "    'delta 1': (lambda layer, position: f'blocks.{layer}.hook_delta_1', position_patching_hook), \n",
    "    'delta 2': (lambda layer, position: f'blocks.{layer}.hook_delta_2', position_patching_hook), \n",
    "    'delta': (lambda layer, position: f'blocks.{layer}.hook_delta', position_patching_hook), \n",
    "    'A_bar': (lambda layer, position: f'blocks.{layer}.hook_A_bar', position_patching_hook), \n",
    "    'B': (lambda layer, position: f'blocks.{layer}.hook_B', position_patching_hook), \n",
    "    'B_bar': (lambda layer, position: f'blocks.{layer}.hook_B_bar', position_patching_hook), \n",
    "    'C': (lambda layer, position: f'blocks.{layer}.hook_C', position_patching_hook), \n",
    "    'ssm input': (lambda layer, position: f'blocks.{layer}.hook_ssm_input', position_patching_hook),\n",
    "    SKIPPING_H_PATCHING: (lambda layer, position: f'blocks.{layer}.hook_h.{position}', skipping_h_hook),\n",
    "    'h': (lambda layer, position: f'blocks.{layer}.hook_h.{position}', h_patching_hook),\n",
    "    H_N_PATCHING: (lambda layer, position: f'blocks.{layer}.hook_h.{position}', h_n_patching_hook),\n",
    "    'y': (lambda layer, position: f'blocks.{layer}.hook_y', position_patching_hook),\n",
    "    'ssm output': (lambda layer, position: f'blocks.{layer}.hook_ssm_output', position_patching_hook),\n",
    "    'after skip': (lambda layer, position: f'blocks.{layer}.hook_after_skip', position_patching_hook),\n",
    "    'out proj': (lambda layer, position: f'blocks.{layer}.hook_out_proj', position_patching_hook),\n",
    "    'resid post': (lambda layer, position: f'blocks.{layer}.hook_resid_post', position_patching_hook),\n",
    "    LAYER_SKIPPING: (lambda layer, position: f'blocks.{layer}.hook_resid_post', layer_output_skipping_hook),\n",
    "}\n",
    "\n",
    "patching_types_keys = list(patching_types.keys())\n",
    "\n",
    "def choose_patching_type(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_patching_type.patching_type = change['new'] # hack, gives this function the patching_type attribute\n",
    "\n",
    "choose_patching_type.patching_type = patching_types_keys[0]\n",
    "\n",
    "patching_type_dropdown = ipywidgets.Dropdown(\n",
    "    options=patching_types_keys,\n",
    "    value=patching_types_keys[0],\n",
    "    description='Patching Type',\n",
    ")\n",
    "patching_type_dropdown.observe(choose_patching_type)\n",
    "display(patching_type_dropdown)\n",
    "\n",
    "\n",
    "fast_conv_keys = ['True', 'False']\n",
    "\n",
    "def choose_fast_conv(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_fast_conv.fast_conv = change['new'] == 'True'\n",
    "\n",
    "choose_fast_conv.fast_conv = fast_conv_keys[0] == 'True'\n",
    "\n",
    "choose_fast_conv_dropdown = ipywidgets.Dropdown(\n",
    "    options=fast_conv_keys,\n",
    "    value=fast_conv_keys[0],\n",
    "    description='fast conv',\n",
    ")\n",
    "choose_fast_conv_dropdown.observe(choose_fast_conv)\n",
    "display(choose_fast_conv_dropdown)\n",
    "\n",
    "\n",
    "fast_ssm_keys = ['False', 'True']\n",
    "\n",
    "def choose_fast_ssm(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_fast_ssm.fast_ssm = change['new'] == 'True'\n",
    "\n",
    "choose_fast_ssm.fast_ssm = fast_ssm_keys[0] == 'True'\n",
    "\n",
    "choose_fast_ssm_dropdown = ipywidgets.Dropdown(\n",
    "    options=fast_ssm_keys,\n",
    "    value=fast_ssm_keys[0],\n",
    "    description='fast ssm',\n",
    ")\n",
    "choose_fast_ssm_dropdown.observe(choose_fast_ssm)\n",
    "display(choose_fast_ssm_dropdown)\n",
    "\n",
    "SHOW_PR = 'Pr'\n",
    "SHOW_LOGITS = 'Logits'\n",
    "SHOW_BOTH = 'Both'\n",
    "show_options = [SHOW_LOGITS, SHOW_PR, SHOW_BOTH]\n",
    "\n",
    "def choose_show_options(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_show_options.show_options = change['new']\n",
    "\n",
    "choose_show_options.show_options = show_options[0]\n",
    "\n",
    "show_options_dropdown = ipywidgets.Dropdown(\n",
    "    options=show_options,\n",
    "    value=show_options[0],\n",
    "    description='logits or pr',\n",
    ")\n",
    "show_options_dropdown.observe(choose_show_options)\n",
    "display(show_options_dropdown)\n",
    "\n",
    "def do_patching(arg, show_plot=True):\n",
    "    with output: # this lets the stuff we output here be visible\n",
    "        clear_output()\n",
    "        patching_type = choose_patching_type.patching_type\n",
    "        hook_name_func, hook_func = patching_types[patching_type]\n",
    "        return run_patching(\n",
    "                     patching_type=patching_type,\n",
    "                     patching_hook_name_func=hook_name_func,\n",
    "                     patching_hook_func=hook_func,\n",
    "                     fast_ssm=choose_fast_ssm.fast_ssm,\n",
    "                     fast_conv=choose_fast_conv.fast_conv,\n",
    "                     show_options=choose_show_options.show_options,\n",
    "                     show_plot=show_plot)\n",
    "\n",
    "patching_button = ipywidgets.Button(description = 'Run Patching')\n",
    "patching_button.on_click(do_patching)\n",
    "display(patching_button)\n",
    "\n",
    "# you can't just display stuff inside a widget callback, you need a wrap any display code in this\n",
    "output = ipywidgets.Output()\n",
    "display(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df76d1-dca9-4848-b9c9-3ad85a414834",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
