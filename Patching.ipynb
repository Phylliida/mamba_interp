{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6efb8f18-92cd-490b-982e-17d528bd77fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 60\n",
    "\n",
    "import plotly.io as pio\n",
    "### optional (fast_conv and fast_ssm) ###\n",
    "\n",
    "## install cuda\n",
    "# if you are on a devbox, use\n",
    "# sudo apt install wget\n",
    "# wget https://developer.download.nvidia.com/compute/cuda/12.0.0/local_installers/cuda_12.0.0_525.60.13_linux.run\n",
    "# sudo sh cuda_12.0.0_525.60.13_linux.run\n",
    "# add /usr/local/cuda-12.0/bin/ to PATH\n",
    "# that gives u nvcc\n",
    "\n",
    "## fast_conv=True\n",
    "# if you want fast_conv=True you need to install\n",
    "# https://github.com/Dao-AILab/causal-conv1d\n",
    "# via\n",
    "# pip install causal-conv1d\n",
    "\n",
    "## fast_ssm=True\n",
    "# if you want fast_ssm=True you need to install\n",
    "# https://github.com/state-spaces/mamba\n",
    "# via\n",
    "# pip install mamba-ssm\n",
    "\n",
    "\n",
    "### install plotly ###\n",
    "\n",
    "# if plotly isn't working, i did\n",
    "# install nvm from\n",
    "# https://github.com/nvm-sh/nvm\n",
    "# and activate it\n",
    "# then i did\n",
    "# nvm install node\n",
    "# jupyter labextension install plotlywidget\n",
    "\n",
    "# if the jupyter widgets aren't working i have no recommendations\n",
    "# only god can help you\n",
    "\n",
    "# Import stuff\n",
    "import torch\n",
    "from functools import partial\n",
    "import transformer_lens.utils as utils\n",
    "import plotly.express as px\n",
    "torch.set_grad_enabled(False)\n",
    "device = utils.get_device()\n",
    "# from neel nanda's examples\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", font_size=None, show=True, color_continuous_midpoint=0.0, **kwargs):\n",
    "    fig = px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=color_continuous_midpoint, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs)\n",
    "    if not font_size is None:\n",
    "        fig.update_layout(\n",
    "          xaxis = dict(\n",
    "            tickmode='array',\n",
    "            tickvals = kwargs['x'],\n",
    "            ticktext = kwargs['x'], \n",
    "            ),\n",
    "           font=dict(size=font_size, color=\"black\"))\n",
    "    if show:\n",
    "        fig.show(renderer)\n",
    "    else:\n",
    "        return fig\n",
    "\n",
    "def show(data, x, xaxis, yaxis, title, clipped):\n",
    "    if clipped:\n",
    "        imshow(data[:,2:], x=x[2:], xaxis=xaxis, yaxis=yaxis, title=title)\n",
    "    else:\n",
    "        imshow(data, x=x, xaxis=xaxis, yaxis=yaxis, title=title)\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e321013-128c-44e9-9061-ed534cf2538d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# load mamba\n",
    "import os\n",
    "from mamba_lens import HookedMamba\n",
    "import os\n",
    "import datetime\n",
    "model = HookedMamba.from_pretrained(\"state-spaces/mamba-370m\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd811756-3a9a-435d-b71f-6872ce8e6a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABC DEF', 'ABC ADE', 'ABC DAE', 'ABC DEA', 'BAC ADE', 'BAC DAE', 'BAC DEA', 'BCA ADE', 'BCA DAE', 'BCA DEA', 'ABC ABD', 'ABC ADB', 'ABC DAB', 'ACB ABD', 'ACB ADB', 'ACB DAB', 'CAB ABD', 'CAB ADB', 'CAB DAB', 'ABC BAD', 'ABC BDA', 'ABC DBA', 'ACB BAD', 'ACB BDA', 'ACB DBA', 'CAB BAD', 'CAB BDA', 'CAB DBA', 'ABC ABC', 'ABC ACB', 'ABC CBA', 'ABC BAC', 'ABC BCA', 'ABC CAB']\n",
      "['Abbey', 'Ada', 'Adelaide', 'Adrian', 'Ag', 'Alberta', 'Alexa', 'Alice', 'Allison', 'Amanda', 'Amber', 'Amy', 'Ana', 'Andrea', 'Andy', 'Angela', 'Anna', 'Anne', 'Annie', 'April', 'Ara', 'Ashley', 'Asia', 'Atlanta', 'Audi', 'Augustine', 'Aurora', 'Austin', 'Bab', 'Barbara', 'Barry', 'Bea', 'Bee', 'Bella', 'Belle', 'Bernie', 'Berry', 'Bert', 'Bess', 'Beth', 'Betty', 'Beverly', 'Billy', 'Bird', 'Blair', 'Blake', 'Bobby', 'Bren', 'Brett', 'Britt', 'Brooks', 'Cal', 'Cam', 'Carey', 'Carolina', 'Caroline', 'Carrie', 'Carroll', 'Cary', 'Casey', 'Cassie', 'Catherine', 'Cecil', 'Cele', 'Chad', 'Charlotte', 'Chelsea', 'Cherry', 'Chloe', 'Christian', 'Christie', 'Christina', 'Christine', 'Chrysler', 'Claire', 'Clara', 'Clare', 'Claude', 'Clem', 'Cody', 'Connie', 'Cookie', 'Cory', 'Cris', 'Crystal', 'Daisy', 'Dale', 'Dallas', 'Dana', 'Danny', 'Dawn', 'Deborah', 'Dee', 'Dell', 'Devon', 'Diana', 'Diane', 'Dion', 'Dix', 'Doe', 'Doll', 'Donna', 'Dorothy', 'Dot', 'Dre', 'Dru', 'Easter', 'Eddie', 'Eden', 'Edith', 'Elaine', 'Eleanor', 'Elena', 'Elizabeth', 'Ellen', 'Elli', 'Elsa', 'Else', 'Emily', 'Emma', 'Erin', 'Esther', 'Eva', 'Eve', 'Faith', 'Fan', 'Fern', 'Florence', 'Florida', 'Frances', 'Frank', 'Freddie', 'Gabriel', 'Gates', 'Gay', 'Geneva', 'George', 'Georgia', 'Gert', 'Gill', 'Glad', 'Glenn', 'Gloria', 'Grace', 'Gray', 'Gus', 'Hannah', 'Happy', 'Heather', 'Helen', 'Hillary', 'Holly', 'Honey', 'Honor', 'Hope', 'Ira', 'Irene', 'Isabel', 'Ivy', 'Jackie', 'Jamie', 'Janet', 'Jean', 'Jennifer', 'Jenny', 'Jerry', 'Jesse', 'Jessica', 'Jill', 'Joan', 'Joey', 'Jordan', 'Joyce', 'Judith', 'Judy', 'Julia', 'Juliet', 'June', 'Kai', 'Karen', 'Kate', 'Katherine', 'Kathleen', 'Kathy', 'Katie', 'Kay', 'Kelley', 'Kelly', 'Kerry', 'Kim', 'Kirby', 'Kitty', 'Kore', 'Kris', 'Kyle', 'Lane', 'Laura', 'Lauren', 'Leah', 'Lee', 'Leigh', 'Leone', 'Leslie', 'Lib', 'Lilly', 'Lily', 'Linda', 'Lindsay', 'Lindsey', 'Lisa', 'Liv', 'Liz', 'Loren', 'Louise', 'Lucia', 'Lucky', 'Lucy', 'Lynn', 'Mae', 'Maggie', 'Margaret', 'Marian', 'Marie', 'Marina', 'Marion', 'Martha', 'Marty', 'Mary', 'Max', 'May', 'Mead', 'Megan', 'Melissa', 'Mercedes', 'Meta', 'Michelle', 'Mil', 'Min', 'Miranda', 'Molly', 'Monica', 'Morgan', 'Nancy', 'Natalie', 'Nell', 'Nicole', 'Nike', 'Nina', 'Olive', 'Olivia', 'Page', 'Pam', 'Patricia', 'Paula', 'Pauli', 'Pearl', 'Peggy', 'Penny', 'Perl', 'Perry', 'Pet', 'Philippe', 'Philippine', 'Pier', 'Quinn', 'Rachel', 'Raf', 'Randy', 'Raven', 'Ray', 'Rebecca', 'Rena', 'Rey', 'Rica', 'Ricky', 'Rita', 'Robin', 'Rosa', 'Rose', 'Rosie', 'Row', 'Ruby', 'Ruth', 'Sally', 'Sam', 'Sandra', 'Sandy', 'Sarah', 'Sean', 'Shane', 'Shannon', 'Sharon', 'Shawn', 'Shelby', 'Shell', 'Sher', 'Shirley', 'Silva', 'Sophie', 'Star', 'Stephanie', 'Storm', 'Sue', 'Sunny', 'Susan', 'Sydney', 'Tate', 'Teddy', 'Teresa', 'Terry', 'Tess', 'Theo', 'Theresa', 'Tim', 'Tina', 'Tommy', 'Tony', 'Tory', 'Tracy', 'Tuesday', 'Valencia', 'Valentine', 'Van', 'Venus', 'Vera', 'Victoria', 'Vin', 'Violet', 'Virginia', 'Viv', 'Wendy', 'Whitney', 'Willie'] {'[PLACE]': ['store', 'garden', 'restaurant', 'school', 'hospital', 'office', 'house', 'station'], '[OBJECT]': ['ring', 'kiss', 'bone', 'basketball', 'computer', 'necklace', 'drink', 'snack']}\n",
      "{'A': 'Violet', 'B': 'Claude', 'C': 'Leigh'}\n",
      "Lately, Violet, Claude, and Leigh had fun at school. Claude and Leigh gave a computer to\n",
      "Lately, Violet, Leigh, and Claude had fun at school. Violet and Leigh gave a computer to\n",
      " Violet\n",
      " Claude\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "import re\n",
    "# manual patching\n",
    "\n",
    "# Alice Bob. End is bob\n",
    "# hypothesize bob stored somewhere in stream, when alice appears again, prompts stored \n",
    "# Carol and dennis, see if output is dennis even though\n",
    "\n",
    "abc_formats = ['ABC DEF']\n",
    "share_one_abc = ['ABC', 'BAC', 'BCA']\n",
    "share_one_ade = [x.replace(\"B\", \"D\").replace(\"C\", \"E\") for x in share_one_abc]\n",
    "abc_formats += [a + \" \" + b for (a,b) in itertools.product(share_one_abc, share_one_ade)]\n",
    "\n",
    "# share two\n",
    "share_two_abc = ['ABC', 'ACB', 'CAB']\n",
    "share_two_ade = [x.replace(\"C\", \"D\") for x in share_two_abc]\n",
    "share_two_all = [(a,b) for (a,b) in itertools.product(share_two_abc, share_two_ade)]\n",
    "abc_formats += [a + \" \" + b for a,b in share_two_all]\n",
    "# swap A and B in second one\n",
    "abc_formats += [a + \" \" + b.replace(\"A\", \"W\").replace(\"B\", \"A\").replace(\"W\",\"B\") for a,b in share_two_all]\n",
    "\n",
    "# share three\n",
    "abc_formats += ['ABC ABC']\n",
    "abc_formats += ['ABC ACB', 'ABC CBA', 'ABC BAC'] # fix one, swap others\n",
    "abc_formats += ['ABC BCA', 'ABC CAB'] # shift\n",
    "\n",
    "print(abc_formats)\n",
    "\n",
    "\n",
    "from test_data import restrict_to_most_common_size, IOI_custom_generator\n",
    "import test_data\n",
    "\n",
    "\n",
    "format = \"\"\"\n",
    "ABC BC A\n",
    "ACB AC B\"\"\".strip()\n",
    "\n",
    "IOI_custom_generator(\n",
    "\n",
    "prompt_uncorrupted, prompt_corrupted, uncorrupted_answer, corrupted_answer = generate_data(format)\n",
    "print(prompt_uncorrupted)\n",
    "print(prompt_corrupted)\n",
    "print(uncorrupted_answer)\n",
    "print(corrupted_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e54af2e7-9115-481b-aa8e-0c8ae0237453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncorrupted prompt\n",
      "Lately, Violet, Claude, and Leigh had fun at school. Claude and Leigh gave a computer to\n",
      "' Violet' logit 7.7681708335876465\n",
      "' Violet' pr 3.4835611932065105e-25\n",
      "' Claude' logit -28.102453231811523\n",
      "' Claude' pr 9.196301431824477e-41\n",
      "\n",
      "corrupted prompt\n",
      "Lately, Violet, Leigh, and Claude had fun at school. Violet and Leigh gave a computer to\n",
      "' Violet' logit -24.652851104736328\n",
      "' Violet' pr 9.491529234365972e-38\n",
      "' Claude' logit -11.31766128540039\n",
      "' Claude' pr 5.871316865690532e-32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa92c49c5bed4e3ba0c96167c8ae9c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Patching Type', options=('layer input', 'normalized input', 'skip', 'in proj', 'conv', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3601f3e4d981415096762d2b5faf14e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='fast conv', options=('True', 'False'), value='True')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c68b4044ff0492ea22f0acd61187d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='fast ssm', options=('True', 'False'), value='True')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059e0a14ea6e4f5fb49c19550081089f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='normalize', options=('True', 'False'), value='True')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a90e6c297a824d7aa25992bdb6e411ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='logits or pr', options=('Both', 'Logits', 'Pr'), value='Both')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c896a0bcffdf4260a4952bf0141acf4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run Patching', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30ad560fe44491889cc6b0eb504ba71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# modified from neel nanda's examples\n",
    "\n",
    "# how should we patch when there are multiple outputs?\n",
    "# probably just use probability space, sum the probabilities of all the incorrect and correct options\n",
    "\n",
    "limited_layers = [0, 7, 10, 11, 13, 18, 19, 21, 23, 24, 28, 32, 38, 39]\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from functools import partial\n",
    "from jaxtyping import Float\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets\n",
    "import torch\n",
    "import plotly.subplots as sp\n",
    "\n",
    "prompt_uncorrupted_tokens = model.to_tokens(prompt_uncorrupted)\n",
    "prompt_corrupted_tokens = model.to_tokens(prompt_corrupted)\n",
    "\n",
    "# logits should be [B,L,V] \n",
    "def uncorrupted_logit_minus_corrupted_logit(logits, uncorrupted_answer, corrupted_answer):\n",
    "    uncorrupted_index = model.to_single_token(uncorrupted_answer)\n",
    "    corrupted_index = model.to_single_token(corrupted_answer)\n",
    "    return logits[0, -1, uncorrupted_index] - logits[0, -1, corrupted_index]\n",
    "\n",
    "# prs should be [B,L,V] \n",
    "def uncorrupted_pr_minus_corrupted_pr(prs, uncorrupted_answer, corrupted_answer):\n",
    "    uncorrupted_index = model.to_single_token(uncorrupted_answer)\n",
    "    corrupted_index = model.to_single_token(corrupted_answer)\n",
    "    return prs[0, -1, uncorrupted_index] - prs[0, -1, corrupted_index]\n",
    "\n",
    "# [B,L,V]\n",
    "corrupted_logits, corrupted_activations = model.run_with_cache(prompt_corrupted_tokens, only_use_these_layers=limited_layers)\n",
    "corrupted_logit_diff = uncorrupted_logit_minus_corrupted_logit(logits=corrupted_logits, uncorrupted_answer=uncorrupted_answer, corrupted_answer=corrupted_answer)\n",
    "corrupted_prs = torch.softmax(corrupted_logits, dim=2)\n",
    "corrupted_pr_diff = uncorrupted_pr_minus_corrupted_pr(prs=corrupted_prs, uncorrupted_answer=uncorrupted_answer, corrupted_answer=corrupted_answer)\n",
    "\n",
    "# [B,L,V]\n",
    "uncorrupted_logits = model(prompt_uncorrupted_tokens, only_use_these_layers=limited_layers)\n",
    "uncorrupted_logit_diff = uncorrupted_logit_minus_corrupted_logit(logits=uncorrupted_logits, uncorrupted_answer=uncorrupted_answer, corrupted_answer=corrupted_answer)\n",
    "uncorrupted_prs = torch.softmax(uncorrupted_logits, dim=2)\n",
    "uncorrupted_pr_diff = uncorrupted_pr_minus_corrupted_pr(prs=uncorrupted_prs, uncorrupted_answer=uncorrupted_answer, corrupted_answer=corrupted_answer)\n",
    "\n",
    "uncorrupted_index = model.to_single_token(uncorrupted_answer)\n",
    "corrupted_index = model.to_single_token(corrupted_answer)\n",
    "print(f'uncorrupted prompt\\n{prompt_uncorrupted}')\n",
    "print(f\"{repr(uncorrupted_answer)} logit {uncorrupted_logits[0,-1,uncorrupted_index]}\")\n",
    "print(f\"{repr(uncorrupted_answer)} pr {uncorrupted_prs[0,-1,uncorrupted_index]}\")\n",
    "print(f\"{repr(corrupted_answer)} logit {uncorrupted_logits[0,-1,corrupted_index]}\")\n",
    "print(f\"{repr(corrupted_answer)} pr {uncorrupted_prs[0,-1,corrupted_index]}\")\n",
    "print(f'\\ncorrupted prompt\\n{prompt_corrupted}')\n",
    "print(f\"{repr(uncorrupted_answer)} logit {corrupted_logits[0,-1,uncorrupted_index]}\")\n",
    "print(f\"{repr(uncorrupted_answer)} pr {corrupted_prs[0,-1,uncorrupted_index]}\")\n",
    "print(f\"{repr(corrupted_answer)} logit {corrupted_logits[0,-1,corrupted_index]}\")\n",
    "print(f\"{repr(corrupted_answer)} pr {corrupted_prs[0,-1,corrupted_index]}\")\n",
    "\n",
    "# We make a tensor to store the results for each patching run. We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.\n",
    "L = len(prompt_uncorrupted_tokens[0])\n",
    "if len(prompt_corrupted_tokens[0]) != len(prompt_uncorrupted_tokens[0]):\n",
    "    raise Exception(\"Prompts are not the same length\") # feel free to comment this out, you can patch for different sized prompts its just a lil sus\n",
    "\n",
    "# diff is logit of uncorrupted_answer - logit of corrupted_answer\n",
    "# we expect corrupted_diff to have a negative value (as corrupted should put high pr on corrupted_answer)\n",
    "# we expect uncorrupted to have a positive value (as uncorrupted should put high pr on uncorrupted_answer)\n",
    "# thus we can treat these as (rough) min and max possible values\n",
    "min_logit_diff = corrupted_logit_diff\n",
    "max_logit_diff = uncorrupted_logit_diff\n",
    "\n",
    "min_pr_diff = corrupted_pr_diff\n",
    "max_pr_diff = uncorrupted_pr_diff\n",
    "\n",
    "# make token labels that describe the patch\n",
    "corrupted_str_tokens = model.to_str_tokens(prompt_corrupted_tokens)\n",
    "uncorrupted_str_tokens = model.to_str_tokens(prompt_uncorrupted_tokens)\n",
    "token_labels = []\n",
    "for index, (corrupted_token, uncorrupted_token) in enumerate(zip(corrupted_str_tokens, uncorrupted_str_tokens)):\n",
    "    if corrupted_token == uncorrupted_token:\n",
    "        token_labels.append(f\"{corrupted_token}_{index}\")\n",
    "    else:\n",
    "        token_labels.append(f\"{uncorrupted_token}->{corrupted_token}_{index}\")\n",
    "\n",
    "def run_patching(patching_hook_name_func, patching_hook_func, show_options, show_plot=True, normalize=False, **kwargs):\n",
    "    hook_title = patching_hook_name_func(layer='{layer}', position='{position}')\n",
    "    print(f\"running patching for {hook_title}\")\n",
    "    global patching_result_logits, patching_result_prs # if you want to access it once this is done running\n",
    "    n_layers = len(limited_layers)\n",
    "    patching_result_logits = torch.zeros((n_layers, L), device=model.cfg.device)\n",
    "    patching_result_prs = torch.zeros((n_layers, L), device=model.cfg.device)\n",
    "    patching_result_logits2 = torch.zeros((n_layers, L), device=model.cfg.device)\n",
    "    patching_result_prs2 = torch.zeros((n_layers, L), device=model.cfg.device)\n",
    "    for i, layer in tqdm(list(enumerate(limited_layers))):\n",
    "        for position in range(L):\n",
    "            patching_hook_name = patching_hook_name_func(layer=layer, position=position)\n",
    "            patching_hook = partial(patching_hook_func, layer=layer, position=position)\n",
    "            # [B,L,V]\n",
    "            patched_logits = model.run_with_hooks(prompt_uncorrupted_tokens, fwd_hooks=[\n",
    "                (patching_hook_name, patching_hook)\n",
    "            ], only_use_these_layers=limited_layers, **kwargs)\n",
    "            # [B,L,V]\n",
    "            patched_prs = torch.softmax(patched_logits, dim=2)\n",
    "\n",
    "            if normalize:\n",
    "                patched_logit_diff = uncorrupted_logit_minus_corrupted_logit(logits=patched_logits,\n",
    "                                                                             uncorrupted_answer=uncorrupted_answer,\n",
    "                                                                             corrupted_answer=corrupted_answer)\n",
    "                if corrupted_answer == uncorrupted_answer:\n",
    "                    print(\"warning: both uncorrupted answer and corrupted answer are same, use unnormalized\")\n",
    "                # normalize it so\n",
    "                # 0 means min_logit_diff (so 0 means that it is acting like the corrupted model)\n",
    "                # 1 means max_logit_diff (so 1 means that it is acting like the uncorrupted model)\n",
    "                normalized_patched_logit_diff = (patched_logit_diff-min_logit_diff)/(max_logit_diff - min_logit_diff)\n",
    "                # now flip them, since most interventions will do nothing and thus act like uncorrupted model, visually its better to have that at 0\n",
    "                # so now\n",
    "                # 0 means that it is acting like the uncorrupted model\n",
    "                # 1 means that it is acting like the corrupted model\n",
    "                normalized_patched_logit_diff = 1.0 - normalized_patched_logit_diff\n",
    "                patching_result_logits[i, position] = normalized_patched_logit_diff\n",
    "                \n",
    "                # same for pr\n",
    "                patched_pr_diff = uncorrupted_pr_minus_corrupted_pr(prs=patched_prs,\n",
    "                                                                    uncorrupted_answer=uncorrupted_answer,\n",
    "                                                                    corrupted_answer=corrupted_answer)\n",
    "                normalized_patched_pr_diff = 1.0-(patched_pr_diff-min_pr_diff)/(max_pr_diff - min_pr_diff)\n",
    "                patching_result_prs[i, position] = normalized_patched_pr_diff\n",
    "            else:\n",
    "                patching_result_logits[i, position] = patched_logits[0,-1,uncorrupted_index]\n",
    "                patching_result_logits2[i, position] = patched_logits[0,-1,corrupted_index]\n",
    "                patching_result_prs[i, position] = patched_prs[0,-1,uncorrupted_index]\n",
    "                patching_result_prs2[i, position] = patched_prs[0,-1,corrupted_index]\n",
    "\n",
    "    layer_labels = [str(layer) for layer in limited_layers]\n",
    "    figs = []\n",
    "    if normalize:\n",
    "        if show_options in [SHOW_LOGITS, SHOW_BOTH]:\n",
    "            figs.append(imshow(patching_result_logits, show=False, x=token_labels, y=layer_labels, xaxis=\"Position\", yaxis=\"Layer\", title=f\"Normalized Logit Difference After Patching {hook_title}\", font_size=8))\n",
    "        if show_options in [SHOW_PR, SHOW_BOTH]:\n",
    "            figs.append(imshow(patching_result_prs, show=False, x=token_labels, y=layer_labels, xaxis=\"Position\", yaxis=\"Layer\", title=f\"Normalized Pr Difference After Patching {hook_title}\", font_size=8))\n",
    "    else:\n",
    "        if show_options in [SHOW_LOGITS, SHOW_BOTH]:\n",
    "            figs.append(imshow(patching_result_logits, color_continuous_midpoint=None, show=False, x=token_labels, y=layer_labels, xaxis=\"Position\", yaxis=\"Layer\", title=f\"Logit of uncorrupted answer {repr(uncorrupted_answer)} after Patching {hook_title}\", font_size=8))\n",
    "            if uncorrupted_answer != corrupted_answer:\n",
    "                figs.append(imshow(patching_result_logits2, color_continuous_midpoint=None, show=False, x=token_labels, y=layer_labels, xaxis=\"Position\", yaxis=\"Layer\", title=f\"Logit of corrupted answer {repr(corrupted_answer)} after Patching {hook_title}\", font_size=8))\n",
    "        if show_options in [SHOW_PR, SHOW_BOTH]:\n",
    "            figs.append(imshow(patching_result_prs, show=False, x=token_labels, y=layer_labels, xaxis=\"Position\", yaxis=\"Layer\", title=f\"Pr of uncorrupted answer {repr(uncorrupted_answer)} after Patching {hook_title}\", font_size=8)) \n",
    "            if uncorrupted_answer != corrupted_answer:\n",
    "                figs.append(imshow(patching_result_prs2, show=False, x=token_labels, y=layer_labels, xaxis=\"Position\", yaxis=\"Layer\", title=f\"Pr of corrupted answer {repr(corrupted_answer)} after Patching {hook_title}\", font_size=8))\n",
    "\n",
    "    traces = []\n",
    "    for fig in figs:\n",
    "        fig.show()\n",
    "        fig_trace = []\n",
    "        for trace in range(len(fig[\"data\"])):\n",
    "            fig_trace.append(fig[\"data\"][trace])\n",
    "        traces.append(fig_trace)\n",
    "    \n",
    "    #this_figure = sp.make_subplots(rows=2, cols=2)\n",
    "    #pos = [[1,1], [1,2], [2,1], [2,2]]\n",
    "    #for i, (fig, trace) in enumerate(zip(figs, traces)):\n",
    "    #    this_figure.append_trace(traces, row=pos[i][0], col=pos[i][1])\n",
    "    #this_figure.display()\n",
    "    '''\n",
    "    print(\"logits\")\n",
    "    for pos in [11,15]:\n",
    "        for layer in range(29, model.cfg.n_layers):\n",
    "            print(pos, layer, patching_result_logits[layer, pos].item())\n",
    "    \n",
    "    print(\"prs\")\n",
    "    for pos in [11,15]:\n",
    "        for layer in range(29, model.cfg.n_layers):\n",
    "            print(pos, layer, patching_result_prs[layer, pos].item())\n",
    "    '''\n",
    "\n",
    "def position_patching_hook( # also works for B L E, B L E N, and B L N sized things\n",
    "    x: Float[torch.Tensor, \"B L D\"],\n",
    "    hook: HookPoint,\n",
    "    position: int,\n",
    "    layer: int # we don't care about this\n",
    ") -> Float[torch.Tensor, \"B L D\"]:\n",
    "    # only intervene on the specific pos\n",
    "    corrupted_x = corrupted_activations[hook.name]\n",
    "    x[:, position, :] = corrupted_x[:, position, :]\n",
    "    return x\n",
    "\n",
    "# 'blocks.{layer}.hook_h.{pos}' is the recurrent state of that layer after processing tokens at and before pos position\n",
    "def h_patching_hook(\n",
    "    h: Float[torch.Tensor, \"B E N\"],\n",
    "    hook: HookPoint,\n",
    "    position: int,\n",
    "    layer: int\n",
    ") -> Float[torch.Tensor, \"B E N\"]:\n",
    "    return corrupted_activations[hook.name]\n",
    "\n",
    "\n",
    "patching_types = {\n",
    "    'layer input': (lambda layer, position: f'blocks.{layer}.hook_resid_pre', position_patching_hook),\n",
    "    'normalized input': (lambda layer, position: f'blocks.{layer}.hook_normalized_input', position_patching_hook), \n",
    "    'skip': (lambda layer, position: f'blocks.{layer}.hook_skip', position_patching_hook), \n",
    "    'in proj': (lambda layer, position: f'blocks.{layer}.hook_in_proj', position_patching_hook), \n",
    "    'conv': (lambda layer, position: f'blocks.{layer}.hook_conv', position_patching_hook), \n",
    "    'delta 1': (lambda layer, position: f'blocks.{layer}.hook_delta_1', position_patching_hook), \n",
    "    'delta 2': (lambda layer, position: f'blocks.{layer}.hook_delta_2', position_patching_hook), \n",
    "    'delta': (lambda layer, position: f'blocks.{layer}.hook_delta', position_patching_hook), \n",
    "    'A_bar': (lambda layer, position: f'blocks.{layer}.hook_A_bar', position_patching_hook), \n",
    "    'B': (lambda layer, position: f'blocks.{layer}.hook_B', position_patching_hook), \n",
    "    'B_bar': (lambda layer, position: f'blocks.{layer}.hook_B_bar', position_patching_hook), \n",
    "    'C': (lambda layer, position: f'blocks.{layer}.hook_C', position_patching_hook), \n",
    "    'ssm input': (lambda layer, position: f'blocks.{layer}.hook_ssm_input', position_patching_hook),\n",
    "    'h': (lambda layer, position: f'blocks.{layer}.hook_h.{position}', h_patching_hook),\n",
    "    'y': (lambda layer, position: f'blocks.{layer}.hook_y', position_patching_hook),\n",
    "    'ssm output': (lambda layer, position: f'blocks.{layer}.hook_ssm_output', position_patching_hook),\n",
    "    'after skip': (lambda layer, position: f'blocks.{layer}.hook_after_skip', position_patching_hook),\n",
    "    'out proj': (lambda layer, position: f'blocks.{layer}.hook_out_proj', position_patching_hook),\n",
    "    'resid post': (lambda layer, position: f'blocks.{layer}.hook_resid_post', position_patching_hook),\n",
    "}\n",
    "\n",
    "patching_types_keys = list(patching_types.keys())\n",
    "\n",
    "def choose_patching_type(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_patching_type.patching_type = change['new'] # hack, gives this function the patching_type attribute\n",
    "\n",
    "choose_patching_type.patching_type = patching_types_keys[0]\n",
    "\n",
    "patching_type_dropdown = ipywidgets.Dropdown(\n",
    "    options=patching_types_keys,\n",
    "    value=patching_types_keys[0],\n",
    "    description='Patching Type',\n",
    ")\n",
    "patching_type_dropdown.observe(choose_patching_type)\n",
    "display(patching_type_dropdown)\n",
    "\n",
    "\n",
    "fast_conv_keys = ['True', 'False']\n",
    "\n",
    "def choose_fast_conv(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_fast_conv.fast_conv = change['new'] == 'True'\n",
    "\n",
    "choose_fast_conv.fast_conv = fast_conv_keys[0] == 'True'\n",
    "\n",
    "choose_fast_conv_dropdown = ipywidgets.Dropdown(\n",
    "    options=fast_conv_keys,\n",
    "    value=fast_conv_keys[0],\n",
    "    description='fast conv',\n",
    ")\n",
    "choose_fast_conv_dropdown.observe(choose_fast_conv)\n",
    "display(choose_fast_conv_dropdown)\n",
    "\n",
    "\n",
    "\n",
    "fast_ssm_keys = ['True', 'False']\n",
    "\n",
    "def choose_fast_ssm(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_fast_ssm.fast_ssm = change['new'] == 'True'\n",
    "\n",
    "choose_fast_ssm.fast_ssm = fast_ssm_keys[0] == 'True'\n",
    "\n",
    "choose_fast_ssm_dropdown = ipywidgets.Dropdown(\n",
    "    options=fast_ssm_keys,\n",
    "    value=fast_ssm_keys[0],\n",
    "    description='fast ssm',\n",
    ")\n",
    "choose_fast_ssm_dropdown.observe(choose_fast_ssm)\n",
    "display(choose_fast_ssm_dropdown)\n",
    "\n",
    "\n",
    "normalized_keys = ['True', 'False']\n",
    "\n",
    "def choose_normalized(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_normalized.normalized = change['new'] == 'True'\n",
    "\n",
    "choose_normalized.normalized = normalized_keys[0] == 'True'\n",
    "\n",
    "normalized_dropdown = ipywidgets.Dropdown(\n",
    "    options=normalized_keys,\n",
    "    value=normalized_keys[0],\n",
    "    description='normalize',\n",
    ")\n",
    "normalized_dropdown.observe(choose_normalized)\n",
    "display(normalized_dropdown)\n",
    "\n",
    "SHOW_PR = 'Pr'\n",
    "SHOW_LOGITS = 'Logits'\n",
    "SHOW_BOTH = 'Both'\n",
    "show_options = [SHOW_BOTH, SHOW_LOGITS, SHOW_PR]\n",
    "\n",
    "def choose_show_options(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        choose_show_options.show_options = change['new']\n",
    "\n",
    "choose_show_options.show_options = show_options[0]\n",
    "\n",
    "show_options_dropdown = ipywidgets.Dropdown(\n",
    "    options=show_options,\n",
    "    value=show_options[0],\n",
    "    description='logits or pr',\n",
    ")\n",
    "show_options_dropdown.observe(choose_show_options)\n",
    "display(show_options_dropdown)\n",
    "\n",
    "def do_patching(arg, show_plot=True):\n",
    "    with output: # this lets the stuff we output here be visible\n",
    "        clear_output()\n",
    "        hook_name_func, hook_func = patching_types[choose_patching_type.patching_type]\n",
    "        return run_patching(patching_hook_name_func=hook_name_func,\n",
    "                     patching_hook_func=hook_func,\n",
    "                     fast_ssm=choose_fast_ssm.fast_ssm,\n",
    "                     fast_conv=choose_fast_conv.fast_conv,\n",
    "                     normalize=choose_normalized.normalized,\n",
    "                     show_options=choose_show_options.show_options,\n",
    "                     show_plot=show_plot)\n",
    "\n",
    "patching_button = ipywidgets.Button(description = 'Run Patching')\n",
    "patching_button.on_click(do_patching)\n",
    "display(patching_button)\n",
    "\n",
    "# you can't just display stuff inside a widget callback, you need a wrap any display code in this\n",
    "output = ipywidgets.Output()\n",
    "display(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f7bec9a3-c54e-4199-9207-3b9b89497da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d05c7-113b-4e6c-a7ad-51f361b0eabc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
