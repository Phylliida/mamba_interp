{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9a78832-f1cb-42a6-a907-f48bdd542a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import torch\n",
    "from einops import rearrange\n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7be2fe9a-9fac-41ad-86cd-73ca5d0f3db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f6dbb95ba30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from mamba_lens import HookedMamba\n",
    "model = HookedMamba.from_pretrained(\"state-spaces/mamba-370m\", device='cuda')\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "781e3268-eb97-4b19-bb45-f3584a770828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_token(tokenizer):\n",
    "    return model.tokenizer.encode(model.tokenizer.pad_token, add_special_tokens=False)[0]\n",
    "\n",
    "# given data that is [N,V] and indicies that are [N,K] with each index being an index into the V space\n",
    "# this does what you'd want, it indexes them\n",
    "# idk, see the test\n",
    "def index_into(data, indices):\n",
    "    num_data, num_per_data = indices.size()\n",
    "    # we want\n",
    "    # [0,0,0,...,] num per data of these\n",
    "    # [1,1,1,...,] num per data of these\n",
    "    # ...\n",
    "    # [num_data-1, num_data-1, ...]\n",
    "    first_axis_index = torch.arange(num_data, dtype=torch.long).view(num_data, 1)*torch.ones([num_data, num_per_data], dtype=torch.long)\n",
    "    # now we flatten it so it has an index for each term aligned with our indices\n",
    "    first_axis_index = first_axis_index.flatten()\n",
    "\n",
    "    second_axis_index = indices.flatten()\n",
    "    # now we can just index, and then view back to our original shape\n",
    "    return data[first_axis_index, second_axis_index].view(num_data, num_per_data)\n",
    "    \n",
    "\n",
    "def eval(model, data, correct, incorrect, **kwargs):\n",
    "        num_examples = correct.size()[0]\n",
    "        logits = model(data, **kwargs)[:,-1]\n",
    "        tops = torch.argsort(-logits, dim=1)\n",
    "        pad = get_pad_token(tokenizer=model.tokenizer)\n",
    "        prs = torch.nn.functional.softmax(logits, dim=1)\n",
    "        prs[:,pad] = 0 # manually set pad pr to zero because sometimes we need to pad num correct or num incorrect\n",
    "        #for i in range(tops.size()[0]):\n",
    "        #    print(model.to_str_tokens([tops[i,0]]), tops[i,0], logits[i, tops[i,0]], prs[i, tops[i,0]])\n",
    "        #    break\n",
    "        # [n_data, n_correct]\n",
    "        correct_prs = index_into(prs, correct)\n",
    "        # [n_data, n_incorrect]\n",
    "        incorrect_prs = index_into(prs, incorrect)\n",
    "        # [n_data, 1]\n",
    "        total_prs = correct_prs.sum(dim=1, keepdim=True)+incorrect_prs.sum(dim=1, keepdim=True)\n",
    "        total_prs[total_prs == 0] = 1.0\n",
    "        correct_prs /= total_prs\n",
    "        incorrect_prs /= total_prs\n",
    "\n",
    "        # [n_data, n_correct + n_incorrect]\n",
    "        combined = torch.concatenate([correct_prs, incorrect_prs], dim=1)\n",
    "        biggest = torch.argsort(-combined, dim=1)\n",
    "        n_data, n_correct = correct.size()\n",
    "        # if biggest pr is in the correct, we are correct, otherwise, we are not\n",
    "        num_correct = torch.sum(biggest[:,0] < n_correct)\n",
    "        # the sum(dim=1) is because we or of all the different possible probabilities by summing\n",
    "        # then we'll just report the average\n",
    "        return torch.mean(correct_prs.sum(dim=1)).item(), torch.mean(incorrect_prs.sum(dim=1)).item(), num_correct.item()/float(n_data)\n",
    "\n",
    "def add_padding_answers(tokenizer, answers):\n",
    "    longest_len = len(max(answers, key=lambda x: len(x)))\n",
    "    padded_answers = []\n",
    "    pad_token = get_pad_token(tokenizer=tokenizer)\n",
    "    for answer in answers:\n",
    "        padded_answers.append(answer + [pad_token]*(longest_len-len(answer)))\n",
    "    return padded_answers\n",
    "\n",
    "def get_batched_data(data):\n",
    "    batched_data = []\n",
    "    batched_correct = []\n",
    "    batched_incorrect = []\n",
    "    \n",
    "    for i, (prompt, corrects, incorrects) in enumerate(data):\n",
    "        if i < 3:\n",
    "            print(prompt, corrects, incorrects)\n",
    "        batched_data.append(torch.tensor(model.tokenizer.encode(prompt), device=model.cfg.device))\n",
    "        batched_correct.append([model.tokenizer.encode(correct)[0] for correct in corrects])\n",
    "        batched_incorrect.append([model.tokenizer.encode(incorrect)[0] for incorrect in incorrects])\n",
    "    try:\n",
    "        batched_data = torch.stack(batched_data)\n",
    "        batched_correct = torch.tensor(add_padding_answers(tokenizer=model.tokenizer, answers=batched_correct))\n",
    "        batched_incorrect = torch.tensor(add_padding_answers(tokenizer=model.tokenizer, answers=batched_incorrect))\n",
    "    except RuntimeError:\n",
    "        typical_len = len(batched_data[0])\n",
    "        for s in batched_data:\n",
    "            if not len(s) == typical_len:\n",
    "                print(len(s), \"is len of this, typical len is\", typical_len, \"for sequence\", model.to_str_tokens(s))\n",
    "        raise\n",
    "    return batched_data, batched_correct, batched_incorrect\n",
    "\n",
    "def bar_chart(data, x_labels, y_label, title, font_size=None):\n",
    "    # it requires a pandas dict with the columns and rows named, annoying\n",
    "    # by default rows and columns are named with ints so we relabel them accordingly\n",
    "    renames = dict([(i, x_labels[i]) for i in range(len(x_labels))])\n",
    "    ps = pd.DataFrame(data.cpu().numpy()).rename(renames, axis='rows').rename({0: y_label}, axis='columns')\n",
    "    fig = px.bar(ps, y=y_label, x=x_labels, title=title)\n",
    "    if not font_size is None:\n",
    "        fig.update_layout(\n",
    "          xaxis = dict(\n",
    "            tickmode='array',\n",
    "            tickvals = x_labels,\n",
    "            ticktext = x_labels, \n",
    "            ),\n",
    "           font=dict(size=font_size, color=\"black\"))\n",
    "        \n",
    "        #fig.update_xaxes(title_font=dict(size=font_size))\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440ec1b4-7953-4fd3-b4a2-d71229f5c372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "nouns using ['accord', 'affair', 'agreement', 'appraisal', 'assaults', 'assessment', 'attack', 'attempts', 'campaign', 'case', 'challenge', 'chaos', 'clash', 'collaboration', 'coma', 'competition', 'confrontation', 'consequence', 'conspiracy', 'construction', 'consultation', 'contact', 'contract', 'convention', 'cooperation', 'custody', 'deal', 'decline', 'decrease', 'demonstrations', 'development', 'disagreement', 'disorder', 'dispute', 'domination', 'dynasty', 'effect', 'effort', 'employment', 'endeavor', 'engagement', 'epidemic', 'evaluation', 'exchange', 'existence', 'expansion', 'expedition', 'experiments', 'fall', 'fame', 'flights', 'friendship', 'growth', 'hardship', 'hostility', 'illness', 'impact', 'imprisonment', 'improvement', 'incarceration', 'increase', 'invasion', 'investigation', 'journey', 'kingdom', 'marriage', 'negotiation', 'obstruction', 'operation', 'order', 'outbreak', 'outcome', 'overhaul', 'plague', 'plan', 'practice', 'process', 'program', 'progress', 'project', 'pursuit', 'quest', 'raids', 'reforms', 'reign', 'relationship', 'retaliation', 'riot', 'rise', 'rivalry', 'romance', 'rule', 'sanctions', 'shift', 'siege', 'stature', 'stint', 'strikes', 'study', 'test', 'testing', 'tests', 'therapy', 'tour', 'tradition', 'treaty', 'trial', 'trip', 'unemployment', 'voyage', 'warfare', 'work']\n",
      "The obstruction lasted from the year 1497 to 14 ['98'] ['10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97']\n",
      "The rise lasted from the year 1216 to 12 ['17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98'] ['10', '11', '12', '13', '14', '15', '16']\n",
      "The rise lasted from the year 1445 to 14 ['46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98'] ['10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45']\n",
      "valid\n",
      "nouns using ['accord', 'affair', 'agreement', 'appraisal', 'assaults', 'assessment', 'attack', 'attempts', 'campaign', 'case', 'challenge', 'chaos', 'clash', 'collaboration', 'coma', 'competition', 'confrontation', 'consequence', 'conspiracy', 'construction', 'consultation', 'contact', 'contract', 'convention', 'cooperation', 'custody', 'deal', 'decline', 'decrease', 'demonstrations', 'development', 'disagreement', 'disorder', 'dispute', 'domination', 'dynasty', 'effect', 'effort', 'employment', 'endeavor', 'engagement', 'epidemic', 'evaluation', 'exchange', 'existence', 'expansion', 'expedition', 'experiments', 'fall', 'fame', 'flights', 'friendship', 'growth', 'hardship', 'hostility', 'illness', 'impact', 'imprisonment', 'improvement', 'incarceration', 'increase', 'invasion', 'investigation', 'journey', 'kingdom', 'marriage', 'negotiation', 'obstruction', 'operation', 'order', 'outbreak', 'outcome', 'overhaul', 'plague', 'plan', 'practice', 'process', 'program', 'progress', 'project', 'pursuit', 'quest', 'raids', 'reforms', 'reign', 'relationship', 'retaliation', 'riot', 'rise', 'rivalry', 'romance', 'rule', 'sanctions', 'shift', 'siege', 'stature', 'stint', 'strikes', 'study', 'test', 'testing', 'tests', 'therapy', 'tour', 'tradition', 'treaty', 'trial', 'trip', 'unemployment', 'voyage', 'warfare', 'work']\n",
      "The experiments lasted from the year 1450 to 14 ['51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98'] ['10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50']\n",
      "The incarceration lasted from the year 1317 to 13 ['18', '19', '20', '25', '30', '32', '40', '50', '60', '64', '80'] ['10', '11', '12', '13', '14', '15', '16', '17']\n",
      "The process lasted from the year 1548 to 15 ['49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98'] ['10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48']\n",
      "test\n",
      "nouns using ['accord', 'affair', 'agreement', 'appraisal', 'assaults', 'assessment', 'attack', 'attempts', 'campaign', 'case', 'challenge', 'chaos', 'clash', 'collaboration', 'coma', 'competition', 'confrontation', 'consequence', 'conspiracy', 'construction', 'consultation', 'contact', 'contract', 'convention', 'cooperation', 'custody', 'deal', 'decline', 'decrease', 'demonstrations', 'development', 'disagreement', 'disorder', 'dispute', 'domination', 'dynasty', 'effect', 'effort', 'employment', 'endeavor', 'engagement', 'epidemic', 'evaluation', 'exchange', 'existence', 'expansion', 'expedition', 'experiments', 'fall', 'fame', 'flights', 'friendship', 'growth', 'hardship', 'hostility', 'illness', 'impact', 'imprisonment', 'improvement', 'incarceration', 'increase', 'invasion', 'investigation', 'journey', 'kingdom', 'marriage', 'negotiation', 'obstruction', 'operation', 'order', 'outbreak', 'outcome', 'overhaul', 'plague', 'plan', 'practice', 'process', 'program', 'progress', 'project', 'pursuit', 'quest', 'raids', 'reforms', 'reign', 'relationship', 'retaliation', 'riot', 'rise', 'rivalry', 'romance', 'rule', 'sanctions', 'shift', 'siege', 'stature', 'stint', 'strikes', 'study', 'test', 'testing', 'tests', 'therapy', 'tour', 'tradition', 'treaty', 'trial', 'trip', 'unemployment', 'voyage', 'warfare', 'work']\n",
      "The tour lasted from the year 1523 to 15 ['24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98'] ['10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23']\n",
      "The employment lasted from the year 1332 to 13 ['40', '50', '60', '64', '80'] ['10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '25', '30', '32']\n",
      "The riot lasted from the year 1457 to 14 ['58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98'] ['10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57']\n",
      "0.9763083457946777 0.023691654205322266 1.0\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [00:12,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing layer 1\n",
      "0.9652735590934753 0.03472641110420227 1.0\n",
      "[0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [00:11,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing layer 2\n",
      "0.9260562062263489 0.07394378632307053 1.0\n",
      "[0, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [00:11,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing layer 3\n",
      "0.9292219281196594 0.07077810913324356 1.0\n",
      "[0, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [00:11,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing layer 4\n",
      "0.9292176365852356 0.07078230381011963 1.0\n",
      "[0, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:10,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing layer 5\n",
      "0.925422191619873 0.0745777040719986 1.0\n",
      "[0, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:10,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing layer 6\n",
      "0.9333218932151794 0.06667809933423996 1.0\n",
      "[0, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [00:09,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing layer 7\n",
      "0.9566264748573303 0.04337349906563759 1.0\n",
      "[0, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:09,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing layer 8\n",
      "0.9570193290710449 0.04298064485192299 1.0\n",
      "[0, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:08,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing layer 9\n",
      "0.9584576487541199 0.041542358696460724 1.0\n",
      "[0, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [00:08,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing layer 10\n",
      "0.9539596438407898 0.04604038968682289 1.0\n",
      "[0, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [00:07,  4.76it/s]"
     ]
    }
   ],
   "source": [
    "from docstring import docstring_prompt_generator_function\n",
    "from importlib import reload\n",
    "import test_data\n",
    "reload(test_data)\n",
    "from test_data import IOI_generator, BABA_TEMPLATES, greater_than_data_generator\n",
    "\n",
    "out_acc = torch.zeros([model.cfg.n_layers], device=model.cfg.device)\n",
    "out_prs_correct = torch.zeros([model.cfg.n_layers], device=model.cfg.device)\n",
    "out_prs_incorrect = torch.zeros([model.cfg.n_layers], device=model.cfg.device)\n",
    "\n",
    "num_examples = 100\n",
    "\n",
    "seed = 27\n",
    "valid_seed = 37\n",
    "test_seed = 47\n",
    "\n",
    "data_type = 'greater than'\n",
    "\n",
    "if data_type == 'ioi':\n",
    "    data = IOI_generator(templates=[BABA_TEMPLATES[0]], tokenizer=model.tokenizer, num_examples=num_examples, seed=seed)\n",
    "    valid_data = IOI_generator(templates=[BABA_TEMPLATES[0]], tokenizer=model.tokenizer, num_examples=num_examples, seed=valid_seed)\n",
    "    test_data = IOI_generator(templates=[BABA_TEMPLATES[0]], tokenizer=model.tokenizer, num_examples=num_examples, seed=test_seed)\n",
    "elif data_type == 'docstring':\n",
    "    data = docstring_prompt_generator_function(tokenizer=model.tokenizer, num_examples=num_examples, corrupt='random_answer', seed=seed)\n",
    "    valid_data = docstring_prompt_generator_function(tokenizer=model.tokenizer, num_examples=num_examples, corrupt='random_answer', seed=valid_seed)\n",
    "    test_data = docstring_prompt_generator_function(tokenizer=model.tokenizer, num_examples=num_examples, corrupt='random_answer', seed=test_seed)\n",
    "elif data_type == 'greater than':\n",
    "    data = greater_than_data_generator(tokenizer=model.tokenizer, num_examples=num_examples, seed=seed)\n",
    "    valid_data = greater_than_data_generator(tokenizer=model.tokenizer, num_examples=num_examples, seed=valid_seed)\n",
    "    test_data = greater_than_data_generator(tokenizer=model.tokenizer, num_examples=num_examples, seed=test_seed)\n",
    "\n",
    "print(\"data\")\n",
    "batched_data, batched_correct, batched_incorrect = get_batched_data(data)\n",
    "print(\"valid\")\n",
    "vbatched_data, vbatched_correct, vbatched_incorrect = get_batched_data(valid_data)\n",
    "print(\"test\")\n",
    "tbatched_data, tbatched_correct, tbatched_incorrect = get_batched_data(test_data)\n",
    "\n",
    "history = []\n",
    "history_stats = []\n",
    "layers_to_remove = []\n",
    "while len(layers_to_remove) < model.cfg.n_layers:\n",
    "    base_layers = list(range(model.cfg.n_layers))\n",
    "\n",
    "\n",
    "    for layer in layers_to_remove:\n",
    "        base_layers.remove(layer)\n",
    "    history.append(list(base_layers))\n",
    "    \n",
    "    correct, incorrect, acc = eval(model, vbatched_data, vbatched_correct, vbatched_incorrect,\n",
    "                                      only_use_these_layers=base_layers, fast_ssm=True, fast_conv=True)\n",
    "    print(correct, incorrect, acc)\n",
    "    history_stats.append((correct, incorrect, acc))\n",
    "    print(base_layers)\n",
    "    \n",
    "    for i, start_layer in tqdm.tqdm(enumerate(base_layers)):\n",
    "        #layers = list(range(start_layer, end_layer+1))\n",
    "        layers = list(base_layers)\n",
    "        layers.remove(start_layer)\n",
    "\n",
    "        \n",
    "        correct, incorrect, acc = eval(model, batched_data, batched_correct, batched_incorrect,\n",
    "                                      only_use_these_layers=layers, fast_ssm=True, fast_conv=True)\n",
    "        out_prs_correct[i] = correct\n",
    "        out_prs_incorrect[i] = incorrect\n",
    "        out_acc[i] = acc\n",
    "        \n",
    "    \n",
    "    best_layer_to_remove = base_layers[torch.argsort(-out_acc[:len(base_layers)])[0]]\n",
    "    print(\"removing layer\", best_layer_to_remove)\n",
    "    layer_names = [f'layer {x}' for x in base_layers]\n",
    "    layers_to_remove.append(best_layer_to_remove)\n",
    "\n",
    "history_stats = torch.tensor(history_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da0be93-5825-415c-aec5-bff16316d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_chart(history_stats[:,0], x_labels=[str(x) for x in history], y_label='relative pr of correct', title=f\"{data_type} pruning layers relative pr correct\", font_size=4)\n",
    "bar_chart(history_stats[:,1], x_labels=[str(x) for x in history], y_label='relative pr of incorrect', title=f\"{data_type} pruning layers relative pr incorrect\", font_size=4)\n",
    "bar_chart(history_stats[:,2], x_labels=[str(x) for x in history], y_label='accuracy', title=f\"{data_type} pruning layers accuracy\", font_size=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
